{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  漲停看它五天內還會不會漲停\n",
    "\n",
    "- 照老師的方法，把資料整理成 150天 一長條一筆，如果往前沒有 150 天就不用。\n",
    "- 掃出所有漲停的日子，以及後 n 天，如果又有漲停就繼續 one-shot 再延 n 天。這樣縮小 dataset 範圍。\n",
    "- 從 dataset 中，問 AI 明天破不破前一日最低？平衡正反雙方，取得 n 值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import peforth as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reDef unknown\n",
      "reDef \\\n"
     ]
    }
   ],
   "source": [
    "%%f\n",
    "\\ Now we redefine the 'unknown' command that does nothing at default  \n",
    ": unknown ( token -- thing Y|N) // Try to find the unknown token in __main__\n",
    "  py> getattr(sys.modules['__main__'],pop(),\"Ûnknôwn\") \n",
    "  py> str(tos())==\"Ûnknôwn\" if drop false else true then ;\n",
    "  /// here after, when FORTH come accross an unknown token, instead of alerting \n",
    "  /// it try to find the token in python __main__ module name space.\n",
    ": path-to-find-modules ( <path> -- ) // Add path to sys.path so \"import module-name\" can find the module\n",
    "    CR word trim ( \"path\" ) py: sys.path.append(pop()) ;\n",
    "code \\ print(nexttoken('\\n')) end-code // Redefine \\ command to print the comment line \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('000777.csv',encoding='gbk',parse_dates=[0],index_col=0)\n",
    "\n",
    "# data frame 中帶有 None 的 rows 都剔除掉。 本來有 4752 rows 現在只剩 4639 \n",
    "# 用法: df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "data = data.replace(to_replace='None', value=np.nan).dropna()  \n",
    "\n",
    "# pdb.set_trace() # data 本來是新的在上面\n",
    "data.sort_index(0,ascending=True,inplace=True) # 排序 sort 過後變成最舊的在上面\n",
    "\n",
    "# 上市第一天的資料不要，因為【漲跌幅】無效。\n",
    "data = data.query('index > \"1997-07-10\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data has 20 years long, but here only analysis the last 150 days of a chosen day\n",
    "dayfeature = 150\n",
    "oneshot = 5  # 漲停後慣性天數，使正反平衡\n",
    "dayforesee = oneshot  # 問未來幾天如何如何，最後就要扣掉幾天，避免 out of bounds. 0 代表當天。\n",
    "featurenum = 5*dayfeature  # 取五個有關係的欄位，特徵值。\n",
    "cross_validation_rate = 0.2\n",
    "xlength = data.shape[0]-dayfeature-dayforesee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 總天數 ) data :> shape[0] tib. \\ ==> 4638 (<class 'int'>)\n",
      "( leading days that are not used ) dayfeature tib. \\ ==> 150 (<class 'int'>)\n",
      "( latest days that are not used ) dayforesee tib. \\ ==> 5 (<class 'int'>)\n",
      "( 有效天數 ) data :> shape[0] dayfeature - dayforesee - tib. \\ ==> 4483 (<class 'int'>)\n"
     ]
    }
   ],
   "source": [
    "%f ( 總天數 ) data :> shape[0] tib. ( 4638 )\n",
    "%f ( leading days that are not used ) dayfeature tib.   \n",
    "%f ( latest days that are not used ) dayforesee tib.\n",
    "%f ( 有效天數 ) data :> shape[0] dayfeature - dayforesee - tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.zeros((xlength,featurenum+2))  # +2 是當天開盤跟當天日期\n",
    "y=np.zeros((xlength));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重疊 shift dayfeature (150) 天的五個「行情數目」加上下一天的開盤 共 751 個「數目」當作 feature X \n",
    "for i in range(0,xlength):\n",
    "    x[i,0:featurenum]=np.array(data[i:i+dayfeature] \\\n",
    "          [[u'收盘价',u'最高价',u'最低价',u'开盘价',u'成交量']]).reshape((1,featurenum))\n",
    "    x[i,featurenum]=data.iloc[i+dayfeature][u'开盘价']\n",
    "    x[i,featurenum+1]=data.index[i+dayfeature].value  # backward pd.to_datetime(value).date()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預測明天是否會「續漲」且「不破前低」? ochl \n",
    "for i in range(0,xlength):\n",
    "    y[i]=0              \n",
    "    i0h = float(data.iloc[i][u'最高价']) # i 這天的\n",
    "    i0l = float(data.iloc[i][u'最低价']) # i 這天的\n",
    "    i1h = float(data.iloc[i+1][u'最高价']) # i 明天的\n",
    "    i1l = float(data.iloc[i+1][u'最低价']) # i 明天的\n",
    "    if i1h > i0h and i1l > i0l :\n",
    "            y[i]=1\n",
    "y = np.int32(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1629"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 12,\n",
       " 13,\n",
       " 15,\n",
       " 19,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 31,\n",
       " 32,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 42,\n",
       " 48,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 58,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 70,\n",
       " 71,\n",
       " 74,\n",
       " 76,\n",
       " 78,\n",
       " 79,\n",
       " 85,\n",
       " 89,\n",
       " 99,\n",
       " 100,\n",
       " 107,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 127,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 138,\n",
       " 156,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 166,\n",
       " 169,\n",
       " 172,\n",
       " 174,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 202,\n",
       " 203,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 212,\n",
       " 213,\n",
       " 217,\n",
       " 221,\n",
       " 222,\n",
       " 227,\n",
       " 228,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 236,\n",
       " 237,\n",
       " 239,\n",
       " 240,\n",
       " 242,\n",
       " 245,\n",
       " 248,\n",
       " 252,\n",
       " 260,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 299,\n",
       " 304,\n",
       " 306,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 314,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 321,\n",
       " 322,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 330,\n",
       " 331,\n",
       " 335,\n",
       " 345,\n",
       " 347,\n",
       " 352,\n",
       " 356,\n",
       " 359,\n",
       " 366,\n",
       " 368,\n",
       " 369,\n",
       " 375,\n",
       " 377,\n",
       " 378,\n",
       " 381,\n",
       " 385,\n",
       " 389,\n",
       " 390,\n",
       " 392,\n",
       " 393,\n",
       " 396,\n",
       " 400,\n",
       " 407,\n",
       " 413,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 425,\n",
       " 426,\n",
       " 428,\n",
       " 433,\n",
       " 436,\n",
       " 446,\n",
       " 447,\n",
       " 449,\n",
       " 451,\n",
       " 455,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 463,\n",
       " 466,\n",
       " 467,\n",
       " 469,\n",
       " 470,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 478,\n",
       " 481,\n",
       " 486,\n",
       " 490,\n",
       " 494,\n",
       " 495,\n",
       " 497,\n",
       " 498,\n",
       " 501,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 511,\n",
       " 512,\n",
       " 514,\n",
       " 517,\n",
       " 522,\n",
       " 526,\n",
       " 527,\n",
       " 529,\n",
       " 544,\n",
       " 547,\n",
       " 548,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 569,\n",
       " 571,\n",
       " 586,\n",
       " 590,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 604,\n",
       " 605,\n",
       " 613,\n",
       " 616,\n",
       " 617,\n",
       " 620,\n",
       " 621,\n",
       " 625,\n",
       " 631,\n",
       " 637,\n",
       " 638,\n",
       " 642,\n",
       " 643,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 663,\n",
       " 664,\n",
       " 667,\n",
       " 669,\n",
       " 674,\n",
       " 675,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 688,\n",
       " 689,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 696,\n",
       " 702,\n",
       " 704,\n",
       " 709,\n",
       " 714,\n",
       " 715,\n",
       " 717,\n",
       " 719,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 736,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 744,\n",
       " 749,\n",
       " 754,\n",
       " 755,\n",
       " 758,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 767,\n",
       " 775,\n",
       " 776,\n",
       " 778,\n",
       " 787,\n",
       " 790,\n",
       " 795,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 803,\n",
       " 804,\n",
       " 809,\n",
       " 810,\n",
       " 813,\n",
       " 817,\n",
       " 823,\n",
       " 827,\n",
       " 828,\n",
       " 833,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 848,\n",
       " 850,\n",
       " 861,\n",
       " 862,\n",
       " 866,\n",
       " 870,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 881,\n",
       " 883,\n",
       " 884,\n",
       " 887,\n",
       " 895,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 901,\n",
       " 902,\n",
       " 906,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 918,\n",
       " 919,\n",
       " 921,\n",
       " 924,\n",
       " 925,\n",
       " 929,\n",
       " 930,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 942,\n",
       " 944,\n",
       " 945,\n",
       " 947,\n",
       " 948,\n",
       " 951,\n",
       " 952,\n",
       " 956,\n",
       " 959,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 986,\n",
       " 989,\n",
       " 992,\n",
       " 994,\n",
       " 997,\n",
       " 1001,\n",
       " 1003,\n",
       " 1006,\n",
       " 1007,\n",
       " 1008,\n",
       " 1010,\n",
       " 1014,\n",
       " 1026,\n",
       " 1027,\n",
       " 1030,\n",
       " 1031,\n",
       " 1033,\n",
       " 1035,\n",
       " 1039,\n",
       " 1040,\n",
       " 1042,\n",
       " 1044,\n",
       " 1046,\n",
       " 1047,\n",
       " 1048,\n",
       " 1052,\n",
       " 1054,\n",
       " 1055,\n",
       " 1059,\n",
       " 1072,\n",
       " 1073,\n",
       " 1090,\n",
       " 1094,\n",
       " 1095,\n",
       " 1096,\n",
       " 1097,\n",
       " 1098,\n",
       " 1101,\n",
       " 1104,\n",
       " 1105,\n",
       " 1108,\n",
       " 1109,\n",
       " 1110,\n",
       " 1111,\n",
       " 1115,\n",
       " 1116,\n",
       " 1119,\n",
       " 1120,\n",
       " 1125,\n",
       " 1131,\n",
       " 1133,\n",
       " 1134,\n",
       " 1138,\n",
       " 1142,\n",
       " 1143,\n",
       " 1146,\n",
       " 1147,\n",
       " 1152,\n",
       " 1157,\n",
       " 1162,\n",
       " 1170,\n",
       " 1172,\n",
       " 1177,\n",
       " 1179,\n",
       " 1180,\n",
       " 1181,\n",
       " 1182,\n",
       " 1191,\n",
       " 1195,\n",
       " 1196,\n",
       " 1198,\n",
       " 1202,\n",
       " 1203,\n",
       " 1204,\n",
       " 1206,\n",
       " 1222,\n",
       " 1223,\n",
       " 1230,\n",
       " 1231,\n",
       " 1232,\n",
       " 1238,\n",
       " 1243,\n",
       " 1255,\n",
       " 1259,\n",
       " 1260,\n",
       " 1266,\n",
       " 1268,\n",
       " 1269,\n",
       " 1286,\n",
       " 1287,\n",
       " 1289,\n",
       " 1295,\n",
       " 1298,\n",
       " 1299,\n",
       " 1301,\n",
       " 1302,\n",
       " 1311,\n",
       " 1314,\n",
       " 1315,\n",
       " 1317,\n",
       " 1318,\n",
       " 1320,\n",
       " 1324,\n",
       " 1326,\n",
       " 1328,\n",
       " 1331,\n",
       " 1333,\n",
       " 1334,\n",
       " 1336,\n",
       " 1344,\n",
       " 1345,\n",
       " 1355,\n",
       " 1358,\n",
       " 1359,\n",
       " 1363,\n",
       " 1365,\n",
       " 1366,\n",
       " 1373,\n",
       " 1375,\n",
       " 1391,\n",
       " 1395,\n",
       " 1396,\n",
       " 1398,\n",
       " 1400,\n",
       " 1401,\n",
       " 1408,\n",
       " 1409,\n",
       " 1410,\n",
       " 1411,\n",
       " 1412,\n",
       " 1414,\n",
       " 1424,\n",
       " 1425,\n",
       " 1430,\n",
       " 1435,\n",
       " 1437,\n",
       " 1438,\n",
       " 1439,\n",
       " 1443,\n",
       " 1444,\n",
       " 1446,\n",
       " 1450,\n",
       " 1452,\n",
       " 1457,\n",
       " 1463,\n",
       " 1466,\n",
       " 1467,\n",
       " 1469,\n",
       " 1472,\n",
       " 1478,\n",
       " 1482,\n",
       " 1487,\n",
       " 1499,\n",
       " 1504,\n",
       " 1506,\n",
       " 1514,\n",
       " 1517,\n",
       " 1518,\n",
       " 1519,\n",
       " 1521,\n",
       " 1526,\n",
       " 1529,\n",
       " 1532,\n",
       " 1541,\n",
       " 1543,\n",
       " 1548,\n",
       " 1549,\n",
       " 1552,\n",
       " 1553,\n",
       " 1556,\n",
       " 1559,\n",
       " 1560,\n",
       " 1561,\n",
       " 1562,\n",
       " 1564,\n",
       " 1565,\n",
       " 1567,\n",
       " 1568,\n",
       " 1569,\n",
       " 1572,\n",
       " 1573,\n",
       " 1580,\n",
       " 1581,\n",
       " 1582,\n",
       " 1583,\n",
       " 1584,\n",
       " 1589,\n",
       " 1591,\n",
       " 1592,\n",
       " 1593,\n",
       " 1596,\n",
       " 1597,\n",
       " 1598,\n",
       " 1601,\n",
       " 1604,\n",
       " 1607,\n",
       " 1612,\n",
       " 1618,\n",
       " 1619,\n",
       " 1625,\n",
       " 1626,\n",
       " 1627,\n",
       " 1628,\n",
       " 1631,\n",
       " 1632,\n",
       " 1635,\n",
       " 1639,\n",
       " 1641,\n",
       " 1642,\n",
       " 1645,\n",
       " 1649,\n",
       " 1652,\n",
       " 1657,\n",
       " 1658,\n",
       " 1661,\n",
       " 1664,\n",
       " 1665,\n",
       " 1668,\n",
       " 1669,\n",
       " 1671,\n",
       " 1673,\n",
       " 1674,\n",
       " 1690,\n",
       " 1700,\n",
       " 1704,\n",
       " 1705,\n",
       " 1716,\n",
       " 1717,\n",
       " 1718,\n",
       " 1719,\n",
       " 1720,\n",
       " 1723,\n",
       " 1729,\n",
       " 1730,\n",
       " 1734,\n",
       " 1735,\n",
       " 1742,\n",
       " 1746,\n",
       " 1750,\n",
       " 1752,\n",
       " 1758,\n",
       " 1759,\n",
       " 1763,\n",
       " 1766,\n",
       " 1767,\n",
       " 1780,\n",
       " 1781,\n",
       " 1790,\n",
       " 1792,\n",
       " 1794,\n",
       " 1798,\n",
       " 1803,\n",
       " 1810,\n",
       " 1811,\n",
       " 1813,\n",
       " 1816,\n",
       " 1817,\n",
       " 1818,\n",
       " 1827,\n",
       " 1828,\n",
       " 1829,\n",
       " 1833,\n",
       " 1840,\n",
       " 1842,\n",
       " 1845,\n",
       " 1849,\n",
       " 1851,\n",
       " 1860,\n",
       " 1863,\n",
       " 1864,\n",
       " 1868,\n",
       " 1871,\n",
       " 1874,\n",
       " 1875,\n",
       " 1881,\n",
       " 1882,\n",
       " 1884,\n",
       " 1886,\n",
       " 1890,\n",
       " 1894,\n",
       " 1896,\n",
       " 1897,\n",
       " 1900,\n",
       " 1906,\n",
       " 1907,\n",
       " 1918,\n",
       " 1919,\n",
       " 1921,\n",
       " 1923,\n",
       " 1925,\n",
       " 1926,\n",
       " 1927,\n",
       " 1928,\n",
       " 1929,\n",
       " 1930,\n",
       " 1932,\n",
       " 1933,\n",
       " 1936,\n",
       " 1938,\n",
       " 1943,\n",
       " 1944,\n",
       " 1945,\n",
       " 1946,\n",
       " 1948,\n",
       " 1953,\n",
       " 1956,\n",
       " 1957,\n",
       " 1958,\n",
       " 1960,\n",
       " 1961,\n",
       " 1967,\n",
       " 1968,\n",
       " 1970,\n",
       " 1971,\n",
       " 1975,\n",
       " 1978,\n",
       " 1986,\n",
       " 1990,\n",
       " 1997,\n",
       " 1998,\n",
       " 2002,\n",
       " 2007,\n",
       " 2011,\n",
       " 2014,\n",
       " 2016,\n",
       " 2022,\n",
       " 2023,\n",
       " 2026,\n",
       " 2028,\n",
       " 2030,\n",
       " 2031,\n",
       " 2035,\n",
       " 2038,\n",
       " 2039,\n",
       " 2040,\n",
       " 2044,\n",
       " 2055,\n",
       " 2056,\n",
       " 2059,\n",
       " 2064,\n",
       " 2067,\n",
       " 2070,\n",
       " 2071,\n",
       " 2072,\n",
       " 2074,\n",
       " 2075,\n",
       " 2080,\n",
       " 2081,\n",
       " 2084,\n",
       " 2088,\n",
       " 2089,\n",
       " 2090,\n",
       " 2091,\n",
       " 2093,\n",
       " 2095,\n",
       " 2098,\n",
       " 2099,\n",
       " 2100,\n",
       " 2101,\n",
       " 2104,\n",
       " 2105,\n",
       " 2109,\n",
       " 2112,\n",
       " 2113,\n",
       " 2114,\n",
       " 2116,\n",
       " 2117,\n",
       " 2118,\n",
       " 2120,\n",
       " 2124,\n",
       " 2130,\n",
       " 2131,\n",
       " 2138,\n",
       " 2141,\n",
       " 2143,\n",
       " 2146,\n",
       " 2150,\n",
       " 2152,\n",
       " 2153,\n",
       " 2155,\n",
       " 2160,\n",
       " 2164,\n",
       " 2165,\n",
       " 2167,\n",
       " 2168,\n",
       " 2169,\n",
       " 2170,\n",
       " 2173,\n",
       " 2174,\n",
       " 2177,\n",
       " 2179,\n",
       " 2183,\n",
       " 2185,\n",
       " 2186,\n",
       " 2187,\n",
       " 2194,\n",
       " 2200,\n",
       " 2205,\n",
       " 2209,\n",
       " 2210,\n",
       " 2213,\n",
       " 2216,\n",
       " 2219,\n",
       " 2224,\n",
       " 2226,\n",
       " 2227,\n",
       " 2228,\n",
       " 2229,\n",
       " 2233,\n",
       " 2234,\n",
       " 2235,\n",
       " 2236,\n",
       " 2240,\n",
       " 2241,\n",
       " 2242,\n",
       " 2243,\n",
       " 2246,\n",
       " 2249,\n",
       " 2250,\n",
       " 2251,\n",
       " 2252,\n",
       " 2253,\n",
       " 2255,\n",
       " 2256,\n",
       " 2259,\n",
       " 2260,\n",
       " 2261,\n",
       " 2262,\n",
       " 2266,\n",
       " 2268,\n",
       " 2269,\n",
       " 2277,\n",
       " 2278,\n",
       " 2284,\n",
       " 2285,\n",
       " 2286,\n",
       " 2289,\n",
       " 2290,\n",
       " 2291,\n",
       " 2295,\n",
       " 2297,\n",
       " 2298,\n",
       " 2299,\n",
       " 2302,\n",
       " 2307,\n",
       " 2308,\n",
       " 2309,\n",
       " 2310,\n",
       " 2313,\n",
       " 2317,\n",
       " 2322,\n",
       " 2323,\n",
       " 2325,\n",
       " 2326,\n",
       " 2336,\n",
       " 2337,\n",
       " 2338,\n",
       " 2341,\n",
       " 2343,\n",
       " 2344,\n",
       " 2345,\n",
       " 2351,\n",
       " 2352,\n",
       " 2358,\n",
       " 2361,\n",
       " 2363,\n",
       " 2367,\n",
       " 2368,\n",
       " 2369,\n",
       " 2371,\n",
       " 2372,\n",
       " 2373,\n",
       " 2377,\n",
       " 2378,\n",
       " 2379,\n",
       " 2381,\n",
       " 2382,\n",
       " 2383,\n",
       " 2384,\n",
       " 2389,\n",
       " 2391,\n",
       " 2392,\n",
       " 2395,\n",
       " 2398,\n",
       " 2399,\n",
       " 2406,\n",
       " 2407,\n",
       " 2408,\n",
       " 2412,\n",
       " 2413,\n",
       " 2417,\n",
       " 2421,\n",
       " 2422,\n",
       " 2423,\n",
       " 2424,\n",
       " 2425,\n",
       " 2433,\n",
       " 2434,\n",
       " 2435,\n",
       " 2438,\n",
       " 2439,\n",
       " 2444,\n",
       " 2445,\n",
       " 2446,\n",
       " 2448,\n",
       " 2449,\n",
       " 2452,\n",
       " 2456,\n",
       " 2458,\n",
       " 2459,\n",
       " 2460,\n",
       " 2463,\n",
       " 2467,\n",
       " 2469,\n",
       " 2470,\n",
       " 2471,\n",
       " 2472,\n",
       " 2473,\n",
       " 2475,\n",
       " 2476,\n",
       " 2478,\n",
       " 2479,\n",
       " 2482,\n",
       " 2483,\n",
       " 2486,\n",
       " 2493,\n",
       " 2497,\n",
       " 2500,\n",
       " 2502,\n",
       " 2504,\n",
       " 2505,\n",
       " 2506,\n",
       " 2511,\n",
       " 2512,\n",
       " 2514,\n",
       " 2515,\n",
       " 2521,\n",
       " 2528,\n",
       " 2529,\n",
       " 2531,\n",
       " 2538,\n",
       " 2539,\n",
       " 2542,\n",
       " 2550,\n",
       " 2551,\n",
       " 2554,\n",
       " 2555,\n",
       " 2556,\n",
       " 2557,\n",
       " 2559,\n",
       " 2563,\n",
       " 2568,\n",
       " 2572,\n",
       " 2573,\n",
       " 2575,\n",
       " 2576,\n",
       " 2587,\n",
       " 2589,\n",
       " 2590,\n",
       " 2592,\n",
       " 2596,\n",
       " 2598,\n",
       " 2599,\n",
       " 2600,\n",
       " 2605,\n",
       " 2609,\n",
       " 2612,\n",
       " 2613,\n",
       " 2614,\n",
       " 2625,\n",
       " 2626,\n",
       " 2630,\n",
       " 2631,\n",
       " 2637,\n",
       " 2640,\n",
       " 2641,\n",
       " 2651,\n",
       " 2652,\n",
       " 2654,\n",
       " 2655,\n",
       " 2656,\n",
       " 2660,\n",
       " 2663,\n",
       " 2666,\n",
       " 2667,\n",
       " 2668,\n",
       " 2674,\n",
       " 2676,\n",
       " 2679,\n",
       " 2680,\n",
       " 2682,\n",
       " 2683,\n",
       " 2685,\n",
       " 2686,\n",
       " 2687,\n",
       " 2688,\n",
       " 2691,\n",
       " 2694,\n",
       " 2696,\n",
       " 2697,\n",
       " 2699,\n",
       " 2701,\n",
       " 2702,\n",
       " 2703,\n",
       " 2707,\n",
       " 2708,\n",
       " 2710,\n",
       " 2717,\n",
       " 2718,\n",
       " 2719,\n",
       " 2720,\n",
       " 2721,\n",
       " 2724,\n",
       " 2725,\n",
       " 2727,\n",
       " 2731,\n",
       " 2732,\n",
       " 2734,\n",
       " 2735,\n",
       " 2737,\n",
       " 2738,\n",
       " 2740,\n",
       " 2742,\n",
       " 2743,\n",
       " 2747,\n",
       " 2748,\n",
       " 2749,\n",
       " 2755,\n",
       " 2756,\n",
       " 2760,\n",
       " 2761,\n",
       " 2762,\n",
       " 2764,\n",
       " 2765,\n",
       " 2766,\n",
       " 2768,\n",
       " 2769,\n",
       " 2772,\n",
       " 2773,\n",
       " 2774,\n",
       " 2781,\n",
       " 2782,\n",
       " 2784,\n",
       " 2786,\n",
       " 2793,\n",
       " 2794,\n",
       " 2796,\n",
       " 2797,\n",
       " 2798,\n",
       " 2802,\n",
       " 2805,\n",
       " ...]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ i for i in range(xlength) if y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(  ) x :> shape tib. \\ ==> (4483, 752) (<class 'tuple'>)\n",
      "(  ) x :> shape[0] tib. \\ ==> 4483 (<class 'int'>)\n",
      "(  ) y :> shape tib. \\ ==> (4483,) (<class 'tuple'>)\n",
      "(  ) y :> shape[0] tib. \\ ==> 4483 (<class 'int'>)\n",
      "(  ) y py> sum(pop()) tib. \\ ==> 1629 (<class 'numpy.int32'>)\n"
     ]
    }
   ],
   "source": [
    "%f (  ) x :> shape tib. ( ?, 751 )\n",
    "%f (  ) x :> shape[0] tib. ( ? )\n",
    "%f (  ) y :> shape tib. ( ?, )    \n",
    "%f (  ) y :> shape[0] tib. ( ? )    \n",
    "%f (  ) y py> sum(pop()) tib. ( 1629 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接看【涨跌幅】> 9.8 就算漲停板，從【漲停板】的日子往後多取 oneshot 天。\n",
    "# 符合條件的日子組成 dataset 其中含有 oneshot 天慣性，使正反兩方平衡。\n",
    "# [ ] 找出如何消除重複的方法, 這個 loop 就簡單了。回想起來了，用 set() 是取 unique 的好方法。\n",
    "\n",
    "x_mask = np.zeros((xlength,)) # 全部放 0 以下只針對目標填 1 \n",
    "x_mask = np.int32(x_mask)\n",
    "for i in range(xlength):\n",
    "    if float(data.iloc[i+dayfeature-1][u'涨跌幅'])>=9.8:\n",
    "        for j in range(oneshot):\n",
    "            x_mask[i+j-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_mask tib. \\ ==> [0 0 0 ..., 0 0 0] (<class 'numpy.ndarray'>)\n",
      "x_mask :> shape tib. \\ ==> (4483,) (<class 'tuple'>)\n",
      "x_mask py> sum(pop()) tib. \\ ==> 350 (<class 'numpy.int32'>)\n"
     ]
    }
   ],
   "source": [
    "%f x_mask tib.\n",
    "%f x_mask :> shape tib.\n",
    "%f x_mask py> sum(pop()) tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_original = []\n",
    "y_original = []\n",
    "for i in range(x_mask.shape[0]):\n",
    "    if x_mask[i]:\n",
    "        x_original.append(x[i])\n",
    "        y_original.append(y[i])\n",
    "x,x_original = x_original,x\n",
    "y,y_original = y_original,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4483, 752)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_original).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350,)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_selected).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1999-05-24'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(pd.to_datetime(x_selected[0][751]).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2000-03-02',\n",
       " '2000-03-03',\n",
       " '2000-03-15',\n",
       " '2000-03-16',\n",
       " '2000-03-17',\n",
       " '2000-03-20',\n",
       " '2000-03-21',\n",
       " '2000-05-08',\n",
       " '2000-05-09',\n",
       " '2000-05-10']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(pd.to_datetime(x_selected[i][751]).date()) for i in range(len(x_selected))][30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "日期\n",
       "2000-05-08       10.0\n",
       "2000-05-09     8.6475\n",
       "2000-05-10    -4.4218\n",
       "2000-05-11     4.1281\n",
       "2000-05-12    -0.2051\n",
       "2000-05-15     1.1644\n",
       "2000-05-16     1.6249\n",
       "2000-05-17      7.062\n",
       "2000-05-18     2.2402\n",
       "2000-05-19     1.0347\n",
       "Name: 涨跌幅, dtype: object"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.query('index >= \"2000-5-8\"')[u'涨跌幅'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>股票代码</th>\n",
       "      <th>名称</th>\n",
       "      <th>收盘价</th>\n",
       "      <th>最高价</th>\n",
       "      <th>最低价</th>\n",
       "      <th>开盘价</th>\n",
       "      <th>前收盘</th>\n",
       "      <th>涨跌额</th>\n",
       "      <th>涨跌幅</th>\n",
       "      <th>换手率</th>\n",
       "      <th>成交量</th>\n",
       "      <th>成交金额</th>\n",
       "      <th>总市值</th>\n",
       "      <th>流通市值</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>日期</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1997-07-11</th>\n",
       "      <td>'000777</td>\n",
       "      <td>中核苏阀</td>\n",
       "      <td>9.97</td>\n",
       "      <td>9.97</td>\n",
       "      <td>8.48</td>\n",
       "      <td>8.75</td>\n",
       "      <td>9.06</td>\n",
       "      <td>0.91</td>\n",
       "      <td>10.0442</td>\n",
       "      <td>42.3227</td>\n",
       "      <td>11427140</td>\n",
       "      <td>1.085246e+08</td>\n",
       "      <td>747750000.0</td>\n",
       "      <td>269190000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-07-14</th>\n",
       "      <td>'000777</td>\n",
       "      <td>中核苏阀</td>\n",
       "      <td>10.97</td>\n",
       "      <td>10.97</td>\n",
       "      <td>10.33</td>\n",
       "      <td>10.35</td>\n",
       "      <td>9.97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0301</td>\n",
       "      <td>46.358</td>\n",
       "      <td>12516652</td>\n",
       "      <td>1.350109e+08</td>\n",
       "      <td>822750000.0</td>\n",
       "      <td>296190000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-07-15</th>\n",
       "      <td>'000777</td>\n",
       "      <td>中核苏阀</td>\n",
       "      <td>11.53</td>\n",
       "      <td>11.80</td>\n",
       "      <td>10.81</td>\n",
       "      <td>11.30</td>\n",
       "      <td>10.97</td>\n",
       "      <td>0.56</td>\n",
       "      <td>5.1048</td>\n",
       "      <td>31.2442</td>\n",
       "      <td>8435941</td>\n",
       "      <td>9.504755e+07</td>\n",
       "      <td>864750000.0</td>\n",
       "      <td>311310000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-07-16</th>\n",
       "      <td>'000777</td>\n",
       "      <td>中核苏阀</td>\n",
       "      <td>10.98</td>\n",
       "      <td>11.50</td>\n",
       "      <td>10.90</td>\n",
       "      <td>11.49</td>\n",
       "      <td>11.53</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-4.7702</td>\n",
       "      <td>16.7164</td>\n",
       "      <td>4513434</td>\n",
       "      <td>5.010548e+07</td>\n",
       "      <td>823500000.0</td>\n",
       "      <td>296460000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-07-17</th>\n",
       "      <td>'000777</td>\n",
       "      <td>中核苏阀</td>\n",
       "      <td>10.99</td>\n",
       "      <td>11.26</td>\n",
       "      <td>10.52</td>\n",
       "      <td>10.81</td>\n",
       "      <td>10.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0911</td>\n",
       "      <td>12.9689</td>\n",
       "      <td>3501611</td>\n",
       "      <td>3.830279e+07</td>\n",
       "      <td>824250000.0</td>\n",
       "      <td>296730000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-07-18</th>\n",
       "      <td>'000777</td>\n",
       "      <td>中核苏阀</td>\n",
       "      <td>11.10</td>\n",
       "      <td>11.12</td>\n",
       "      <td>10.50</td>\n",
       "      <td>10.80</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.0009</td>\n",
       "      <td>9.4632</td>\n",
       "      <td>2555060</td>\n",
       "      <td>2.768096e+07</td>\n",
       "      <td>832500000.0</td>\n",
       "      <td>299700000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-07-21</th>\n",
       "      <td>'000777</td>\n",
       "      <td>中核苏阀</td>\n",
       "      <td>11.20</td>\n",
       "      <td>11.78</td>\n",
       "      <td>11.08</td>\n",
       "      <td>11.28</td>\n",
       "      <td>11.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9009</td>\n",
       "      <td>14.888</td>\n",
       "      <td>4019750</td>\n",
       "      <td>4.607238e+07</td>\n",
       "      <td>840000000.0</td>\n",
       "      <td>302400000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-07-22</th>\n",
       "      <td>'000777</td>\n",
       "      <td>中核苏阀</td>\n",
       "      <td>11.96</td>\n",
       "      <td>12.10</td>\n",
       "      <td>11.23</td>\n",
       "      <td>11.23</td>\n",
       "      <td>11.20</td>\n",
       "      <td>0.76</td>\n",
       "      <td>6.7857</td>\n",
       "      <td>23.5468</td>\n",
       "      <td>6357648</td>\n",
       "      <td>7.452792e+07</td>\n",
       "      <td>897000000.0</td>\n",
       "      <td>322920000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-07-23</th>\n",
       "      <td>'000777</td>\n",
       "      <td>中核苏阀</td>\n",
       "      <td>11.66</td>\n",
       "      <td>12.25</td>\n",
       "      <td>11.51</td>\n",
       "      <td>12.10</td>\n",
       "      <td>11.96</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-2.5084</td>\n",
       "      <td>15.6519</td>\n",
       "      <td>4226004</td>\n",
       "      <td>5.035515e+07</td>\n",
       "      <td>874500000.0</td>\n",
       "      <td>314820000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-07-24</th>\n",
       "      <td>'000777</td>\n",
       "      <td>中核苏阀</td>\n",
       "      <td>11.59</td>\n",
       "      <td>11.85</td>\n",
       "      <td>11.50</td>\n",
       "      <td>11.69</td>\n",
       "      <td>11.66</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.6003</td>\n",
       "      <td>7.4968</td>\n",
       "      <td>2024134</td>\n",
       "      <td>2.364517e+07</td>\n",
       "      <td>869250000.0</td>\n",
       "      <td>312930000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               股票代码    名称    收盘价    最高价    最低价    开盘价    前收盘    涨跌额      涨跌幅  \\\n",
       "日期                                                                             \n",
       "1997-07-11  '000777  中核苏阀   9.97   9.97   8.48   8.75   9.06   0.91  10.0442   \n",
       "1997-07-14  '000777  中核苏阀  10.97  10.97  10.33  10.35   9.97    1.0  10.0301   \n",
       "1997-07-15  '000777  中核苏阀  11.53  11.80  10.81  11.30  10.97   0.56   5.1048   \n",
       "1997-07-16  '000777  中核苏阀  10.98  11.50  10.90  11.49  11.53  -0.55  -4.7702   \n",
       "1997-07-17  '000777  中核苏阀  10.99  11.26  10.52  10.81  10.98   0.01   0.0911   \n",
       "1997-07-18  '000777  中核苏阀  11.10  11.12  10.50  10.80  10.99   0.11   1.0009   \n",
       "1997-07-21  '000777  中核苏阀  11.20  11.78  11.08  11.28  11.10    0.1   0.9009   \n",
       "1997-07-22  '000777  中核苏阀  11.96  12.10  11.23  11.23  11.20   0.76   6.7857   \n",
       "1997-07-23  '000777  中核苏阀  11.66  12.25  11.51  12.10  11.96   -0.3  -2.5084   \n",
       "1997-07-24  '000777  中核苏阀  11.59  11.85  11.50  11.69  11.66  -0.07  -0.6003   \n",
       "\n",
       "                换手率       成交量          成交金额          总市值         流通市值  \n",
       "日期                                                                     \n",
       "1997-07-11  42.3227  11427140  1.085246e+08  747750000.0  269190000.0  \n",
       "1997-07-14   46.358  12516652  1.350109e+08  822750000.0  296190000.0  \n",
       "1997-07-15  31.2442   8435941  9.504755e+07  864750000.0  311310000.0  \n",
       "1997-07-16  16.7164   4513434  5.010548e+07  823500000.0  296460000.0  \n",
       "1997-07-17  12.9689   3501611  3.830279e+07  824250000.0  296730000.0  \n",
       "1997-07-18   9.4632   2555060  2.768096e+07  832500000.0  299700000.0  \n",
       "1997-07-21   14.888   4019750  4.607238e+07  840000000.0  302400000.0  \n",
       "1997-07-22  23.5468   6357648  7.452792e+07  897000000.0  322920000.0  \n",
       "1997-07-23  15.6519   4226004  5.035515e+07  874500000.0  314820000.0  \n",
       "1997-07-24   7.4968   2024134  2.364517e+07  869250000.0  312930000.0  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hcche\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280, 752)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280, 751)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train)[:,:751].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.02200000e+01,   1.05900000e+01,   1.02000000e+01, ...,\n",
       "          9.30000000e+00,   8.15290000e+05,   8.91000000e+00],\n",
       "       [  3.23900000e+01,   3.27000000e+01,   3.15700000e+01, ...,\n",
       "          2.65600000e+01,   3.48851600e+07,   2.78500000e+01]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train)[:,:751][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  1.02200000e+01,   1.05900000e+01,   1.02000000e+01,\n",
       "          1.04600000e+01,   7.54514000e+05,   1.02800000e+01,\n",
       "          1.03200000e+01,   1.01200000e+01,   1.01400000e+01,\n",
       "          4.27917000e+05,   1.00300000e+01,   1.03200000e+01,\n",
       "          1.00000000e+01,   1.03000000e+01,   4.26100000e+05,\n",
       "          1.00600000e+01,   1.01800000e+01,   9.77000000e+00,\n",
       "          9.98000000e+00,   9.57440000e+05,   1.00500000e+01,\n",
       "          1.01500000e+01,   9.96000000e+00,   1.00600000e+01,\n",
       "          2.46900000e+05,   9.99000000e+00,   1.01400000e+01,\n",
       "          9.90000000e+00,   1.01000000e+01,   3.25625000e+05,\n",
       "          9.98000000e+00,   1.02000000e+01,   9.96000000e+00,\n",
       "          1.01000000e+01,   2.87100000e+05,   1.01200000e+01,\n",
       "          1.01300000e+01,   9.98000000e+00,   9.98000000e+00,\n",
       "          2.05302000e+05,   1.01000000e+01,   1.01600000e+01,\n",
       "          1.00400000e+01,   1.01500000e+01,   1.36740000e+05,\n",
       "          9.87000000e+00,   1.01500000e+01,   9.80000000e+00,\n",
       "          1.01500000e+01,   2.05480000e+05,   9.92000000e+00,\n",
       "          9.96000000e+00,   9.70000000e+00,   9.87000000e+00,\n",
       "          2.62830000e+05,   9.67000000e+00,   1.01000000e+01,\n",
       "          9.65000000e+00,   1.01000000e+01,   2.70200000e+05,\n",
       "          9.65000000e+00,   9.67000000e+00,   9.40000000e+00,\n",
       "          9.67000000e+00,   5.42408000e+05,   9.41000000e+00,\n",
       "          9.69000000e+00,   9.41000000e+00,   9.66000000e+00,\n",
       "          2.33280000e+05,   9.20000000e+00,   9.40000000e+00,\n",
       "          9.20000000e+00,   9.40000000e+00,   2.75500000e+05,\n",
       "          9.12000000e+00,   9.30000000e+00,   9.01000000e+00,\n",
       "          9.10000000e+00,   1.87788000e+05,   9.11000000e+00,\n",
       "          9.19000000e+00,   9.02000000e+00,   9.05000000e+00,\n",
       "          1.30260000e+05,   9.58000000e+00,   9.60000000e+00,\n",
       "          9.15000000e+00,   9.28000000e+00,   3.33927000e+05,\n",
       "          9.37000000e+00,   9.60000000e+00,   9.36000000e+00,\n",
       "          9.60000000e+00,   2.69480000e+05,   9.29000000e+00,\n",
       "          9.40000000e+00,   9.25000000e+00,   9.30000000e+00,\n",
       "          1.62436000e+05,   9.39000000e+00,   9.48000000e+00,\n",
       "          9.30000000e+00,   9.48000000e+00,   1.20660000e+05,\n",
       "          9.26000000e+00,   9.44000000e+00,   9.25000000e+00,\n",
       "          9.40000000e+00,   1.43480000e+05,   9.06000000e+00,\n",
       "          9.26000000e+00,   9.06000000e+00,   9.26000000e+00,\n",
       "          2.81700000e+05,   9.04000000e+00,   9.20000000e+00,\n",
       "          8.73000000e+00,   9.11000000e+00,   3.37840000e+05,\n",
       "          8.92000000e+00,   9.10000000e+00,   8.87000000e+00,\n",
       "          9.10000000e+00,   1.95692000e+05,   8.65000000e+00,\n",
       "          8.81000000e+00,   8.65000000e+00,   8.80000000e+00,\n",
       "          2.37720000e+05,   8.48000000e+00,   8.63000000e+00,\n",
       "          8.35000000e+00,   8.50000000e+00,   2.64660000e+05,\n",
       "          8.57000000e+00,   8.69000000e+00,   8.48000000e+00,\n",
       "          8.58000000e+00,   2.06850000e+05,   8.58000000e+00,\n",
       "          8.62000000e+00,   8.40000000e+00,   8.57000000e+00,\n",
       "          1.46400000e+05,   8.99000000e+00,   9.01000000e+00,\n",
       "          8.58000000e+00,   8.58000000e+00,   3.81255000e+05,\n",
       "          9.05000000e+00,   9.08000000e+00,   8.90000000e+00,\n",
       "          9.05000000e+00,   2.54290000e+05,   8.85000000e+00,\n",
       "          9.12000000e+00,   8.80000000e+00,   9.09000000e+00,\n",
       "          2.38120000e+05,   8.72000000e+00,   8.80000000e+00,\n",
       "          8.70000000e+00,   8.79000000e+00,   2.76342000e+05,\n",
       "          8.70000000e+00,   8.98000000e+00,   8.53000000e+00,\n",
       "          8.65000000e+00,   1.77250000e+05,   8.56000000e+00,\n",
       "          8.78000000e+00,   8.51000000e+00,   8.75000000e+00,\n",
       "          2.54900000e+05,   8.99000000e+00,   8.99000000e+00,\n",
       "          8.48000000e+00,   8.50000000e+00,   4.09019000e+05,\n",
       "          9.58000000e+00,   9.72000000e+00,   9.13000000e+00,\n",
       "          9.13000000e+00,   1.60906600e+06,   9.41000000e+00,\n",
       "          9.65000000e+00,   9.30000000e+00,   9.52000000e+00,\n",
       "          6.12726000e+05,   9.70000000e+00,   9.71000000e+00,\n",
       "          9.35000000e+00,   9.37000000e+00,   5.45095000e+05,\n",
       "          9.60000000e+00,   9.94000000e+00,   9.55000000e+00,\n",
       "          9.71000000e+00,   6.43022000e+05,   9.35000000e+00,\n",
       "          9.63000000e+00,   9.31000000e+00,   9.60000000e+00,\n",
       "          4.72955000e+05,   9.17000000e+00,   9.30000000e+00,\n",
       "          9.12000000e+00,   9.30000000e+00,   3.34735000e+05,\n",
       "          9.28000000e+00,   9.35000000e+00,   9.14000000e+00,\n",
       "          9.15000000e+00,   2.62330000e+05,   9.12000000e+00,\n",
       "          9.37000000e+00,   9.11000000e+00,   9.26000000e+00,\n",
       "          2.24975000e+05,   8.75000000e+00,   9.17000000e+00,\n",
       "          8.68000000e+00,   9.17000000e+00,   4.29768000e+05,\n",
       "          8.81000000e+00,   8.87000000e+00,   8.68000000e+00,\n",
       "          8.70000000e+00,   1.83291000e+05,   8.80000000e+00,\n",
       "          8.92000000e+00,   8.80000000e+00,   8.87000000e+00,\n",
       "          1.24813000e+05,   8.70000000e+00,   8.81000000e+00,\n",
       "          8.70000000e+00,   8.81000000e+00,   1.70300000e+05,\n",
       "          8.43000000e+00,   8.60000000e+00,   8.42000000e+00,\n",
       "          8.60000000e+00,   3.12510000e+05,   8.20000000e+00,\n",
       "          8.40000000e+00,   8.18000000e+00,   8.35000000e+00,\n",
       "          2.39580000e+05,   8.40000000e+00,   8.41000000e+00,\n",
       "          8.26000000e+00,   8.26000000e+00,   2.05720000e+05,\n",
       "          8.51000000e+00,   8.52000000e+00,   8.38000000e+00,\n",
       "          8.50000000e+00,   1.49700000e+05,   8.44000000e+00,\n",
       "          8.51000000e+00,   8.40000000e+00,   8.48000000e+00,\n",
       "          1.50664000e+05,   8.77000000e+00,   8.78000000e+00,\n",
       "          8.46000000e+00,   8.46000000e+00,   2.01570000e+05,\n",
       "          8.74000000e+00,   8.92000000e+00,   8.66000000e+00,\n",
       "          8.77000000e+00,   1.96174000e+05,   8.83000000e+00,\n",
       "          8.89000000e+00,   8.68000000e+00,   8.70000000e+00,\n",
       "          2.34060000e+05,   8.90000000e+00,   8.90000000e+00,\n",
       "          8.60000000e+00,   8.80000000e+00,   3.10660000e+05,\n",
       "          8.96000000e+00,   9.15000000e+00,   8.91000000e+00,\n",
       "          8.91000000e+00,   4.64196000e+05,   8.75000000e+00,\n",
       "          8.96000000e+00,   8.69000000e+00,   8.96000000e+00,\n",
       "          2.61520000e+05,   8.70000000e+00,   8.96000000e+00,\n",
       "          8.60000000e+00,   8.78000000e+00,   7.63760000e+05,\n",
       "          8.95000000e+00,   8.96000000e+00,   8.65000000e+00,\n",
       "          8.74000000e+00,   8.34274000e+05,   8.98000000e+00,\n",
       "          9.14000000e+00,   8.90000000e+00,   9.00000000e+00,\n",
       "          7.11528000e+05,   8.85000000e+00,   8.96000000e+00,\n",
       "          8.74000000e+00,   8.96000000e+00,   5.24350000e+05,\n",
       "          8.68000000e+00,   8.90000000e+00,   8.67000000e+00,\n",
       "          8.90000000e+00,   5.23384000e+05,   8.53000000e+00,\n",
       "          8.63000000e+00,   8.49000000e+00,   8.51000000e+00,\n",
       "          4.86640000e+05,   8.71000000e+00,   8.74000000e+00,\n",
       "          8.45000000e+00,   8.53000000e+00,   4.36476000e+05,\n",
       "          8.56000000e+00,   8.74000000e+00,   8.54000000e+00,\n",
       "          8.70000000e+00,   5.78632000e+05,   8.51000000e+00,\n",
       "          8.58000000e+00,   8.47000000e+00,   8.56000000e+00,\n",
       "          3.77600000e+05,   8.61000000e+00,   8.77000000e+00,\n",
       "          8.50000000e+00,   8.50000000e+00,   3.16200000e+05,\n",
       "          8.47000000e+00,   8.58000000e+00,   8.45000000e+00,\n",
       "          8.58000000e+00,   3.12262000e+05,   8.44000000e+00,\n",
       "          8.55000000e+00,   8.41000000e+00,   8.42000000e+00,\n",
       "          2.81050000e+05,   8.55000000e+00,   8.60000000e+00,\n",
       "          8.34000000e+00,   8.41000000e+00,   3.69453000e+05,\n",
       "          8.44000000e+00,   8.59000000e+00,   8.43000000e+00,\n",
       "          8.55000000e+00,   1.71000000e+05,   8.39000000e+00,\n",
       "          8.46000000e+00,   8.35000000e+00,   8.40000000e+00,\n",
       "          3.22763000e+05,   8.44000000e+00,   8.50000000e+00,\n",
       "          8.38000000e+00,   8.38000000e+00,   3.24310000e+05,\n",
       "          8.67000000e+00,   8.70000000e+00,   8.34000000e+00,\n",
       "          8.44000000e+00,   5.79078000e+05,   8.60000000e+00,\n",
       "          8.72000000e+00,   8.51000000e+00,   8.70000000e+00,\n",
       "          4.30960000e+05,   8.79000000e+00,   8.80000000e+00,\n",
       "          8.55000000e+00,   8.60000000e+00,   8.18254000e+05,\n",
       "          8.66000000e+00,   8.85000000e+00,   8.61000000e+00,\n",
       "          8.85000000e+00,   3.65790000e+05,   8.46000000e+00,\n",
       "          8.68000000e+00,   8.43000000e+00,   8.66000000e+00,\n",
       "          6.33400000e+05,   8.56000000e+00,   8.66000000e+00,\n",
       "          8.35000000e+00,   8.45000000e+00,   9.20260000e+05,\n",
       "          8.38000000e+00,   8.58000000e+00,   8.36000000e+00,\n",
       "          8.55000000e+00,   5.24525000e+05,   8.25000000e+00,\n",
       "          8.36000000e+00,   8.24000000e+00,   8.36000000e+00,\n",
       "          3.98480000e+05,   8.30000000e+00,   8.45000000e+00,\n",
       "          8.19000000e+00,   8.25000000e+00,   3.40678000e+05,\n",
       "          8.14000000e+00,   8.40000000e+00,   8.13000000e+00,\n",
       "          8.35000000e+00,   3.58400000e+05,   8.17000000e+00,\n",
       "          8.25000000e+00,   7.80000000e+00,   7.90000000e+00,\n",
       "          6.53524000e+05,   8.22000000e+00,   8.26000000e+00,\n",
       "          8.00000000e+00,   8.20000000e+00,   3.95040000e+05,\n",
       "          8.31000000e+00,   8.57000000e+00,   8.26000000e+00,\n",
       "          8.26000000e+00,   1.04873000e+06,   8.14000000e+00,\n",
       "          8.23000000e+00,   7.90000000e+00,   8.10000000e+00,\n",
       "          5.45364000e+05,   8.14000000e+00,   8.25000000e+00,\n",
       "          8.06000000e+00,   8.20000000e+00,   2.81702000e+05,\n",
       "          7.90000000e+00,   8.24000000e+00,   7.90000000e+00,\n",
       "          8.00000000e+00,   2.86040000e+05,   7.60000000e+00,\n",
       "          7.88000000e+00,   7.58000000e+00,   7.82000000e+00,\n",
       "          3.54800000e+05,   7.63000000e+00,   7.80000000e+00,\n",
       "          7.58000000e+00,   7.58000000e+00,   2.50300000e+05,\n",
       "          7.77000000e+00,   7.78000000e+00,   7.60000000e+00,\n",
       "          7.70000000e+00,   2.11520000e+05,   7.81000000e+00,\n",
       "          7.86000000e+00,   7.66000000e+00,   7.77000000e+00,\n",
       "          2.29626000e+05,   7.60000000e+00,   7.83000000e+00,\n",
       "          7.58000000e+00,   7.83000000e+00,   2.22220000e+05,\n",
       "          7.66000000e+00,   7.67000000e+00,   7.55000000e+00,\n",
       "          7.57000000e+00,   1.68230000e+05,   7.95000000e+00,\n",
       "          8.00000000e+00,   7.62000000e+00,   7.70000000e+00,\n",
       "          4.04840000e+05,   7.77000000e+00,   7.99000000e+00,\n",
       "          7.71000000e+00,   7.95000000e+00,   1.49140000e+05,\n",
       "          7.26000000e+00,   7.68000000e+00,   7.25000000e+00,\n",
       "          7.50000000e+00,   4.18800000e+05,   7.45000000e+00,\n",
       "          7.50000000e+00,   7.26000000e+00,   7.30000000e+00,\n",
       "          3.44950000e+05,   7.35000000e+00,   7.47000000e+00,\n",
       "          7.33000000e+00,   7.47000000e+00,   2.29060000e+05,\n",
       "          7.22000000e+00,   7.39000000e+00,   7.20000000e+00,\n",
       "          7.35000000e+00,   1.53440000e+05,   6.96000000e+00,\n",
       "          7.22000000e+00,   6.93000000e+00,   7.22000000e+00,\n",
       "          3.24220000e+05,   6.79000000e+00,   6.99000000e+00,\n",
       "          6.65000000e+00,   6.70000000e+00,   1.89060000e+05,\n",
       "          6.79000000e+00,   6.92000000e+00,   6.73000000e+00,\n",
       "          6.80000000e+00,   1.45306000e+05,   7.20000000e+00,\n",
       "          7.20000000e+00,   6.70000000e+00,   6.80000000e+00,\n",
       "          4.67679000e+05,   7.43000000e+00,   7.45000000e+00,\n",
       "          7.24000000e+00,   7.25000000e+00,   6.02975000e+05,\n",
       "          7.44000000e+00,   7.58000000e+00,   7.34000000e+00,\n",
       "          7.49000000e+00,   6.18979000e+05,   8.18000000e+00,\n",
       "          8.18000000e+00,   7.21000000e+00,   7.39000000e+00,\n",
       "          2.37026100e+06,   7.92000000e+00,   8.60000000e+00,\n",
       "          7.80000000e+00,   8.45000000e+00,   2.29543500e+06,\n",
       "          8.15000000e+00,   8.41000000e+00,   7.81000000e+00,\n",
       "          7.88000000e+00,   1.40773800e+06,   8.27000000e+00,\n",
       "          8.42000000e+00,   8.10000000e+00,   8.18000000e+00,\n",
       "          1.27398800e+06,   8.09000000e+00,   8.38000000e+00,\n",
       "          7.90000000e+00,   8.30000000e+00,   9.87951000e+05,\n",
       "          7.90000000e+00,   8.20000000e+00,   7.79000000e+00,\n",
       "          8.10000000e+00,   9.29870000e+05,   8.04000000e+00,\n",
       "          8.06000000e+00,   7.65000000e+00,   7.89000000e+00,\n",
       "          6.90422000e+05,   8.13000000e+00,   8.38000000e+00,\n",
       "          8.01000000e+00,   8.04000000e+00,   1.28735100e+06,\n",
       "          7.84000000e+00,   8.12000000e+00,   7.71000000e+00,\n",
       "          8.12000000e+00,   4.96465000e+05,   7.98000000e+00,\n",
       "          8.01000000e+00,   7.75000000e+00,   7.85000000e+00,\n",
       "          4.28556000e+05,   8.27000000e+00,   8.28000000e+00,\n",
       "          7.79000000e+00,   7.98000000e+00,   9.68445000e+05,\n",
       "          8.68000000e+00,   8.70000000e+00,   8.10000000e+00,\n",
       "          8.37000000e+00,   2.15813800e+06,   8.80000000e+00,\n",
       "          8.86000000e+00,   8.53000000e+00,   8.68000000e+00,\n",
       "          2.90898100e+06,   8.64000000e+00,   9.00000000e+00,\n",
       "          8.60000000e+00,   8.90000000e+00,   1.78238200e+06,\n",
       "          9.50000000e+00,   9.50000000e+00,   8.51000000e+00,\n",
       "          8.64000000e+00,   5.65670300e+06,   1.04500000e+01,\n",
       "          1.04500000e+01,   9.98000000e+00,   9.99000000e+00,\n",
       "          8.58445800e+06,   9.60000000e+00,   1.10000000e+01,\n",
       "          9.45000000e+00,   1.09600000e+01,   5.51323100e+06,\n",
       "          1.03700000e+01,   1.05000000e+01,   9.61000000e+00,\n",
       "          9.61000000e+00,   3.10885800e+06,   1.02700000e+01,\n",
       "          1.05500000e+01,   9.90000000e+00,   1.05000000e+01,\n",
       "          2.53352200e+06,   1.03800000e+01,   1.08000000e+01,\n",
       "          9.96000000e+00,   1.02700000e+01,   3.03321000e+06,\n",
       "          1.05600000e+01,   1.05800000e+01,   1.02000000e+01,\n",
       "          1.03800000e+01,   2.29657300e+06,   1.05300000e+01,\n",
       "          1.06000000e+01,   1.02500000e+01,   1.05600000e+01,\n",
       "          2.04050000e+06,   1.09600000e+01,   1.12800000e+01,\n",
       "          1.05800000e+01,   1.06000000e+01,   3.09163300e+06,\n",
       "          1.10000000e+01,   1.10200000e+01,   1.06900000e+01,\n",
       "          1.09800000e+01,   2.91357800e+06,   1.08800000e+01,\n",
       "          1.18000000e+01,   1.08100000e+01,   1.10000000e+01,\n",
       "          5.87994500e+06,   1.19700000e+01,   1.19700000e+01,\n",
       "          1.10800000e+01,   1.10800000e+01,   4.45920200e+06,\n",
       "          1.31700000e+01,   1.31700000e+01,   1.26000000e+01,\n",
       "          1.30000000e+01,   9.20442700e+06,   1.23900000e+01,\n",
       "          1.34200000e+01,   1.23000000e+01,   1.33900000e+01,\n",
       "          3.90448500e+06,   1.11500000e+01,   1.21900000e+01,\n",
       "          1.11500000e+01,   1.21900000e+01,   1.85710500e+06,\n",
       "          1.14100000e+01,   1.14700000e+01,   1.00400000e+01,\n",
       "          1.07500000e+01,   3.91323200e+06,   1.12200000e+01,\n",
       "          1.19800000e+01,   1.08200000e+01,   1.14000000e+01,\n",
       "          2.32636100e+06,   1.04100000e+01,   1.11700000e+01,\n",
       "          1.03000000e+01,   1.11000000e+01,   1.45684600e+06,\n",
       "          1.06800000e+01,   1.07300000e+01,   1.00800000e+01,\n",
       "          1.04200000e+01,   1.14772900e+06,   1.06700000e+01,\n",
       "          1.08900000e+01,   1.04000000e+01,   1.07800000e+01,\n",
       "          9.26399000e+05,   1.05300000e+01,   1.06800000e+01,\n",
       "          1.03900000e+01,   1.05000000e+01,   8.54184000e+05,\n",
       "          1.03200000e+01,   1.06000000e+01,   1.01800000e+01,\n",
       "          1.05300000e+01,   1.01802900e+06,   9.82000000e+00,\n",
       "          1.03000000e+01,   9.74000000e+00,   1.03000000e+01,\n",
       "          8.54630000e+05,   1.00900000e+01,   1.01000000e+01,\n",
       "          9.74000000e+00,   9.80000000e+00,   5.95161000e+05,\n",
       "          9.86000000e+00,   1.01500000e+01,   9.85000000e+00,\n",
       "          1.01000000e+01,   4.77261000e+05,   9.50000000e+00,\n",
       "          9.95000000e+00,   9.50000000e+00,   9.95000000e+00,\n",
       "          6.59930000e+05,   9.00000000e+00,   9.30000000e+00,\n",
       "          8.90000000e+00,   9.30000000e+00,   8.15290000e+05,\n",
       "          8.91000000e+00,   9.32428800e+17]),\n",
       " array([  3.23900000e+01,   3.27000000e+01,   3.15700000e+01,\n",
       "          3.19600000e+01,   1.68845670e+07,   3.28900000e+01,\n",
       "          3.30000000e+01,   3.20200000e+01,   3.24700000e+01,\n",
       "          2.24160890e+07,   3.28100000e+01,   3.30500000e+01,\n",
       "          3.25400000e+01,   3.30000000e+01,   1.58950490e+07,\n",
       "          3.25000000e+01,   3.29500000e+01,   3.23500000e+01,\n",
       "          3.28200000e+01,   1.42042590e+07,   3.22200000e+01,\n",
       "          3.27800000e+01,   3.20100000e+01,   3.27000000e+01,\n",
       "          1.03743090e+07,   3.30000000e+01,   3.30000000e+01,\n",
       "          3.21700000e+01,   3.22300000e+01,   1.73961570e+07,\n",
       "          3.31200000e+01,   3.39400000e+01,   3.28800000e+01,\n",
       "          3.30100000e+01,   2.38962120e+07,   3.40000000e+01,\n",
       "          3.40000000e+01,   3.29000000e+01,   3.35200000e+01,\n",
       "          2.69229900e+07,   3.50000000e+01,   3.66000000e+01,\n",
       "          3.37000000e+01,   3.40300000e+01,   4.84578610e+07,\n",
       "          3.49100000e+01,   3.64500000e+01,   3.44000000e+01,\n",
       "          3.60000000e+01,   3.09598670e+07,   3.35500000e+01,\n",
       "          3.48000000e+01,   3.34800000e+01,   3.48000000e+01,\n",
       "          2.48202710e+07,   3.33800000e+01,   3.41900000e+01,\n",
       "          3.32300000e+01,   3.34000000e+01,   1.24537390e+07,\n",
       "          3.37000000e+01,   3.37000000e+01,   3.23900000e+01,\n",
       "          3.32800000e+01,   1.24792040e+07,   3.38800000e+01,\n",
       "          3.43700000e+01,   3.30700000e+01,   3.36100000e+01,\n",
       "          1.65334500e+07,   3.32300000e+01,   3.44000000e+01,\n",
       "          3.31200000e+01,   3.43900000e+01,   1.53458660e+07,\n",
       "          3.32200000e+01,   3.41000000e+01,   3.29600000e+01,\n",
       "          3.34000000e+01,   1.34105960e+07,   3.30300000e+01,\n",
       "          3.34000000e+01,   3.27000000e+01,   3.31900000e+01,\n",
       "          1.36701630e+07,   3.36100000e+01,   3.36400000e+01,\n",
       "          3.30500000e+01,   3.30500000e+01,   1.68732390e+07,\n",
       "          3.40500000e+01,   3.44800000e+01,   3.34200000e+01,\n",
       "          3.36900000e+01,   2.99732140e+07,   3.47600000e+01,\n",
       "          3.50000000e+01,   3.37000000e+01,   3.40400000e+01,\n",
       "          3.27969290e+07,   3.52100000e+01,   3.59000000e+01,\n",
       "          3.42700000e+01,   3.46900000e+01,   4.51666930e+07,\n",
       "          3.46000000e+01,   3.54700000e+01,   3.45000000e+01,\n",
       "          3.54000000e+01,   3.30276140e+07,   3.51000000e+01,\n",
       "          3.51100000e+01,   3.43700000e+01,   3.48000000e+01,\n",
       "          3.03488010e+07,   3.48700000e+01,   3.58900000e+01,\n",
       "          3.38100000e+01,   3.54400000e+01,   3.77999250e+07,\n",
       "          3.43100000e+01,   3.47000000e+01,   3.40600000e+01,\n",
       "          3.45500000e+01,   2.32867500e+07,   3.45800000e+01,\n",
       "          3.48800000e+01,   3.36300000e+01,   3.41300000e+01,\n",
       "          2.23170840e+07,   3.42500000e+01,   3.48600000e+01,\n",
       "          3.40900000e+01,   3.47000000e+01,   1.47020470e+07,\n",
       "          3.61600000e+01,   3.66600000e+01,   3.44500000e+01,\n",
       "          3.44500000e+01,   6.08791300e+07,   3.51700000e+01,\n",
       "          3.60000000e+01,   3.50600000e+01,   3.60000000e+01,\n",
       "          3.57304390e+07,   3.53900000e+01,   3.57900000e+01,\n",
       "          3.50600000e+01,   3.50600000e+01,   2.52235550e+07,\n",
       "          3.76400000e+01,   3.87500000e+01,   3.54000000e+01,\n",
       "          3.54000000e+01,   6.49411690e+07,   3.88400000e+01,\n",
       "          3.93500000e+01,   3.69100000e+01,   3.70500000e+01,\n",
       "          5.77450820e+07,   4.04700000e+01,   4.19300000e+01,\n",
       "          3.83600000e+01,   3.91000000e+01,   5.65870150e+07,\n",
       "          3.90600000e+01,   4.19800000e+01,   3.80000000e+01,\n",
       "          4.07500000e+01,   4.96057330e+07,   3.81100000e+01,\n",
       "          3.97700000e+01,   3.68000000e+01,   3.96000000e+01,\n",
       "          3.52354050e+07,   3.85900000e+01,   3.88500000e+01,\n",
       "          3.75000000e+01,   3.77000000e+01,   2.43056710e+07,\n",
       "          3.85000000e+01,   3.88800000e+01,   3.77700000e+01,\n",
       "          3.88800000e+01,   2.71674890e+07,   4.13800000e+01,\n",
       "          4.19800000e+01,   3.84400000e+01,   3.84900000e+01,\n",
       "          5.98456120e+07,   3.94100000e+01,   4.14200000e+01,\n",
       "          3.92000000e+01,   4.14200000e+01,   3.82545700e+07,\n",
       "          4.33500000e+01,   4.33500000e+01,   4.00000000e+01,\n",
       "          4.02000000e+01,   7.10028040e+07,   4.44000000e+01,\n",
       "          4.76900000e+01,   4.43600000e+01,   4.65000000e+01,\n",
       "          1.04013529e+08,   4.59300000e+01,   4.67000000e+01,\n",
       "          4.18900000e+01,   4.30100000e+01,   7.25056540e+07,\n",
       "          4.48700000e+01,   4.58000000e+01,   4.36000000e+01,\n",
       "          4.54100000e+01,   3.68773150e+07,   4.60100000e+01,\n",
       "          4.85000000e+01,   4.55000000e+01,   4.65900000e+01,\n",
       "          5.27586190e+07,   4.46000000e+01,   4.76300000e+01,\n",
       "          4.38800000e+01,   4.68100000e+01,   5.42885320e+07,\n",
       "          4.31600000e+01,   4.39000000e+01,   4.25500000e+01,\n",
       "          4.35000000e+01,   3.48261500e+07,   4.38200000e+01,\n",
       "          4.41800000e+01,   4.29900000e+01,   4.35500000e+01,\n",
       "          2.86811880e+07,   4.15500000e+01,   4.38500000e+01,\n",
       "          4.12500000e+01,   4.38300000e+01,   3.22453530e+07,\n",
       "          4.19500000e+01,   4.25000000e+01,   4.08600000e+01,\n",
       "          4.10000000e+01,   1.98601820e+07,   4.27100000e+01,\n",
       "          4.35000000e+01,   4.19900000e+01,   4.22100000e+01,\n",
       "          2.33502280e+07,   4.16600000e+01,   4.25000000e+01,\n",
       "          4.11800000e+01,   4.25000000e+01,   1.77897390e+07,\n",
       "          4.11000000e+01,   4.31500000e+01,   4.10000000e+01,\n",
       "          4.17000000e+01,   2.24686700e+07,   3.99900000e+01,\n",
       "          4.17000000e+01,   3.95700000e+01,   4.10100000e+01,\n",
       "          1.97230200e+07,   4.03000000e+01,   4.19900000e+01,\n",
       "          4.02200000e+01,   4.06600000e+01,   1.96138860e+07,\n",
       "          4.12600000e+01,   4.13600000e+01,   4.03500000e+01,\n",
       "          4.06200000e+01,   1.65686750e+07,   4.43700000e+01,\n",
       "          4.49700000e+01,   4.15000000e+01,   4.22000000e+01,\n",
       "          3.69027710e+07,   4.64000000e+01,   4.76100000e+01,\n",
       "          4.44400000e+01,   4.45000000e+01,   5.99677480e+07,\n",
       "          4.40700000e+01,   4.68000000e+01,   4.36300000e+01,\n",
       "          4.68000000e+01,   4.02764480e+07,   4.49300000e+01,\n",
       "          4.62500000e+01,   4.37000000e+01,   4.40700000e+01,\n",
       "          3.73655620e+07,   4.31900000e+01,   4.42700000e+01,\n",
       "          4.26800000e+01,   4.42700000e+01,   2.42999830e+07,\n",
       "          4.29100000e+01,   4.40000000e+01,   4.22800000e+01,\n",
       "          4.27000000e+01,   1.98531100e+07,   4.32000000e+01,\n",
       "          4.34900000e+01,   4.21100000e+01,   4.28400000e+01,\n",
       "          2.19339530e+07,   4.45000000e+01,   4.55000000e+01,\n",
       "          4.32400000e+01,   4.32400000e+01,   3.37777740e+07,\n",
       "          4.78000000e+01,   4.87700000e+01,   4.34100000e+01,\n",
       "          4.41100000e+01,   4.55581230e+07,   4.72800000e+01,\n",
       "          4.85000000e+01,   4.60100000e+01,   4.82100000e+01,\n",
       "          5.51309260e+07,   4.87700000e+01,   4.90000000e+01,\n",
       "          4.63900000e+01,   4.72800000e+01,   5.41098480e+07,\n",
       "          5.28800000e+01,   5.31900000e+01,   4.83000000e+01,\n",
       "          4.89800000e+01,   6.25553730e+07,   5.28000000e+01,\n",
       "          5.49700000e+01,   5.06100000e+01,   5.30200000e+01,\n",
       "          5.43863160e+07,   4.75200000e+01,   5.36300000e+01,\n",
       "          4.75200000e+01,   5.28000000e+01,   5.07625780e+07,\n",
       "          4.66000000e+01,   4.84500000e+01,   4.32800000e+01,\n",
       "          4.70000000e+01,   3.56587260e+07,   5.12600000e+01,\n",
       "          5.12600000e+01,   4.75200000e+01,   4.75200000e+01,\n",
       "          4.12597870e+07,   5.09300000e+01,   5.39500000e+01,\n",
       "          4.92200000e+01,   5.20200000e+01,   4.27813070e+07,\n",
       "          4.84800000e+01,   5.08100000e+01,   4.70000000e+01,\n",
       "          5.08000000e+01,   3.16273940e+07,   4.93100000e+01,\n",
       "          4.98800000e+01,   4.44500000e+01,   4.85600000e+01,\n",
       "          2.39649700e+07,   4.94600000e+01,   5.03300000e+01,\n",
       "          4.84800000e+01,   4.97900000e+01,   2.87721480e+07,\n",
       "          4.92500000e+01,   5.03300000e+01,   4.75300000e+01,\n",
       "          4.94700000e+01,   2.84434340e+07,   5.36000000e+01,\n",
       "          5.41000000e+01,   4.98000000e+01,   4.98000000e+01,\n",
       "          5.67437680e+07,   4.92300000e+01,   5.19200000e+01,\n",
       "          4.90000000e+01,   5.10000000e+01,   4.78955720e+07,\n",
       "          4.77000000e+01,   4.88000000e+01,   4.71000000e+01,\n",
       "          4.88000000e+01,   3.13723740e+07,   4.76000000e+01,\n",
       "          4.82900000e+01,   4.70300000e+01,   4.77600000e+01,\n",
       "          2.16430740e+07,   4.60300000e+01,   4.80500000e+01,\n",
       "          4.60000000e+01,   4.76100000e+01,   2.22772140e+07,\n",
       "          4.56600000e+01,   4.78700000e+01,   4.50000000e+01,\n",
       "          4.70100000e+01,   2.55973030e+07,   4.53600000e+01,\n",
       "          4.65500000e+01,   4.20000000e+01,   4.56600000e+01,\n",
       "          2.68933930e+07,   4.26200000e+01,   4.53600000e+01,\n",
       "          4.25000000e+01,   4.53600000e+01,   1.61766460e+07,\n",
       "          3.83600000e+01,   4.17000000e+01,   3.83600000e+01,\n",
       "          4.17000000e+01,   1.77033260e+07,   3.86900000e+01,\n",
       "          3.89400000e+01,   3.47800000e+01,   3.80000000e+01,\n",
       "          1.95111330e+07,   3.97800000e+01,   4.03900000e+01,\n",
       "          3.88300000e+01,   3.94600000e+01,   2.05174870e+07,\n",
       "          3.75000000e+01,   4.04100000e+01,   3.63700000e+01,\n",
       "          3.96800000e+01,   1.83370020e+07,   3.37100000e+01,\n",
       "          3.60000000e+01,   3.37100000e+01,   3.60000000e+01,\n",
       "          1.58580540e+07,   3.03400000e+01,   3.52500000e+01,\n",
       "          3.03400000e+01,   3.50000000e+01,   2.20487040e+07,\n",
       "          3.28500000e+01,   3.31700000e+01,   2.73100000e+01,\n",
       "          2.97600000e+01,   2.92259740e+07,   3.10300000e+01,\n",
       "          3.54900000e+01,   3.03100000e+01,   3.27500000e+01,\n",
       "          2.79042420e+07,   2.79300000e+01,   3.12000000e+01,\n",
       "          2.79300000e+01,   3.09000000e+01,   2.00690170e+07,\n",
       "          2.51400000e+01,   2.85800000e+01,   2.51400000e+01,\n",
       "          2.66200000e+01,   1.99679990e+07,   2.42000000e+01,\n",
       "          2.76400000e+01,   2.26300000e+01,   2.76400000e+01,\n",
       "          2.62727060e+07,   2.17800000e+01,   2.30800000e+01,\n",
       "          2.17800000e+01,   2.30800000e+01,   1.06654190e+07,\n",
       "          1.96000000e+01,   2.05000000e+01,   1.96000000e+01,\n",
       "          1.96000000e+01,   4.91913790e+07,   2.15600000e+01,\n",
       "          2.15600000e+01,   1.83000000e+01,   1.83000000e+01,\n",
       "          2.20342650e+07,   2.37200000e+01,   2.37200000e+01,\n",
       "          2.25000000e+01,   2.26900000e+01,   1.50118900e+07,\n",
       "          2.60900000e+01,   2.60900000e+01,   2.47100000e+01,\n",
       "          2.60000000e+01,   2.63344770e+07,   2.78000000e+01,\n",
       "          2.86700000e+01,   2.64800000e+01,   2.72400000e+01,\n",
       "          4.24016350e+07,   2.50200000e+01,   2.71000000e+01,\n",
       "          2.50200000e+01,   2.69000000e+01,   2.28979030e+07,\n",
       "          2.57900000e+01,   2.66800000e+01,   2.27100000e+01,\n",
       "          2.50200000e+01,   2.14979730e+07,   2.80100000e+01,\n",
       "          2.83200000e+01,   2.60000000e+01,   2.60000000e+01,\n",
       "          2.27790270e+07,   2.90500000e+01,   3.00000000e+01,\n",
       "          2.80000000e+01,   2.84800000e+01,   2.75749280e+07,\n",
       "          2.98400000e+01,   3.04800000e+01,   2.79000000e+01,\n",
       "          2.85000000e+01,   2.66331850e+07,   2.98500000e+01,\n",
       "          3.02600000e+01,   2.86600000e+01,   2.94900000e+01,\n",
       "          2.32219280e+07,   3.10500000e+01,   3.17500000e+01,\n",
       "          2.95600000e+01,   2.98900000e+01,   2.82745640e+07,\n",
       "          3.01200000e+01,   3.15700000e+01,   2.98000000e+01,\n",
       "          3.10600000e+01,   2.91551930e+07,   2.71100000e+01,\n",
       "          3.06000000e+01,   2.71100000e+01,   2.90000000e+01,\n",
       "          1.99439840e+07,   2.52000000e+01,   2.76600000e+01,\n",
       "          2.44000000e+01,   2.64000000e+01,   2.14471340e+07,\n",
       "          2.75800000e+01,   2.76400000e+01,   2.46900000e+01,\n",
       "          2.57000000e+01,   1.74965310e+07,   2.93500000e+01,\n",
       "          3.03400000e+01,   2.69800000e+01,   2.76200000e+01,\n",
       "          4.21218340e+07,   2.81800000e+01,   2.97000000e+01,\n",
       "          2.72000000e+01,   2.87700000e+01,   2.52622870e+07,\n",
       "          2.53600000e+01,   2.83600000e+01,   2.53600000e+01,\n",
       "          2.75000000e+01,   1.92285360e+07,   2.76300000e+01,\n",
       "          2.76800000e+01,   2.55000000e+01,   2.55000000e+01,\n",
       "          1.89263900e+07,   2.78000000e+01,   2.85000000e+01,\n",
       "          2.67000000e+01,   2.78700000e+01,   2.26545550e+07,\n",
       "          2.99800000e+01,   3.05800000e+01,   2.68100000e+01,\n",
       "          2.70000000e+01,   4.79660780e+07,   3.08500000e+01,\n",
       "          3.13000000e+01,   2.93700000e+01,   3.00200000e+01,\n",
       "          4.48826930e+07,   3.39400000e+01,   3.39400000e+01,\n",
       "          3.17000000e+01,   3.31500000e+01,   3.48264910e+07,\n",
       "          3.50500000e+01,   3.73300000e+01,   3.45100000e+01,\n",
       "          3.50000000e+01,   6.69175650e+07,   3.40900000e+01,\n",
       "          3.60200000e+01,   3.39500000e+01,   3.44300000e+01,\n",
       "          3.54467040e+07,   3.52000000e+01,   3.55000000e+01,\n",
       "          3.35200000e+01,   3.38900000e+01,   2.87567260e+07,\n",
       "          3.54300000e+01,   3.73800000e+01,   3.48000000e+01,\n",
       "          3.60400000e+01,   3.57498370e+07,   3.49200000e+01,\n",
       "          3.56700000e+01,   3.38000000e+01,   3.48900000e+01,\n",
       "          2.48951990e+07,   3.14300000e+01,   3.52200000e+01,\n",
       "          3.14300000e+01,   3.49800000e+01,   3.35940030e+07,\n",
       "          3.17800000e+01,   3.24900000e+01,   2.82900000e+01,\n",
       "          2.94500000e+01,   2.44302480e+07,   2.92000000e+01,\n",
       "          3.12900000e+01,   2.90000000e+01,   3.07500000e+01,\n",
       "          1.65913520e+07,   2.83400000e+01,   3.00600000e+01,\n",
       "          2.70900000e+01,   2.85000000e+01,   1.75526530e+07,\n",
       "          2.55100000e+01,   2.72500000e+01,   2.55100000e+01,\n",
       "          2.72000000e+01,   1.12641910e+07,   2.29600000e+01,\n",
       "          2.37000000e+01,   2.29600000e+01,   2.29600000e+01,\n",
       "          1.21032150e+07,   2.08000000e+01,   2.39800000e+01,\n",
       "          2.06600000e+01,   2.30000000e+01,   2.19430010e+07,\n",
       "          2.19400000e+01,   2.19900000e+01,   1.96000000e+01,\n",
       "          2.12000000e+01,   1.81190710e+07,   2.41300000e+01,\n",
       "          2.41300000e+01,   2.21600000e+01,   2.26000000e+01,\n",
       "          2.38738810e+07,   2.40200000e+01,   2.55000000e+01,\n",
       "          2.39000000e+01,   2.43100000e+01,   2.45264600e+07,\n",
       "          2.16200000e+01,   2.35600000e+01,   2.16200000e+01,\n",
       "          2.33000000e+01,   1.77643660e+07,   2.00100000e+01,\n",
       "          2.26000000e+01,   1.96300000e+01,   1.96300000e+01,\n",
       "          1.49874050e+07,   2.05100000e+01,   2.15000000e+01,\n",
       "          2.02200000e+01,   2.06600000e+01,   1.37842890e+07,\n",
       "          2.25600000e+01,   2.25600000e+01,   2.00000000e+01,\n",
       "          2.00100000e+01,   1.77773480e+07,   2.37000000e+01,\n",
       "          2.40100000e+01,   2.26000000e+01,   2.30300000e+01,\n",
       "          2.33479940e+07,   2.58700000e+01,   2.60700000e+01,\n",
       "          2.30100000e+01,   2.32000000e+01,   4.44564390e+07,\n",
       "          2.54000000e+01,   2.58900000e+01,   2.46700000e+01,\n",
       "          2.50000000e+01,   3.22046490e+07,   2.28600000e+01,\n",
       "          2.66800000e+01,   2.28600000e+01,   2.59000000e+01,\n",
       "          3.07194030e+07,   2.09900000e+01,   2.32500000e+01,\n",
       "          2.06800000e+01,   2.13000000e+01,   2.12379830e+07,\n",
       "          2.30900000e+01,   2.30900000e+01,   2.11300000e+01,\n",
       "          2.15000000e+01,   3.12949890e+07,   2.44500000e+01,\n",
       "          2.54000000e+01,   2.37100000e+01,   2.39400000e+01,\n",
       "          5.52587150e+07,   2.63500000e+01,   2.65000000e+01,\n",
       "          2.45000000e+01,   2.54000000e+01,   5.00639930e+07,\n",
       "          2.81000000e+01,   2.88800000e+01,   2.66400000e+01,\n",
       "          2.66400000e+01,   4.73270280e+07,   2.75600000e+01,\n",
       "          2.84500000e+01,   2.70800000e+01,   2.81000000e+01,\n",
       "          3.46838310e+07,   2.78100000e+01,   2.86900000e+01,\n",
       "          2.65000000e+01,   2.65600000e+01,   3.48851600e+07,\n",
       "          2.78500000e+01,   1.44305280e+18])]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 應該只有一個 feature 是個 shape = (751,) 的 array. 這怎麼表達?\n",
    "my_feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[751,])]  \n",
    "my_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[256,128],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=2,\n",
    "    dropout=0.2,\n",
    "    model_dir=\"model/DNN\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training inputs 從 mnist 抄過來改的\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_train[:featurenum+1]},\n",
    "    y=y_train,\n",
    "    num_epochs=None,\n",
    "    batch_size=1000,\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(input_fn=train_input_fn, steps=3000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 應該只有一個 feature 是個 shape = (751,) 的 array. 這怎麼表達?\n",
    "my_feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[751,])]  \n",
    "my_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[256,128],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=2,\n",
    "    dropout=0.4,\n",
    "    model_dir=\"model/DNN\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training inputs 從 mnist 抄過來改的\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_train},\n",
    "    y=y_train,\n",
    "    num_epochs=None,\n",
    "    batch_size=1000,\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(input_fn=train_input_fn, steps=3000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  [待續] 改良餵進資料的方式，直接用 shape(150,5) 跳過 150天打包的過程。\n",
    "\n",
    "- 要回溯的天數先指定，最前面這麼多天扣掉。\n",
    "- 要往後參考的天數先指定，最後面這麼多天扣掉。排除這兩條件，剩下來的就是可用的天數。\n",
    "- cross_validation 的工作很簡單，看取多少趴數當 test 從尾巴切走就對了。所以 ％ 數對應一個切分日期。\n",
    "- 改寫 input_fn 讓它從 train or test set 裡面挑一個出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 預測「未來 dayforesee 天會不會漲 20%？」\n",
    "\n",
    "實際 4638天當中有 1677天 符合條件\n",
    "用全部資料來比對： ( 沒事的，漏掉的，多的，一致的 ) tib. \\ ==> [2418, 1243, 383, 434] \n",
    "用 Test 比對 ： ( 沒事的，漏掉的，多的，一致的 ) tib. \\ ==> [433, 297, 110, 56]\n",
    "這樣看起來好像有點被記住了的問題，所以 dropout 用到了 0.4 （40％） 得到以上結果，只能這樣了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "alert(\"要不要把 model/DNN directory 先殺掉？\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import peforth as f\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%f\n",
    "\\ Now we redefine the 'unknown' command that does nothing at default  \n",
    ": unknown ( token -- thing Y|N) // Try to find the unknown token in __main__\n",
    "  py> getattr(sys.modules['__main__'],pop(),\"Ûnknôwn\") \n",
    "  py> str(tos())==\"Ûnknôwn\" if drop false else true then ;\n",
    "  /// here after, when FORTH come accross an unknown token, instead of alerting \n",
    "  /// it try to find the token in python __main__ module name space.\n",
    ": path-to-find-modules ( <path> -- ) // Add path to sys.path so \"import module-name\" can find the module\n",
    "    CR word trim ( \"path\" ) py: sys.path.append(pop()) ;\n",
    "code \\ print(nexttoken('\\n')) end-code // Redefine \\ command to print the comment line \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('000777.csv',encoding='gbk',parse_dates=[0],index_col=0)\n",
    "\n",
    "# data frame 中帶有 None 的 rows 都剔除掉。 本來有 4752 rows 現在只剩 4639 \n",
    "# 用法: df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "data = data.replace(to_replace='None', value=np.nan).dropna()  \n",
    "\n",
    "# pdb.set_trace() # data 本來是新的在上面\n",
    "data.sort_index(0,ascending=True,inplace=True) # 排序 sort 過後變成最舊的在上面\n",
    "\n",
    "# 上市第一天的資料不要，因為【漲跌幅】無效。\n",
    "data = data.query('index > \"1997-07-10\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data has 20 years long, but here only analysis the last 150 days of a chosen day\n",
    "dayfeature=150\n",
    "dayforesee=10  # 問未來幾天如何如何，最後就要扣掉幾天，避免 out of bounds. 0 代表當天。\n",
    "featurenum=5*dayfeature  # 取五個有關係的欄位，特徵值。\n",
    "x=np.zeros((data.shape[0]-dayfeature-dayforesee,featurenum+1))\n",
    "y=np.zeros((data.shape[0]-dayfeature-dayforesee));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重疊 shift 150 天的五個「行情數目」加上下一天的開盤 共 751 個「數目」當作 feature X \n",
    "\n",
    "for i in range(0,data.shape[0]-dayfeature-dayforesee):\n",
    "    x[i,0:featurenum]=np.array(data[i:i+dayfeature] \\\n",
    "          [[u'收盘价',u'最高价',u'最低价',u'开盘价',u'成交量']]).reshape((1,featurenum))\n",
    "    # x[i,featurenum]=data.ix[i+dayfeature][u'开盘价']  # ix deprecated\n",
    "    x[i,featurenum]=data.iloc[i+dayfeature][u'开盘价']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預測未來 dayforesee 天是否會「漲20%」?\n",
    "\n",
    "for i in range(0,data.shape[0]-dayfeature-dayforesee):\n",
    "    y[i]=0              \n",
    "    pi = float(data.iloc[i][u'收盘价'])\n",
    "    for j in range(dayforesee):\n",
    "        pi5 = float(data.iloc[j+i+dayfeature][u'最高价'])\n",
    "        if (pi5-pi)/pi >= 0.20 :\n",
    "            y[i]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.int32(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f ( 有這麼多天符合條件 ) y py> sum(pop()) tib.\n",
    "%f x_train :> shape tib.\n",
    "%f x_test :> shape tib.\n",
    "%f y_train :> shape tib.\n",
    "%f y_test :> shape tib.\n",
    "%f x_train :> shape[0] tib.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 應該只有一個 feature 是個 shape = (751,) 的 array. 這怎麼表達?\n",
    "my_feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[751,])]  \n",
    "my_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[256,128],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=2,\n",
    "    dropout=0.4,\n",
    "    model_dir=\"model/DNN\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training inputs 從 mnist 抄過來改的\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_train},\n",
    "    y=y_train,\n",
    "    num_epochs=None,\n",
    "    batch_size=1000,\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(input_fn=train_input_fn, steps=3000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test inputs  從 mnist 抄過來改的\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_test},\n",
    "    y=y_test,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))\n",
    "\n",
    "# End of \"mnist_estimator.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用全部資料來比對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make input function for prediction:\n",
    "#   shuffle=False -> do not randomize input data\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x},\n",
    "    y=y,\n",
    "    num_epochs=1,  # Just one epoch\n",
    "    shuffle=False, # Don't shuffle so we can compare to true labels later\n",
    "    )  # 參考 GitHub/tensorflow-workshop/examples/07_structured_data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從 wh300/DNNClassifier_Predicting.ipynb 抄來的例子\n",
    "# Predict 整批 test input \n",
    "preds = classifier.predict(pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = [i for i in preds];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [int(i['probabilities'][1] > i['probabilities'][0]) for i in predict];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%f predictions py> sum(pop()) tib.\n",
    "%f y py> sum(pop()) tib.\n",
    "%f y :> [:50] . cr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 AI 預測的結果，與人工 tag 比較。\n",
    "categories = [0,0,0,0]\n",
    "for i in range(y.shape[0]): \n",
    "    if int(predictions[i])==int(y[i]) and int(y[i])==0: categories[0]+=1; continue\n",
    "    if int(predictions[i])!=int(y[i]) and int(y[i])==1: categories[1]+=1; continue\n",
    "    if int(predictions[i])!=int(y[i]) and int(y[i])==0: categories[2]+=1; continue\n",
    "    if int(predictions[i])==int(y[i]) and int(y[i])==1: categories[3]+=1; continue\n",
    "%f categories ( 沒事的，漏掉的，多的，一致的 ) tib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 只用 Test dataset 來比對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make input function for prediction:\n",
    "#   shuffle=False -> do not randomize input data\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_test},\n",
    "    y=y_test,\n",
    "    num_epochs=1,  # Just one epoch\n",
    "    shuffle=False, # Don't shuffle so we can compare to true labels later\n",
    "    )  # 參考 GitHub/tensorflow-workshop/examples/07_structured_data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從 wh300/DNNClassifier_Predicting.ipynb 抄來的例子\n",
    "# Predict 整批 test input \n",
    "preds = classifier.predict(pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = [i for i in preds];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [int(i['probabilities'][1] > i['probabilities'][0]) for i in predict];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%f predictions py> sum(pop()) tib.\n",
    "%f y py> sum(pop()) tib.\n",
    "%f y :> [:50] . cr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 AI 預測的結果，與人工 tag 比較。\n",
    "categories = [0,0,0,0]\n",
    "for i in range(y_test.shape[0]): \n",
    "    if int(predictions[i])==int(y[i]) and int(y[i])==0: categories[0]+=1; continue\n",
    "    if int(predictions[i])!=int(y[i]) and int(y[i])==1: categories[1]+=1; continue\n",
    "    if int(predictions[i])!=int(y[i]) and int(y[i])==0: categories[2]+=1; continue\n",
    "    if int(predictions[i])==int(y[i]) and int(y[i])==1: categories[3]+=1; continue\n",
    "%f categories ( 沒事的，漏掉的，多的，一致的 ) tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!start tensorboard --logdir=\"model/DNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 都搞懂了，接下來打包 000777.csv 試試看吧!\n",
    "\n",
    "這把預測「明天會不會漲停？」結果因為實際 4638天當中只有88天漲停，全部猜「不」就有 98.10％ 的 accuracy. 可能正是因為怎麼猜別的都沒有這個好。 DNN 全部猜「不會」。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "alert(\"要不要把 DNN_model directory 先殺掉？\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import peforth as f\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%f\n",
    "\\ Now we redefine the 'unknown' command that does nothing at default  \n",
    ": unknown ( token -- thing Y|N) // Try to find the unknown token in __main__\n",
    "  py> getattr(sys.modules['__main__'],pop(),\"Ûnknôwn\") \n",
    "  py> str(tos())==\"Ûnknôwn\" if drop false else true then ;\n",
    "  /// here after, when FORTH come accross an unknown token, instead of alerting \n",
    "  /// it try to find the token in python __main__ module name space.\n",
    ": path-to-find-modules ( <path> -- ) // Add path to sys.path so \"import module-name\" can find the module\n",
    "    CR word trim ( \"path\" ) py: sys.path.append(pop()) ;\n",
    "code \\ print(nexttoken('\\n')) end-code // Redefine \\ command to print the comment line \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('000777.csv',encoding='gbk',parse_dates=[0],index_col=0)\n",
    "\n",
    "# data frame 中帶有 None 的 rows 都剔除掉。 本來有 4752 rows 現在只剩 4639 \n",
    "# 用法: df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "data = data.replace(to_replace='None', value=np.nan).dropna()  \n",
    "\n",
    "# pdb.set_trace() # data 本來是新的在上面\n",
    "data.sort_index(0,ascending=True,inplace=True) # 排序 sort 過後變成最舊的在上面\n",
    "\n",
    "# 上市第一天的資料不要，因為【漲跌幅】無效。\n",
    "data = data.query('index > \"1997-07-10\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data has 20 years long, but here only analysis the last 150 days of a chosen day\n",
    "dayfeature=150\n",
    "dayforesee=5  # 問未來幾天如何如何，最後就要扣掉幾天，避免 out of bounds. 0 代表當天。\n",
    "featurenum=5*dayfeature  # 取五個有關係的欄位，特徵值。\n",
    "x=np.zeros((data.shape[0]-dayfeature-dayforesee,featurenum+1))\n",
    "y=np.zeros((data.shape[0]-dayfeature-dayforesee));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重疊 shift 150 天的五個「行情數目」加上下一天的開盤 共 751 個「數目」當作 feature X \n",
    "\n",
    "for i in range(0,data.shape[0]-dayfeature-dayforesee):\n",
    "    x[i,0:featurenum]=np.array(data[i:i+dayfeature] \\\n",
    "          [[u'收盘价',u'最高价',u'最低价',u'开盘价',u'成交量']]).reshape((1,featurenum))\n",
    "    # x[i,featurenum]=data.ix[i+dayfeature][u'开盘价']  # ix deprecated\n",
    "    x[i,featurenum]=data.iloc[i+dayfeature][u'开盘价']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預測當天是否會「漲停」?\n",
    "\n",
    "# data.shape[0]-dayfeature 是 4489 同上 x, 所以這是在製作相對所有 x 的 y. x 是 (4488, 751)\n",
    "# dayfeature 是 150 \n",
    "# 直接看【涨跌幅】> 9.8 就算漲停板\n",
    "\n",
    "for i in range(0,data.shape[0]-dayfeature-dayforesee):\n",
    "    if float(data.iloc[i+dayfeature][u'涨跌幅'])>=9.8:\n",
    "        y[i]=1.0\n",
    "    else:\n",
    "        y[i]=0.0          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f x_train :> shape tib.\n",
    "%f x_test :> shape tib.\n",
    "%f y_train :> shape tib.\n",
    "%f y_test :> shape tib.\n",
    "%f x_train :> shape[0] tib.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用 cross_validation 切分好 train-test feature & label 之後, 可望弄成 input_fn 要的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 應該只有一個 feature 是個 shape = (751,) 的 array. 這怎麼表達?\n",
    "my_feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[751,])]  \n",
    "my_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[256, 128],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=2,\n",
    "    dropout=0.2,\n",
    "    model_dir=\"DNN_model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上 classifier 弄出來了，還是要面對 input_function 怎麼弄的問題。Lambda 太難懂了，從 wh300 或哪裡去抄吧。\n",
    "看到的都是 tf.estimator.inputs.pandas_input_fn 我曾查出有 'numpy_input_fn' \n",
    "\n",
    "    c:\\Users\\hcche\\Documents\\GitHub\\twstock\\mnist_estimator.ipynb \n",
    "    # Specify feature\n",
    "    feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[28, 28])]\n",
    "    幹！ 回想起來了，用這個例子可以簡化 150天 的資料製作方式\n",
    "\n",
    "Estimator 的 DNNClassifier 怎麼用? 上面這個 mnist 也有似乎更進一步的發揮\n",
    "\n",
    "    # Build 2 layer DNN classifier\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        hidden_units=[256, 32],\n",
    "        optimizer=tf.train.AdamOptimizer(1e-4),\n",
    "        n_classes=10,\n",
    "        dropout=0.1,\n",
    "        model_dir=\"./tmp/mnist_model\"\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training inputs 從 mnist 抄過來改的\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_train},\n",
    "    y=y_train,\n",
    "    num_epochs=None,\n",
    "    batch_size=x_train.shape[0],  # 漲停很少，故 batch 要用全部天數，否則它全部猜「不會」\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(input_fn=train_input_fn, steps=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test inputs  從 mnist 抄過來改的\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_test},\n",
    "    y=y_test,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))\n",
    "\n",
    "# End of \"mnist_estimator.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讚！ 接下來怎麼看 predict 的結果 . . . 參考 wh300 吧\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make input function for prediction:\n",
    "#   shuffle=False -> do not randomize input data\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x},\n",
    "    y=y,\n",
    "    num_epochs=1,  # Just one epoch\n",
    "    shuffle=False, # Don't shuffle so we can compare to true labels later\n",
    "    )  # 參考 GitHub/tensorflow-workshop/examples/07_structured_data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從 wh300/DNNClassifier_Predicting.ipynb 抄來的例子\n",
    "# Predict 整批 test input \n",
    "preds = classifier.predict(pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [int(i['probabilities'][1] > i['probabilities'][0]) for i in preds];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f predictions py> sum(pop()) tib.\n",
    "%f y py> sum(pop()) tib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我在 Tensorflow 官網的教學網頁上找到了用 Iris 當例題的 estimator tutorial \n",
    "\n",
    "https://www.tensorflow.org/versions/master/get_started/premade_estimators (also on my Ynote \"Get Started with Estimators _TensorFlow_\") 有正式、詳細的介紹。\n",
    "這篇文章鼓勵人用 Estimator API 跟 Dataset API。本文研讀、實驗這篇文章。\n",
    "文章引用的 GitHub resource 我已經 clone 下來了在 T550 `c:\\Users\\hcche\\Documents\\GitHub\\models`  \n",
    "\n",
    "重點是，直接用 pandas data-frame 來當作 feature - label 餵給 input function 這樣更自然，免去搞 150 天行情資料的麻煩。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peforth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%f\n",
    "\\ Now we redefine the 'unknown' command that does nothing at default  \n",
    ": unknown ( token -- thing Y|N) // Try to find the unknown token in __main__\n",
    "  py> getattr(sys.modules['__main__'],pop(),\"Ûnknôwn\") \n",
    "  py> str(tos())==\"Ûnknôwn\" if drop false else true then ;\n",
    "  /// here after, when FORTH come accross an unknown token, instead of alerting \n",
    "  /// it try to find the token in python __main__ module name space.\n",
    "\n",
    ": path-to-find-modules ( <path> -- ) // Add path to sys.path so \"import module-name\" can find the module\n",
    "    CR word trim ( \"path\" ) py: sys.path.append(pop()) ;\n",
    "\n",
    "\\ set path for 'import iris_data'\n",
    "path-to-find-modules c:\\Users\\hcche\\Documents\\GitHub\\models\\samples\\core\\get_started\n",
    "\n",
    "code \\ print(nexttoken('\\n')) end-code // Redefine \\ command to print the comment line \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(np.array(irisX).reshape(-1,4), columns = iris.feature_names)    \n",
    "x = pd.DataFrame(iris.data, columns = iris.feature_names)    \n",
    "x[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(iris.target, columns = [\"target\"])    \n",
    "y[48:52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/pandas_input_fn\n",
    "\n",
    "tf.estimator.inputs.pandas_input_fn(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    num_epochs=5,\n",
    "    shuffle=False,\n",
    "    queue_capacity=None,\n",
    "    num_threads=1,\n",
    "    target_column='target'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 照 Tutorial 執行 -- 一把就成功！\n",
    "\n",
    "直接把本 notebook 建個 hard link 到 c:\\Users\\hcche\\Documents\\GitHub\\models\\samples\\core\\get_started 去執行...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run premade_estimator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開始照教材研讀\n",
    "    \n",
    "premade_estimator.py iris_data.py 都抓過來，他本來都放在 main() 內執行，不利 study 今予拆開，分段執行亦可，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "\"\"\"An Example of a DNNClassifier for the Iris dataset.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "import iris_data  # iris_data.py 在本目錄下 GitHub\\models\\samples\\core\\get_started\\iris_data.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這 argparse module 很常見 DeepSpeech 就有，這裡用不著吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', default=100, type=int, help='batch size')\n",
    "parser.add_argument('--train_steps', default=1000, type=int,\n",
    "                    help='number of training steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 不能在 jupyter notebook 下直接跑，因為 argv 不如預期。\n",
    "from IPython.display import display,Image;display(Image('error1.jpg', width=700))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "研究一下 parser (argparse module) 怎麼回事兒 <== 只能在 python 用，ipython & jupyter notebook 都不行\n",
    "http://localhost:8888/notebooks/Documents/GitHub/models/samples/core/get_started/My%20estimator%20to%20predict%20iris.ipynb\n",
    "\n",
    "[x] 這是在 Jupyter Notebook 下記錄到的\n",
    "    OK Argv . cr\n",
    "    ['c:\\\\users\\\\hcche\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\site-packages\\\\ipykernel_launcher.py', '-f', 'C:\\\\Users\\\\hcche\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-d21c0b87-4db1-432d-82ca-cd398f2dd5ad.json']\n",
    "    OK \n",
    "    看起來有三個 args -- .py script 本身, -f , 還有一個 .json, 後兩個是 jupyter notebook 加的，難怪\n",
    "    要出問題。\n",
    "\n",
    "[x] 改用 ipython 直接跑 premade_estimator.py 看看 .... 成功。 \n",
    "    --> 那他看到的 Argv 是怎樣? -->['c:\\\\Users\\\\hcche\\\\Documents\\\\GitHub\\\\models\\\\samples\\\\core\\\\get_started\\\\premade_estimator.py']\n",
    "        等於是只有 argv[0] 的 .py script 本身一個 args 那就沒問題了。\n",
    "\n",
    "[x] 我猜測了一下，矇對了，這樣執行：\n",
    "    ipython premade_estimator.py --batch_size=1234 --train_steps=5678\n",
    "    搗出了一些 warning 警告說 args 可能有問題，但總之得到的 args 是：\n",
    "    argv 是這樣 \n",
    "    ['c:\\\\Users\\\\hcche\\\\Documents\\\\GitHub\\\\models\\\\samples\\\\core\\\\get_started\\\\premade_estimator.py']\n",
    "    args 是這樣\n",
    "    Namespace(batch_size=100, train_steps=1000)\n",
    "[x] 確定不能用 ipython, 用 python 以下皆可：\n",
    "    python premade_estimator.py --batch_size=64 --train_steps=1000\n",
    "    python premade_estimator.py --batch_size 64 --train_steps 1000\n",
    "\n",
    "反正我現在知道怎麼讓 jupyter notebook 能跑了! 改這樣即可： \n",
    "    args = parser.parse_args(\"\")  # 用 default 值\n",
    "    args = parser.parse_args([\"--batch_size=100\", \"--train_steps=1000\"])  # 代替 command line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Command Line Arguments 要用模擬的\n",
    "Module argparse 只能在 python command line 跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ [x] 沒有指定時，自動取 default 值，真不知道這哪來的？看到了，從 parser 那邊 code 裡給的。\n",
    "args = parser.parse_args([\"--batch_size=100\", \"--train_steps=1000\"])\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 研究  (train_x, train_y), (test_x, test_y) = iris_data.load_data()\n",
    "\n",
    "看出來了 iris_data.py 是把 150 筆資料切成 120 : 30 for training and testing respectively.\n",
    "這是個壞消息，他沒有像 sklearn 的自動切的那啥 function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " (train_x, train_y), (test_x, test_y) = iris_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f train_x type . cr\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f train_y type . cr\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f test_x type . cr\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f test_y type . cr\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ [ ] 這是 feature columns, 我晚點可以嘗試把它弄成光一個 shape 為 (4,) 的單一 feature \n",
    "for key in train_x.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ [ ] 這是 feature columns, 我晚點可以嘗試把它弄成光一個 shape 為 (4,) 的單一 feature \n",
    "my_feature_columns = []\n",
    "for key in train_x.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "my_feature_columns    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%f \\ 從 main 裡拆分出來，方便 tracing. 可看出若無指定 model directory 自動放在\n",
    "%f \\     AppData\\Local\\Temp\\tmpijrg8mkm <-- 每次都不一樣，這樣好嗎？\n",
    "%f \\ 以及本 classifier 的種種細節\n",
    "%f \\ \n",
    "\n",
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[10, 10],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ 複習一下 lambda --> \n",
    "%f \\ lambda 是個 function, 產生 key(i) 取 x 的第 i 個 element。用於 sorted() 時指定哪個當 key \n",
    "s = [('a', 3), ('b', 2), ('c', 1)]\n",
    "key = lambda x:x[1]\n",
    "print(sorted(s, key=key))\n",
    "%f \\ 單獨使用時，看起來不太一樣，其實做的是一樣的事情。\n",
    "%f key :> ([('a',3),('b',2),('c',1),('d',4)]) tib.\n",
    "%f key tib.\n",
    "%f key .source\n",
    "%f \\ 當沒有給 lambda argument 時，是怎樣？\n",
    "x = lambda:s\n",
    "%f x tib. \\ 直接傳回 : 之後的東西(執行後)的傳回值\n",
    "%f x type tib.\n",
    "%f \\ 查看 x 的 source code \n",
    "%f x .source\n",
    "%f x :> () tib.\n",
    "\n",
    "%f \\ 所以，input_fn = lambda:iris_data.train_input_fn(train_x, train_y,args.batch_size)\n",
    "%f \\ 當中的 iris_data.train_input_fn() 是傳回一個 function 但是該 function 的 argument 是活的，\n",
    "%f \\ 要延後到最後一秒鐘才確定，因此要用 lambda 或單純的 function 包一層用來指定 arguments。\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%f \\ 這就開始訓練了 . . .  \n",
    "%f \\ \n",
    "\n",
    "# Train the Model.\n",
    "classifier.train(\n",
    "    input_fn=lambda:iris_data.train_input_fn(train_x, train_y,\n",
    "                                             args.batch_size),\n",
    "    steps=args.train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這時候的 classifier 已經受過訓練了! 可以接受考核了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model.\n",
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda:iris_data.eval_input_fn(test_x, test_y,\n",
    "                                            args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ 複習一下 **eval_result 啥意思？\n",
    "%f eval_result . cr\n",
    "%f eval_result :> ['accuracy'] . cr\n",
    "{**eval_result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt = {'accuracy': 0.96666664, 'average_loss': 0.056607112, 'loss': 1.6982133, 'global_step': 1000}\n",
    "print('Test set accuracy: {loss:0.3f}'.format(**rslt))\n",
    "print('Test set accuracy: {:0.3f}'.format(r['accuracy']))\n",
    "print('Test set accuracy: {a:0.3f}'.format(**{'a': 11,'b': 22}))\n",
    "print('Test set accuracy: {b:0.3f}'.format(**{'a': 33,'b': 44}))\n",
    "%f \\ 最後一式拆解不成功，拆開來的寫法也沒意義，要嘛直接寫就好了。\n",
    "%f \\ print('Test set accuracy: {b:0.3f}'.format(({'a': 55},{'b': 66})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這一段的意思是, 準備了 3 或 4 組資料 predict_x 分別的正確答案如 expected 所列\n",
    "# Generate predictions from the model\n",
    "expected = ['Setosa', 'Versicolor', 'Virginica','Versicolor']\n",
    "predict_x = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9,6.2],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1,2.9],\n",
    "    'PetalLength': [1.7, 4.2, 5.4,4.3],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1,1.3],\n",
    "}\n",
    "\n",
    "[(i,j) for i,j in zip(predict_x,expected)]\n",
    "[i for i in zip(predict_x,expected)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(\n",
    "    input_fn=lambda:iris_data.eval_input_fn(predict_x,\n",
    "                                            labels=None,\n",
    "                                            batch_size=args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這樣寫也可以\n",
    "predictions = classifier.predict(\n",
    "    lambda:iris_data.eval_input_fn(predict_x,\n",
    "                                            labels=None,\n",
    "                                            batch_size=args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f predictions dir . cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ 以上 predictions 瞬間就取得了\n",
    "%f predictions tib.\n",
    "%f \\ 這 'generator' 用過就沒了，要再看一遍就得重新跑一次 classifier.predict()\n",
    "pred = [i for i in predictions]\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_dict, expec in zip(predictions, expected):\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print(template.format(iris_data.SPECIES[class_id],\n",
    "                          100 * probability, expec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip?\n",
    "%f \\ zip() 傳回 iterator 像拉拉鍊一樣，所有第一個一組、所有第二個一組、如此以往到哪個先結束為止\n",
    "z = zip((1,2,3,4),(11,22,33),{'aa':1,'bb':2})\n",
    "%f z type tib.\n",
    "z1 = [i for i in z]\n",
    "z2 = [i for i in z]\n",
    "%f z1 tib.\n",
    "%f z2 tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[(pred_dict, expec) for pred_dict, expec in zip(pred, expected)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以賣弄這個方法時 pred （測試資料）的個數與 expected （標準結果）就要吻合才有意思。以下簡單就是 prediction 與標準答案的比對列出來看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pred_dict, expec in zip(pred, expected):\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print(template.format(iris_data.SPECIES[class_id],\n",
    "                          100 * probability, expec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下是原本的教學程式\n",
    "\n",
    "### premade_estimator.py\n",
    "c:\\Users\\hcche\\Documents\\GitHub\\models\\samples\\core\\get_started\\premade_estimator.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    args = parser.parse_args(\"\")  # default is ([\"--batch_size=100\", \"--train_steps=1000\"])\n",
    "    # Fetch the data\n",
    "    (train_x, train_y), (test_x, test_y) = iris_data.load_data()\n",
    "\n",
    "    # Feature columns describe how to use the input.\n",
    "    my_feature_columns = []\n",
    "    for key in train_x.keys():\n",
    "        my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "\n",
    "    # Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=my_feature_columns,\n",
    "        # Two hidden layers of 10 nodes each.\n",
    "        hidden_units=[10, 10],\n",
    "        # The model must choose between 3 classes.\n",
    "        n_classes=3)\n",
    "\n",
    "    # Train the Model.\n",
    "    classifier.train(\n",
    "        input_fn=lambda:iris_data.train_input_fn(train_x, train_y,\n",
    "                                                 args.batch_size),\n",
    "        steps=args.train_steps)\n",
    "\n",
    "    # Evaluate the model.\n",
    "    eval_result = classifier.evaluate(\n",
    "        input_fn=lambda:iris_data.eval_input_fn(test_x, test_y,\n",
    "                                                args.batch_size))\n",
    "\n",
    "    print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
    "\n",
    "    # Generate predictions from the model\n",
    "    expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "    predict_x = {\n",
    "        'SepalLength': [5.1, 5.9, 6.9],\n",
    "        'SepalWidth': [3.3, 3.0, 3.1],\n",
    "        'PetalLength': [1.7, 4.2, 5.4],\n",
    "        'PetalWidth': [0.5, 1.5, 2.1],\n",
    "    }\n",
    "\n",
    "    predictions = classifier.predict(\n",
    "        input_fn=lambda:iris_data.eval_input_fn(predict_x,\n",
    "                                                labels=None,\n",
    "                                                batch_size=args.batch_size))\n",
    "\n",
    "    template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
    "\n",
    "    for pred_dict, expec in zip(predictions, expected):\n",
    "        class_id = pred_dict['class_ids'][0]\n",
    "        probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "        print(template.format(iris_data.SPECIES[class_id],\n",
    "                              100 * probability, expec))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    tf.app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iris_data.py\n",
    "c:\\Users\\hcche\\Documents\\GitHub\\models\\samples\\core\\get_started\\iris_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n",
    "                    'PetalLength', 'PetalWidth', 'Species']\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "def maybe_download():\n",
    "    train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)\n",
    "    test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)\n",
    "\n",
    "    return train_path, test_path\n",
    "\n",
    "# 這個有用到\n",
    "def load_data(y_name='Species'):\n",
    "    \"\"\"Returns the iris dataset as (train_x, train_y), (test_x, test_y).\"\"\"\n",
    "    train_path, test_path = maybe_download()\n",
    "\n",
    "    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    train_x, train_y = train, train.pop(y_name)\n",
    "\n",
    "    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    test_x, test_y = test, test.pop(y_name)\n",
    "\n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "# 這個有用到\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# 這個有用到\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    features=dict(features)\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# The remainder of this file contains a simple example of a csv parser,\n",
    "#     implemented using the `Dataset` class.\n",
    "\n",
    "# `tf.parse_csv` sets the types of the outputs to match the examples given in\n",
    "#     the `record_defaults` argument.\n",
    "CSV_TYPES = [[0.0], [0.0], [0.0], [0.0], [0]]\n",
    "\n",
    "def _parse_line(line):\n",
    "    # Decode the line into its fields\n",
    "    fields = tf.decode_csv(line, record_defaults=CSV_TYPES)\n",
    "\n",
    "    # Pack the result into a dictionary\n",
    "    features = dict(zip(CSV_COLUMN_NAMES, fields))\n",
    "\n",
    "    # Separate the label from the features\n",
    "    label = features.pop('Species')\n",
    "\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def csv_input_fn(csv_path, batch_size):\n",
    "    # Create a dataset containing the text lines.\n",
    "    dataset = tf.data.TextLineDataset(csv_path).skip(1)\n",
    "\n",
    "    # Parse each line.\n",
    "    dataset = dataset.map(_parse_line)\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ----- play ground -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates %%csv cell magic. See http://ipython-books.github.io/14-creating-an-ipython-extension-with-custom-magic-commands\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from IPython.core.magic import (register_cell_magic)\n",
    "@register_cell_magic\n",
    "def csv(line, cell):\n",
    "    return pd.read_csv(StringIO(cell), sep=line, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%csv \\s+\n",
    "a b c\n",
    "1 2 3\n",
    "4 5 6\n",
    "7 8 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f words module\n",
    "%f help modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f none value locals\n",
    "def smallworld():\n",
    "    x = 11\n",
    "    y = 22\n",
    "    peforth.push(locals()).dictate('to locals').ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peforth.ok()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn=lambda:iris_data.train_input_fn(train_x, train_y, args.batch_size)\n",
    "%f \\ lambda:iris_data.train_input_fn(_x, _y, _size)  # 定義時 identifiers 可以亂寫,那這好像是 macro 了?\n",
    "%f input_fn constant i/f\n",
    "%f i/f . cr\n",
    "%f i/f type . cr\n",
    "%f i/f dir . cr\n",
    "%f i/f :> () . cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda:locals\n",
    "\n",
    "def outer():\n",
    "    x = '1'\n",
    "    y = '2'\n",
    "    \n",
    "    def given(locs):\n",
    "        return(locs)\n",
    "    \n",
    "    def getit(locs=locals()):\n",
    "        return(locs) \n",
    "    \n",
    "    def getit2(locs=locals):\n",
    "        return(locs()) \n",
    "    \n",
    "    def testgiven():\n",
    "        x = 33\n",
    "        y = 44\n",
    "        l = given(locals())\n",
    "        return(x,y,l)\n",
    "    \n",
    "    def testgetit():\n",
    "        x = 55\n",
    "        y = 66\n",
    "        l = getit()\n",
    "        return(x,y,l)\n",
    "    \n",
    "    def testgetit2():\n",
    "        x = 77\n",
    "        y = 88\n",
    "        l = getit2()\n",
    "        return(x,y,l)\n",
    "\n",
    "    return testgiven(), testgetit(), testgetit2()\n",
    "\n",
    "outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myDebugger(loc):\n",
    "    print(loc)\n",
    "\n",
    "def debuggee():\n",
    "    x = 11\n",
    "    peforth.ok('11>', loc=dict(locals()))\n",
    "    x = 22\n",
    "    peforth.ok('22>', loc=dict(locals()))\n",
    "    x = 33\n",
    "    peforth.ok('33>', loc=dict(locals()))\n",
    "\n",
    "debuggee()    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
