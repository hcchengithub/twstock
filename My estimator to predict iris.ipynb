{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 預測「未來 dayforesee 天會不會漲 20%？」\n",
    "\n",
    "實際 4638天當中有 __ 天符合條件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "alert(\"要不要把 model/DNN directory 先殺掉？\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "alert(\"要不要把 model/DNN directory 先殺掉？\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hcche\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import peforth as f\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reDef unknown\n",
      "reDef \\\n"
     ]
    }
   ],
   "source": [
    "%%f\n",
    "\\ Now we redefine the 'unknown' command that does nothing at default  \n",
    ": unknown ( token -- thing Y|N) // Try to find the unknown token in __main__\n",
    "  py> getattr(sys.modules['__main__'],pop(),\"Ûnknôwn\") \n",
    "  py> str(tos())==\"Ûnknôwn\" if drop false else true then ;\n",
    "  /// here after, when FORTH come accross an unknown token, instead of alerting \n",
    "  /// it try to find the token in python __main__ module name space.\n",
    ": path-to-find-modules ( <path> -- ) // Add path to sys.path so \"import module-name\" can find the module\n",
    "    CR word trim ( \"path\" ) py: sys.path.append(pop()) ;\n",
    "code \\ print(nexttoken('\\n')) end-code // Redefine \\ command to print the comment line \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('000777.csv',encoding='gbk',parse_dates=[0],index_col=0)\n",
    "\n",
    "# data frame 中帶有 None 的 rows 都剔除掉。 本來有 4752 rows 現在只剩 4639 \n",
    "# 用法: df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "data = data.replace(to_replace='None', value=np.nan).dropna()  \n",
    "\n",
    "# pdb.set_trace() # data 本來是新的在上面\n",
    "data.sort_index(0,ascending=True,inplace=True) # 排序 sort 過後變成最舊的在上面\n",
    "\n",
    "# 上市第一天的資料不要，因為【漲跌幅】無效。\n",
    "data = data.query('index > \"1997-07-10\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data has 20 years long, but here only analysis the last 150 days of a chosen day\n",
    "dayfeature=150\n",
    "dayforesee=10  # 問未來幾天如何如何，最後就要扣掉幾天，避免 out of bounds. 0 代表當天。\n",
    "featurenum=5*dayfeature  # 取五個有關係的欄位，特徵值。\n",
    "x=np.zeros((data.shape[0]-dayfeature-dayforesee,featurenum+1))\n",
    "y=np.zeros((data.shape[0]-dayfeature-dayforesee));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重疊 shift 150 天的五個「行情數目」加上下一天的開盤 共 751 個「數目」當作 feature X \n",
    "\n",
    "for i in range(0,data.shape[0]-dayfeature-dayforesee):\n",
    "    x[i,0:featurenum]=np.array(data[i:i+dayfeature] \\\n",
    "          [[u'收盘价',u'最高价',u'最低价',u'开盘价',u'成交量']]).reshape((1,featurenum))\n",
    "    # x[i,featurenum]=data.ix[i+dayfeature][u'开盘价']  # ix deprecated\n",
    "    x[i,featurenum]=data.iloc[i+dayfeature][u'开盘价']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預測未來 dayforesee 天是否會「漲20%」?\n",
    "\n",
    "for i in range(0,data.shape[0]-dayfeature-dayforesee):\n",
    "    y[i]=0              \n",
    "    pi = float(data.iloc[i][u'收盘价'])\n",
    "    for j in range(dayforesee):\n",
    "        pi5 = float(data.iloc[j+i+dayfeature][u'最高价'])\n",
    "        if (pi5-pi)/pi >= 0.20 :\n",
    "            y[i]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.int32(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 有這麼多天符合條件 ) y py> sum(pop()) tib. \\ ==> 1677 (<class 'numpy.int32'>)\n",
      "x_train :> shape tib. \\ ==> (3582, 751) (<class 'tuple'>)\n",
      "x_test :> shape tib. \\ ==> (896, 751) (<class 'tuple'>)\n",
      "y_train :> shape tib. \\ ==> (3582,) (<class 'tuple'>)\n",
      "y_test :> shape tib. \\ ==> (896,) (<class 'tuple'>)\n",
      "x_train :> shape[0] tib. \\ ==> 3582 (<class 'int'>)\n"
     ]
    }
   ],
   "source": [
    "%f ( 有這麼多天符合條件 ) y py> sum(pop()) tib.\n",
    "%f x_train :> shape tib.\n",
    "%f x_test :> shape tib.\n",
    "%f y_train :> shape tib.\n",
    "%f y_test :> shape tib.\n",
    "%f x_train :> shape[0] tib.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_NumericColumn(key='x', shape=(751,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 應該只有一個 feature 是個 shape = (751,) 的 array. 這怎麼表達?\n",
    "my_feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[751,])]  \n",
    "my_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'model/DNN', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000245DC10B1D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[256,128],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=2,\n",
    "    dropout=0.4,\n",
    "    model_dir=\"model/DNN\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training inputs 從 mnist 抄過來改的\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_train},\n",
    "    y=y_train,\n",
    "    num_epochs=None,\n",
    "    batch_size=1000,\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into model/DNN\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1706707600.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 36.4018\n",
      "INFO:tensorflow:loss = 25318508.0, step = 101 (2.749 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.109\n",
      "INFO:tensorflow:loss = 8594579.0, step = 201 (2.696 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.1119\n",
      "INFO:tensorflow:loss = 1974630.0, step = 301 (2.768 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.6263\n",
      "INFO:tensorflow:loss = 894211.6, step = 401 (3.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.3098\n",
      "INFO:tensorflow:loss = 888675.2, step = 501 (3.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.619\n",
      "INFO:tensorflow:loss = 313553.72, step = 601 (2.976 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.7729\n",
      "INFO:tensorflow:loss = 265957.4, step = 701 (2.874 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.1485\n",
      "INFO:tensorflow:loss = 262160.22, step = 801 (2.931 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.953\n",
      "INFO:tensorflow:loss = 176720.83, step = 901 (2.943 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.1775\n",
      "INFO:tensorflow:loss = 168486.3, step = 1001 (2.928 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.6022\n",
      "INFO:tensorflow:loss = 258144.14, step = 1101 (2.976 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.5228\n",
      "INFO:tensorflow:loss = 271455.06, step = 1201 (2.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.2065\n",
      "INFO:tensorflow:loss = 75072.78, step = 1301 (2.923 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.016\n",
      "INFO:tensorflow:loss = 72275.54, step = 1401 (2.942 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.4739\n",
      "INFO:tensorflow:loss = 35960.62, step = 1501 (2.986 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.6057\n",
      "INFO:tensorflow:loss = 214138.67, step = 1601 (2.889 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.7033\n",
      "INFO:tensorflow:loss = 67042.59, step = 1701 (2.969 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.2297\n",
      "INFO:tensorflow:loss = 49125.152, step = 1801 (2.919 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.0966\n",
      "INFO:tensorflow:loss = 80629.52, step = 1901 (2.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.9016\n",
      "INFO:tensorflow:loss = 7302.2754, step = 2001 (2.950 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.1165\n",
      "INFO:tensorflow:loss = 11828.256, step = 2101 (3.018 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.8845\n",
      "INFO:tensorflow:loss = 9696.994, step = 2201 (2.953 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.039\n",
      "INFO:tensorflow:loss = 26226.996, step = 2301 (2.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.5761\n",
      "INFO:tensorflow:loss = 35403.598, step = 2401 (2.891 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.085\n",
      "INFO:tensorflow:loss = 21803.979, step = 2501 (2.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.9416\n",
      "INFO:tensorflow:loss = 12351.984, step = 2601 (2.943 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.4108\n",
      "INFO:tensorflow:loss = 19903.258, step = 2701 (2.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.647\n",
      "INFO:tensorflow:loss = 7663.0864, step = 2801 (2.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.3288\n",
      "INFO:tensorflow:loss = 208873.08, step = 2901 (2.912 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into model/DNN\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 14867.475.\n"
     ]
    }
   ],
   "source": [
    "classifier.train(input_fn=train_input_fn, steps=3000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test inputs  從 mnist 抄過來改的\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_test},\n",
    "    y=y_test,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-03-08:37:37\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model/DNN\\model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-03-08:37:38\n",
      "INFO:tensorflow:Saving dict for global step 3000: accuracy = 0.62053573, accuracy_baseline = 0.60491073, auc = 0.6149124, auc_precision_recall = 0.5510735, average_loss = 580.3221, global_step = 3000, label/mean = 0.3950893, loss = 74281.23, precision = 0.5421687, prediction/mean = 0.44034973, recall = 0.2542373\n",
      "\n",
      "Test Accuracy: 62.053573%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))\n",
    "\n",
    "# End of \"mnist_estimator.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用全部資料來比對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make input function for prediction:\n",
    "#   shuffle=False -> do not randomize input data\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x},\n",
    "    y=y,\n",
    "    num_epochs=1,  # Just one epoch\n",
    "    shuffle=False, # Don't shuffle so we can compare to true labels later\n",
    "    )  # 參考 GitHub/tensorflow-workshop/examples/07_structured_data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從 wh300/DNNClassifier_Predicting.ipynb 抄來的例子\n",
    "# Predict 整批 test input \n",
    "preds = classifier.predict(pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model/DNN\\model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predict = [i for i in preds];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [int(i['probabilities'][1] > i['probabilities'][0]) for i in predict];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions py> sum(pop()) tib. \\ ==> 817 (<class 'int'>)\n",
      "y py> sum(pop()) tib. \\ ==> 1677 (<class 'numpy.int32'>)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "%f predictions py> sum(pop()) tib.\n",
    "%f y py> sum(pop()) tib.\n",
    "%f y :> [:50] . cr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories ( 沒事的，漏掉的，多的，一致的 ) tib. \\ ==> [2418, 1243, 383, 434] (<class 'list'>)\n"
     ]
    }
   ],
   "source": [
    "# 查看 AI 預測的結果，與人工 tag 比較。\n",
    "categories = [0,0,0,0]\n",
    "for i in range(y.shape[0]): \n",
    "    if int(predictions[i])==int(y[i]) and int(y[i])==0: categories[0]+=1; continue\n",
    "    if int(predictions[i])!=int(y[i]) and int(y[i])==1: categories[1]+=1; continue\n",
    "    if int(predictions[i])!=int(y[i]) and int(y[i])==0: categories[2]+=1; continue\n",
    "    if int(predictions[i])==int(y[i]) and int(y[i])==1: categories[3]+=1; continue\n",
    "%f categories ( 沒事的，漏掉的，多的，一致的 ) tib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 只用 Test dataset 來比對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make input function for prediction:\n",
    "#   shuffle=False -> do not randomize input data\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_test},\n",
    "    y=y_test,\n",
    "    num_epochs=1,  # Just one epoch\n",
    "    shuffle=False, # Don't shuffle so we can compare to true labels later\n",
    "    )  # 參考 GitHub/tensorflow-workshop/examples/07_structured_data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從 wh300/DNNClassifier_Predicting.ipynb 抄來的例子\n",
    "# Predict 整批 test input \n",
    "preds = classifier.predict(pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model/DNN\\model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predict = [i for i in preds];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [int(i['probabilities'][1] > i['probabilities'][0]) for i in predict];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions py> sum(pop()) tib. \\ ==> 166 (<class 'int'>)\n",
      "y py> sum(pop()) tib. \\ ==> 1677 (<class 'numpy.int32'>)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "%f predictions py> sum(pop()) tib.\n",
    "%f y py> sum(pop()) tib.\n",
    "%f y :> [:50] . cr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories ( 沒事的，漏掉的，多的，一致的 ) tib. \\ ==> [433, 297, 110, 56] (<class 'list'>)\n"
     ]
    }
   ],
   "source": [
    "# 查看 AI 預測的結果，與人工 tag 比較。\n",
    "categories = [0,0,0,0]\n",
    "for i in range(y_test.shape[0]): \n",
    "    if int(predictions[i])==int(y[i]) and int(y[i])==0: categories[0]+=1; continue\n",
    "    if int(predictions[i])!=int(y[i]) and int(y[i])==1: categories[1]+=1; continue\n",
    "    if int(predictions[i])!=int(y[i]) and int(y[i])==0: categories[2]+=1; continue\n",
    "    if int(predictions[i])==int(y[i]) and int(y[i])==1: categories[3]+=1; continue\n",
    "%f categories ( 沒事的，漏掉的，多的，一致的 ) tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!start tensorboard --logdir=\"model/DNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 都搞懂了，接下來打包 000777.csv 試試看吧!\n",
    "\n",
    "這把預測「明天會不會漲停？」結果因為實際 4638天當中只有88天漲停，全部猜「不」就有 98.10％ 的 accuracy. 可能正是因為怎麼猜別的都沒有這個好。 DNN 全部猜「不會」。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "alert(\"要不要把 DNN_model directory 先殺掉？\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import peforth as f\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%f\n",
    "\\ Now we redefine the 'unknown' command that does nothing at default  \n",
    ": unknown ( token -- thing Y|N) // Try to find the unknown token in __main__\n",
    "  py> getattr(sys.modules['__main__'],pop(),\"Ûnknôwn\") \n",
    "  py> str(tos())==\"Ûnknôwn\" if drop false else true then ;\n",
    "  /// here after, when FORTH come accross an unknown token, instead of alerting \n",
    "  /// it try to find the token in python __main__ module name space.\n",
    ": path-to-find-modules ( <path> -- ) // Add path to sys.path so \"import module-name\" can find the module\n",
    "    CR word trim ( \"path\" ) py: sys.path.append(pop()) ;\n",
    "code \\ print(nexttoken('\\n')) end-code // Redefine \\ command to print the comment line \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('000777.csv',encoding='gbk',parse_dates=[0],index_col=0)\n",
    "\n",
    "# data frame 中帶有 None 的 rows 都剔除掉。 本來有 4752 rows 現在只剩 4639 \n",
    "# 用法: df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "data = data.replace(to_replace='None', value=np.nan).dropna()  \n",
    "\n",
    "# pdb.set_trace() # data 本來是新的在上面\n",
    "data.sort_index(0,ascending=True,inplace=True) # 排序 sort 過後變成最舊的在上面\n",
    "\n",
    "# 上市第一天的資料不要，因為【漲跌幅】無效。\n",
    "data = data.query('index > \"1997-07-10\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data has 20 years long, but here only analysis the last 150 days of a chosen day\n",
    "dayfeature=150\n",
    "dayforesee=5  # 問未來幾天如何如何，最後就要扣掉幾天，避免 out of bounds. 0 代表當天。\n",
    "featurenum=5*dayfeature  # 取五個有關係的欄位，特徵值。\n",
    "x=np.zeros((data.shape[0]-dayfeature-dayforesee,featurenum+1))\n",
    "y=np.zeros((data.shape[0]-dayfeature-dayforesee));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重疊 shift 150 天的五個「行情數目」加上下一天的開盤 共 751 個「數目」當作 feature X \n",
    "\n",
    "for i in range(0,data.shape[0]-dayfeature-dayforesee):\n",
    "    x[i,0:featurenum]=np.array(data[i:i+dayfeature] \\\n",
    "          [[u'收盘价',u'最高价',u'最低价',u'开盘价',u'成交量']]).reshape((1,featurenum))\n",
    "    # x[i,featurenum]=data.ix[i+dayfeature][u'开盘价']  # ix deprecated\n",
    "    x[i,featurenum]=data.iloc[i+dayfeature][u'开盘价']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預測當天是否會「漲停」?\n",
    "\n",
    "# data.shape[0]-dayfeature 是 4489 同上 x, 所以這是在製作相對所有 x 的 y. x 是 (4488, 751)\n",
    "# dayfeature 是 150 \n",
    "# 直接看【涨跌幅】> 9.8 就算漲停板\n",
    "\n",
    "for i in range(0,data.shape[0]-dayfeature-dayforesee):\n",
    "    if float(data.iloc[i+dayfeature][u'涨跌幅'])>=9.8:\n",
    "        y[i]=1.0\n",
    "    else:\n",
    "        y[i]=0.0          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f x_train :> shape tib.\n",
    "%f x_test :> shape tib.\n",
    "%f y_train :> shape tib.\n",
    "%f y_test :> shape tib.\n",
    "%f x_train :> shape[0] tib.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用 cross_validation 切分好 train-test feature & label 之後, 可望弄成 input_fn 要的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 應該只有一個 feature 是個 shape = (751,) 的 array. 這怎麼表達?\n",
    "my_feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[751,])]  \n",
    "my_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[256, 128],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=2,\n",
    "    dropout=0.2,\n",
    "    model_dir=\"DNN_model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上 classifier 弄出來了，還是要面對 input_function 怎麼弄的問題。Lambda 太難懂了，從 wh300 或哪裡去抄吧。\n",
    "看到的都是 tf.estimator.inputs.pandas_input_fn 我曾查出有 'numpy_input_fn' \n",
    "\n",
    "    c:\\Users\\hcche\\Documents\\GitHub\\twstock\\mnist_estimator.ipynb \n",
    "    # Specify feature\n",
    "    feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[28, 28])]\n",
    "    幹！ 回想起來了，用這個例子可以簡化 150天 的資料製作方式\n",
    "\n",
    "Estimator 的 DNNClassifier 怎麼用? 上面這個 mnist 也有似乎更進一步的發揮\n",
    "\n",
    "    # Build 2 layer DNN classifier\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        hidden_units=[256, 32],\n",
    "        optimizer=tf.train.AdamOptimizer(1e-4),\n",
    "        n_classes=10,\n",
    "        dropout=0.1,\n",
    "        model_dir=\"./tmp/mnist_model\"\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training inputs 從 mnist 抄過來改的\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_train},\n",
    "    y=y_train,\n",
    "    num_epochs=None,\n",
    "    batch_size=x_train.shape[0],  # 漲停很少，故 batch 要用全部天數，否則它全部猜「不會」\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(input_fn=train_input_fn, steps=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test inputs  從 mnist 抄過來改的\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_test},\n",
    "    y=y_test,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))\n",
    "\n",
    "# End of \"mnist_estimator.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讚！ 接下來怎麼看 predict 的結果 . . . 參考 wh300 吧\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make input function for prediction:\n",
    "#   shuffle=False -> do not randomize input data\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x},\n",
    "    y=y,\n",
    "    num_epochs=1,  # Just one epoch\n",
    "    shuffle=False, # Don't shuffle so we can compare to true labels later\n",
    "    )  # 參考 GitHub/tensorflow-workshop/examples/07_structured_data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從 wh300/DNNClassifier_Predicting.ipynb 抄來的例子\n",
    "# Predict 整批 test input \n",
    "preds = classifier.predict(pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [int(i['probabilities'][1] > i['probabilities'][0]) for i in preds];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f predictions py> sum(pop()) tib.\n",
    "%f y py> sum(pop()) tib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我在 Tensorflow 官網的教學網頁上找到了用 Iris 當例題的 estimator tutorial \n",
    "\n",
    "https://www.tensorflow.org/versions/master/get_started/premade_estimators (also on my Ynote \"Get Started with Estimators _TensorFlow_\") 有正式、詳細的介紹。\n",
    "這篇文章鼓勵人用 Estimator API 跟 Dataset API。本文研讀、實驗這篇文章。\n",
    "文章引用的 GitHub resource 我已經 clone 下來了在 T550 `c:\\Users\\hcche\\Documents\\GitHub\\models`  \n",
    "\n",
    "重點是，直接用 pandas data-frame 來當作 feature - label 餵給 input function 這樣更自然，免去搞 150 天行情資料的麻煩。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peforth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%f\n",
    "\\ Now we redefine the 'unknown' command that does nothing at default  \n",
    ": unknown ( token -- thing Y|N) // Try to find the unknown token in __main__\n",
    "  py> getattr(sys.modules['__main__'],pop(),\"Ûnknôwn\") \n",
    "  py> str(tos())==\"Ûnknôwn\" if drop false else true then ;\n",
    "  /// here after, when FORTH come accross an unknown token, instead of alerting \n",
    "  /// it try to find the token in python __main__ module name space.\n",
    "\n",
    ": path-to-find-modules ( <path> -- ) // Add path to sys.path so \"import module-name\" can find the module\n",
    "    CR word trim ( \"path\" ) py: sys.path.append(pop()) ;\n",
    "\n",
    "\\ set path for 'import iris_data'\n",
    "path-to-find-modules c:\\Users\\hcche\\Documents\\GitHub\\models\\samples\\core\\get_started\n",
    "\n",
    "code \\ print(nexttoken('\\n')) end-code // Redefine \\ command to print the comment line \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(np.array(irisX).reshape(-1,4), columns = iris.feature_names)    \n",
    "x = pd.DataFrame(iris.data, columns = iris.feature_names)    \n",
    "x[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(iris.target, columns = [\"target\"])    \n",
    "y[48:52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/pandas_input_fn\n",
    "\n",
    "tf.estimator.inputs.pandas_input_fn(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    num_epochs=5,\n",
    "    shuffle=False,\n",
    "    queue_capacity=None,\n",
    "    num_threads=1,\n",
    "    target_column='target'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 照 Tutorial 執行 -- 一把就成功！\n",
    "\n",
    "直接把本 notebook 建個 hard link 到 c:\\Users\\hcche\\Documents\\GitHub\\models\\samples\\core\\get_started 去執行...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run premade_estimator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開始照教材研讀\n",
    "    \n",
    "premade_estimator.py iris_data.py 都抓過來，他本來都放在 main() 內執行，不利 study 今予拆開，分段執行亦可，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "\"\"\"An Example of a DNNClassifier for the Iris dataset.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "import iris_data  # iris_data.py 在本目錄下 GitHub\\models\\samples\\core\\get_started\\iris_data.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這 argparse module 很常見 DeepSpeech 就有，這裡用不著吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', default=100, type=int, help='batch size')\n",
    "parser.add_argument('--train_steps', default=1000, type=int,\n",
    "                    help='number of training steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 不能在 jupyter notebook 下直接跑，因為 argv 不如預期。\n",
    "from IPython.display import display,Image;display(Image('error1.jpg', width=700))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "研究一下 parser (argparse module) 怎麼回事兒 <== 只能在 python 用，ipython & jupyter notebook 都不行\n",
    "http://localhost:8888/notebooks/Documents/GitHub/models/samples/core/get_started/My%20estimator%20to%20predict%20iris.ipynb\n",
    "\n",
    "[x] 這是在 Jupyter Notebook 下記錄到的\n",
    "    OK Argv . cr\n",
    "    ['c:\\\\users\\\\hcche\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\site-packages\\\\ipykernel_launcher.py', '-f', 'C:\\\\Users\\\\hcche\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-d21c0b87-4db1-432d-82ca-cd398f2dd5ad.json']\n",
    "    OK \n",
    "    看起來有三個 args -- .py script 本身, -f , 還有一個 .json, 後兩個是 jupyter notebook 加的，難怪\n",
    "    要出問題。\n",
    "\n",
    "[x] 改用 ipython 直接跑 premade_estimator.py 看看 .... 成功。 \n",
    "    --> 那他看到的 Argv 是怎樣? -->['c:\\\\Users\\\\hcche\\\\Documents\\\\GitHub\\\\models\\\\samples\\\\core\\\\get_started\\\\premade_estimator.py']\n",
    "        等於是只有 argv[0] 的 .py script 本身一個 args 那就沒問題了。\n",
    "\n",
    "[x] 我猜測了一下，矇對了，這樣執行：\n",
    "    ipython premade_estimator.py --batch_size=1234 --train_steps=5678\n",
    "    搗出了一些 warning 警告說 args 可能有問題，但總之得到的 args 是：\n",
    "    argv 是這樣 \n",
    "    ['c:\\\\Users\\\\hcche\\\\Documents\\\\GitHub\\\\models\\\\samples\\\\core\\\\get_started\\\\premade_estimator.py']\n",
    "    args 是這樣\n",
    "    Namespace(batch_size=100, train_steps=1000)\n",
    "[x] 確定不能用 ipython, 用 python 以下皆可：\n",
    "    python premade_estimator.py --batch_size=64 --train_steps=1000\n",
    "    python premade_estimator.py --batch_size 64 --train_steps 1000\n",
    "\n",
    "反正我現在知道怎麼讓 jupyter notebook 能跑了! 改這樣即可： \n",
    "    args = parser.parse_args(\"\")  # 用 default 值\n",
    "    args = parser.parse_args([\"--batch_size=100\", \"--train_steps=1000\"])  # 代替 command line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Command Line Arguments 要用模擬的\n",
    "Module argparse 只能在 python command line 跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ [x] 沒有指定時，自動取 default 值，真不知道這哪來的？看到了，從 parser 那邊 code 裡給的。\n",
    "args = parser.parse_args([\"--batch_size=100\", \"--train_steps=1000\"])\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 研究  (train_x, train_y), (test_x, test_y) = iris_data.load_data()\n",
    "\n",
    "看出來了 iris_data.py 是把 150 筆資料切成 120 : 30 for training and testing respectively.\n",
    "這是個壞消息，他沒有像 sklearn 的自動切的那啥 function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " (train_x, train_y), (test_x, test_y) = iris_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f train_x type . cr\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f train_y type . cr\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f test_x type . cr\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f test_y type . cr\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ [ ] 這是 feature columns, 我晚點可以嘗試把它弄成光一個 shape 為 (4,) 的單一 feature \n",
    "for key in train_x.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ [ ] 這是 feature columns, 我晚點可以嘗試把它弄成光一個 shape 為 (4,) 的單一 feature \n",
    "my_feature_columns = []\n",
    "for key in train_x.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "my_feature_columns    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%f \\ 從 main 裡拆分出來，方便 tracing. 可看出若無指定 model directory 自動放在\n",
    "%f \\     AppData\\Local\\Temp\\tmpijrg8mkm <-- 每次都不一樣，這樣好嗎？\n",
    "%f \\ 以及本 classifier 的種種細節\n",
    "%f \\ \n",
    "\n",
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[10, 10],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ 複習一下 lambda --> \n",
    "%f \\ lambda 是個 function, 產生 key(i) 取 x 的第 i 個 element。用於 sorted() 時指定哪個當 key \n",
    "s = [('a', 3), ('b', 2), ('c', 1)]\n",
    "key = lambda x:x[1]\n",
    "print(sorted(s, key=key))\n",
    "%f \\ 單獨使用時，看起來不太一樣，其實做的是一樣的事情。\n",
    "%f key :> ([('a',3),('b',2),('c',1),('d',4)]) tib.\n",
    "%f key tib.\n",
    "%f key .source\n",
    "%f \\ 當沒有給 lambda argument 時，是怎樣？\n",
    "x = lambda:s\n",
    "%f x tib. \\ 直接傳回 : 之後的東西(執行後)的傳回值\n",
    "%f x type tib.\n",
    "%f \\ 查看 x 的 source code \n",
    "%f x .source\n",
    "%f x :> () tib.\n",
    "\n",
    "%f \\ 所以，input_fn = lambda:iris_data.train_input_fn(train_x, train_y,args.batch_size)\n",
    "%f \\ 當中的 iris_data.train_input_fn() 是傳回一個 function 但是該 function 的 argument 是活的，\n",
    "%f \\ 要延後到最後一秒鐘才確定，因此要用 lambda 或單純的 function 包一層用來指定 arguments。\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%f \\ 這就開始訓練了 . . .  \n",
    "%f \\ \n",
    "\n",
    "# Train the Model.\n",
    "classifier.train(\n",
    "    input_fn=lambda:iris_data.train_input_fn(train_x, train_y,\n",
    "                                             args.batch_size),\n",
    "    steps=args.train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這時候的 classifier 已經受過訓練了! 可以接受考核了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model.\n",
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda:iris_data.eval_input_fn(test_x, test_y,\n",
    "                                            args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ 複習一下 **eval_result 啥意思？\n",
    "%f eval_result . cr\n",
    "%f eval_result :> ['accuracy'] . cr\n",
    "{**eval_result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt = {'accuracy': 0.96666664, 'average_loss': 0.056607112, 'loss': 1.6982133, 'global_step': 1000}\n",
    "print('Test set accuracy: {loss:0.3f}'.format(**rslt))\n",
    "print('Test set accuracy: {:0.3f}'.format(r['accuracy']))\n",
    "print('Test set accuracy: {a:0.3f}'.format(**{'a': 11,'b': 22}))\n",
    "print('Test set accuracy: {b:0.3f}'.format(**{'a': 33,'b': 44}))\n",
    "%f \\ 最後一式拆解不成功，拆開來的寫法也沒意義，要嘛直接寫就好了。\n",
    "%f \\ print('Test set accuracy: {b:0.3f}'.format(({'a': 55},{'b': 66})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這一段的意思是, 準備了 3 或 4 組資料 predict_x 分別的正確答案如 expected 所列\n",
    "# Generate predictions from the model\n",
    "expected = ['Setosa', 'Versicolor', 'Virginica','Versicolor']\n",
    "predict_x = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9,6.2],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1,2.9],\n",
    "    'PetalLength': [1.7, 4.2, 5.4,4.3],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1,1.3],\n",
    "}\n",
    "\n",
    "[(i,j) for i,j in zip(predict_x,expected)]\n",
    "[i for i in zip(predict_x,expected)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(\n",
    "    input_fn=lambda:iris_data.eval_input_fn(predict_x,\n",
    "                                            labels=None,\n",
    "                                            batch_size=args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這樣寫也可以\n",
    "predictions = classifier.predict(\n",
    "    lambda:iris_data.eval_input_fn(predict_x,\n",
    "                                            labels=None,\n",
    "                                            batch_size=args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f predictions dir . cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ 以上 predictions 瞬間就取得了\n",
    "%f predictions tib.\n",
    "%f \\ 這 'generator' 用過就沒了，要再看一遍就得重新跑一次 classifier.predict()\n",
    "pred = [i for i in predictions]\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_dict, expec in zip(predictions, expected):\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print(template.format(iris_data.SPECIES[class_id],\n",
    "                          100 * probability, expec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip?\n",
    "%f \\ zip() 傳回 iterator 像拉拉鍊一樣，所有第一個一組、所有第二個一組、如此以往到哪個先結束為止\n",
    "z = zip((1,2,3,4),(11,22,33),{'aa':1,'bb':2})\n",
    "%f z type tib.\n",
    "z1 = [i for i in z]\n",
    "z2 = [i for i in z]\n",
    "%f z1 tib.\n",
    "%f z2 tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[(pred_dict, expec) for pred_dict, expec in zip(pred, expected)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以賣弄這個方法時 pred （測試資料）的個數與 expected （標準結果）就要吻合才有意思。以下簡單就是 prediction 與標準答案的比對列出來看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pred_dict, expec in zip(pred, expected):\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print(template.format(iris_data.SPECIES[class_id],\n",
    "                          100 * probability, expec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下是原本的教學程式\n",
    "\n",
    "### premade_estimator.py\n",
    "c:\\Users\\hcche\\Documents\\GitHub\\models\\samples\\core\\get_started\\premade_estimator.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    args = parser.parse_args(\"\")  # default is ([\"--batch_size=100\", \"--train_steps=1000\"])\n",
    "    # Fetch the data\n",
    "    (train_x, train_y), (test_x, test_y) = iris_data.load_data()\n",
    "\n",
    "    # Feature columns describe how to use the input.\n",
    "    my_feature_columns = []\n",
    "    for key in train_x.keys():\n",
    "        my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "\n",
    "    # Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=my_feature_columns,\n",
    "        # Two hidden layers of 10 nodes each.\n",
    "        hidden_units=[10, 10],\n",
    "        # The model must choose between 3 classes.\n",
    "        n_classes=3)\n",
    "\n",
    "    # Train the Model.\n",
    "    classifier.train(\n",
    "        input_fn=lambda:iris_data.train_input_fn(train_x, train_y,\n",
    "                                                 args.batch_size),\n",
    "        steps=args.train_steps)\n",
    "\n",
    "    # Evaluate the model.\n",
    "    eval_result = classifier.evaluate(\n",
    "        input_fn=lambda:iris_data.eval_input_fn(test_x, test_y,\n",
    "                                                args.batch_size))\n",
    "\n",
    "    print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
    "\n",
    "    # Generate predictions from the model\n",
    "    expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "    predict_x = {\n",
    "        'SepalLength': [5.1, 5.9, 6.9],\n",
    "        'SepalWidth': [3.3, 3.0, 3.1],\n",
    "        'PetalLength': [1.7, 4.2, 5.4],\n",
    "        'PetalWidth': [0.5, 1.5, 2.1],\n",
    "    }\n",
    "\n",
    "    predictions = classifier.predict(\n",
    "        input_fn=lambda:iris_data.eval_input_fn(predict_x,\n",
    "                                                labels=None,\n",
    "                                                batch_size=args.batch_size))\n",
    "\n",
    "    template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
    "\n",
    "    for pred_dict, expec in zip(predictions, expected):\n",
    "        class_id = pred_dict['class_ids'][0]\n",
    "        probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "        print(template.format(iris_data.SPECIES[class_id],\n",
    "                              100 * probability, expec))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    tf.app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iris_data.py\n",
    "c:\\Users\\hcche\\Documents\\GitHub\\models\\samples\\core\\get_started\\iris_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n",
    "                    'PetalLength', 'PetalWidth', 'Species']\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "def maybe_download():\n",
    "    train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)\n",
    "    test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)\n",
    "\n",
    "    return train_path, test_path\n",
    "\n",
    "# 這個有用到\n",
    "def load_data(y_name='Species'):\n",
    "    \"\"\"Returns the iris dataset as (train_x, train_y), (test_x, test_y).\"\"\"\n",
    "    train_path, test_path = maybe_download()\n",
    "\n",
    "    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    train_x, train_y = train, train.pop(y_name)\n",
    "\n",
    "    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    test_x, test_y = test, test.pop(y_name)\n",
    "\n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "# 這個有用到\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# 這個有用到\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    features=dict(features)\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# The remainder of this file contains a simple example of a csv parser,\n",
    "#     implemented using the `Dataset` class.\n",
    "\n",
    "# `tf.parse_csv` sets the types of the outputs to match the examples given in\n",
    "#     the `record_defaults` argument.\n",
    "CSV_TYPES = [[0.0], [0.0], [0.0], [0.0], [0]]\n",
    "\n",
    "def _parse_line(line):\n",
    "    # Decode the line into its fields\n",
    "    fields = tf.decode_csv(line, record_defaults=CSV_TYPES)\n",
    "\n",
    "    # Pack the result into a dictionary\n",
    "    features = dict(zip(CSV_COLUMN_NAMES, fields))\n",
    "\n",
    "    # Separate the label from the features\n",
    "    label = features.pop('Species')\n",
    "\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def csv_input_fn(csv_path, batch_size):\n",
    "    # Create a dataset containing the text lines.\n",
    "    dataset = tf.data.TextLineDataset(csv_path).skip(1)\n",
    "\n",
    "    # Parse each line.\n",
    "    dataset = dataset.map(_parse_line)\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ----- play ground -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates %%csv cell magic. See http://ipython-books.github.io/14-creating-an-ipython-extension-with-custom-magic-commands\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from IPython.core.magic import (register_cell_magic)\n",
    "@register_cell_magic\n",
    "def csv(line, cell):\n",
    "    return pd.read_csv(StringIO(cell), sep=line, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%csv \\s+\n",
    "a b c\n",
    "1 2 3\n",
    "4 5 6\n",
    "7 8 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f words module\n",
    "%f help modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f none value locals\n",
    "def smallworld():\n",
    "    x = 11\n",
    "    y = 22\n",
    "    peforth.push(locals()).dictate('to locals').ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peforth.ok()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn=lambda:iris_data.train_input_fn(train_x, train_y, args.batch_size)\n",
    "%f \\ lambda:iris_data.train_input_fn(_x, _y, _size)  # 定義時 identifiers 可以亂寫,那這好像是 macro 了?\n",
    "%f input_fn constant i/f\n",
    "%f i/f . cr\n",
    "%f i/f type . cr\n",
    "%f i/f dir . cr\n",
    "%f i/f :> () . cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda:locals\n",
    "\n",
    "def outer():\n",
    "    x = '1'\n",
    "    y = '2'\n",
    "    \n",
    "    def given(locs):\n",
    "        return(locs)\n",
    "    \n",
    "    def getit(locs=locals()):\n",
    "        return(locs) \n",
    "    \n",
    "    def getit2(locs=locals):\n",
    "        return(locs()) \n",
    "    \n",
    "    def testgiven():\n",
    "        x = 33\n",
    "        y = 44\n",
    "        l = given(locals())\n",
    "        return(x,y,l)\n",
    "    \n",
    "    def testgetit():\n",
    "        x = 55\n",
    "        y = 66\n",
    "        l = getit()\n",
    "        return(x,y,l)\n",
    "    \n",
    "    def testgetit2():\n",
    "        x = 77\n",
    "        y = 88\n",
    "        l = getit2()\n",
    "        return(x,y,l)\n",
    "\n",
    "    return testgiven(), testgetit(), testgetit2()\n",
    "\n",
    "outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myDebugger(loc):\n",
    "    print(loc)\n",
    "\n",
    "def debuggee():\n",
    "    x = 11\n",
    "    peforth.ok('11>', loc=dict(locals()))\n",
    "    x = 22\n",
    "    peforth.ok('22>', loc=dict(locals()))\n",
    "    x = 33\n",
    "    peforth.ok('33>', loc=dict(locals()))\n",
    "\n",
    "debuggee()    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
