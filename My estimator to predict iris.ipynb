{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題很奇怪, DNN 的 prediction 全是 0 ! 或全是 1!\n",
    "\n",
    "改 x_train 等，依據某種規則直接手動改一改看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "alert(\"要不要把 model/DNN directory 先殺掉？\")\n",
    "alert(\"注意 batch_size 合不合理？一千也照跑，結果都是 0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "!rd /q /s model\\DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import peforth as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reDef unknown\n",
      "reDef \\\n"
     ]
    }
   ],
   "source": [
    "%%f\n",
    "\\ Now we redefine the 'unknown' command that does nothing at default  \n",
    ": unknown ( token -- thing Y|N) // Try to find the unknown token in __main__\n",
    "  py> getattr(sys.modules['__main__'],pop(),\"Ûnknôwn\") \n",
    "  py> str(tos())==\"Ûnknôwn\" if drop false else true then ;\n",
    "  /// here after, when FORTH come accross an unknown token, instead of alerting \n",
    "  /// it try to find the token in python __main__ module name space.\n",
    ": path-to-find-modules ( <path> -- ) // Add path to sys.path so \"import module-name\" can find the module\n",
    "    CR word trim ( \"path\" ) py: sys.path.append(pop()) ;\n",
    "code \\ print(nexttoken('\\n')) end-code // Redefine \\ command to print the comment line \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要測出 DNN 的真正實力，先用 random matrix 來試探它"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 這個 matrix 很簡單，寬度只有 2 而 label 是其中一條 column 的簡單關係"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.rand(300,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5488135 ,  0.71518937],\n",
       "       [ 0.60276338,  0.54488318],\n",
       "       [ 0.4236548 ,  0.64589411],\n",
       "       [ 0.43758721,  0.891773  ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  label 是其中一條 column 的簡單關係\n",
    "y = []\n",
    "for i in range(len(x)):\n",
    "    if x[i][0] >= 0.5:\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0, 1, 1, 1, 0, 0, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('000777.csv',encoding='gbk',parse_dates=[0],index_col=0)\n",
    "\n",
    "# data frame 中帶有 None 的 rows 都剔除掉。 本來有 4752 rows 現在只剩 4639 \n",
    "# 用法: df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "data = data.replace(to_replace='None', value=np.nan).dropna()  \n",
    "\n",
    "# pdb.set_trace() # data 本來是新的在上面\n",
    "data.sort_index(0,ascending=True,inplace=True) # 排序 sort 過後變成最舊的在上面\n",
    "\n",
    "# 上市第一天的資料不要，因為【漲跌幅】無效。\n",
    "data = data.query('index > \"1997-07-10\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data has 20 years long, but here only analysis the last 150 days of a chosen day\n",
    "dayfeature = 150\n",
    "oneshot = 5  # 漲停後慣性天數，使正反平衡\n",
    "dayforesee = oneshot  # 問未來幾天如何如何，最後就要扣掉幾天，避免 out of bounds. 0 代表當天。\n",
    "featurenum = 5*dayfeature  # 取五個有關係的欄位，ochlq 等五個特徵值。\n",
    "cross_validation_rate = 0.2\n",
    "xlength = data.shape[0]-dayfeature-dayforesee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xwidth = 2 # 751 or experiment width "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%f ( 總天數 ) data :> shape[0] tib. ( 4638 )\n",
    "%f ( leading days that are not used ) dayfeature tib.   \n",
    "%f ( latest days that are not used ) dayforesee tib.\n",
    "%f ( 有效天數 ) data :> shape[0] dayfeature - dayforesee - tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.zeros((xlength,featurenum+2))  # +2 是當天開盤跟當天日期\n",
    "y=np.zeros((xlength));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重疊 shift dayfeature (150) 天的五個「行情特徵值」加上當天的開盤 共 751 個參數當作 feature X \n",
    "for i in range(0,xlength):\n",
    "    x[i,0:featurenum]=np.array(data[i:i+dayfeature] \\\n",
    "          [[u'收盘价',u'最高价',u'最低价',u'开盘价',u'成交量']]).reshape((1,featurenum))\n",
    "    x[i,featurenum]=data.iloc[i+dayfeature][u'开盘价']\n",
    "    x[i,featurenum+1]=data.index[i+dayfeature].value  # backward pd.to_datetime(value).date()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _inspect_ 用【前】 10 天的漲跌來當指標，看 AI 能不能逮到規律？\n",
    "for i in range(len(y)):\n",
    "    if float(data.iloc[i+dayfeature-10][u'涨跌幅'])>=0:\n",
    "        y[i]=1.0\n",
    "    else:\n",
    "        y[i]=0.0          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(  ) x :> shape tib. \\ ==> (300, 2) (<class 'tuple'>)\n",
      "(  ) x :> shape[0] tib. \\ ==> 300 (<class 'int'>)\n",
      "(  ) y :> shape tib. \\ ==> (300,) (<class 'tuple'>)\n",
      "(  ) y :> shape[0] tib. \\ ==> 300 (<class 'int'>)\n",
      "(  ) y py> sum(pop()) tib. \\ ==> 158 (<class 'numpy.int32'>)\n"
     ]
    }
   ],
   "source": [
    "%f (  ) x :> shape tib. ( ?, xwidth )\n",
    "%f (  ) x :> shape[0] tib. ( ? )\n",
    "%f (  ) y :> shape tib. ( ?, )    \n",
    "%f (  ) y :> shape[0] tib. ( ? )    \n",
    "%f (  ) y py> sum(pop()) tib. ( 1629 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hcche\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f x_train np :> array(pop()).shape tib.\n",
    "%f y_train np :> array(pop()).shape tib.\n",
    "%f x_test np :> array(pop()).shape tib.\n",
    "%f y_test np :> array(pop()).shape tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train np :> array(pop()).shape tib. \\ ==> (240, 2) (<class 'tuple'>)\n",
      "y_train np :> array(pop()).shape tib. \\ ==> (240,) (<class 'tuple'>)\n",
      "x_test np :> array(pop()).shape tib. \\ ==> (60, 2) (<class 'tuple'>)\n",
      "y_test np :> array(pop()).shape tib. \\ ==> (60,) (<class 'tuple'>)\n",
      "\n",
      "Failed in </py> (compiling=False): name 'xwidth' is not defined\n",
      "Body:\n",
      "push(pop().array(pop())[:,:xwidth].shape)\n",
      "\n",
      "Failed in </py> (compiling=False): name 'xwidth' is not defined\n",
      "Body:\n",
      "push(pop().array(pop())[:,:xwidth].shape)\n"
     ]
    }
   ],
   "source": [
    "%f ( numpy 才能裁切 matrix ) x_train np :> array(pop())[:,:xwidth].shape tib.\n",
    "%f ( numpy 才能裁切 matrix ) x_test np :> array(pop())[:,:xwidth].shape tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_NumericColumn(key='x', shape=(2,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 應該只有一個 feature 是個 shape = (xwidth,) 的 array. 這怎麼表達?\n",
    "my_feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[xwidth,])]  \n",
    "my_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'model/DNN', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000297B5FAD128>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[32,32],\n",
    "    optimizer=tf.train.AdamOptimizer(1e-8),\n",
    "    n_classes=2,\n",
    "    dropout=0.2,\n",
    "    model_dir=\"model/DNN\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training inputs 從 mnist 抄過來改的\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(x_train)[:,:xwidth]},\n",
    "    y=np.array(y_train),\n",
    "    num_epochs=None,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into model/DNN\\model.ckpt.\n",
      "INFO:tensorflow:loss = 20.2658, step = 1\n",
      "INFO:tensorflow:global_step/sec: 174.315\n",
      "INFO:tensorflow:loss = 21.8276, step = 101 (0.578 sec)\n",
      "INFO:tensorflow:global_step/sec: 331.316\n",
      "INFO:tensorflow:loss = 21.4208, step = 201 (0.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 315.638\n",
      "INFO:tensorflow:loss = 20.5541, step = 301 (0.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 342.661\n",
      "INFO:tensorflow:loss = 22.6038, step = 401 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 329.136\n",
      "INFO:tensorflow:loss = 20.684, step = 501 (0.307 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.422\n",
      "INFO:tensorflow:loss = 20.6938, step = 601 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 319.671\n",
      "INFO:tensorflow:loss = 21.0169, step = 701 (0.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 342.665\n",
      "INFO:tensorflow:loss = 21.5496, step = 801 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 311.704\n",
      "INFO:tensorflow:loss = 21.4298, step = 901 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 323.808\n",
      "INFO:tensorflow:loss = 20.6098, step = 1001 (0.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 380.447\n",
      "INFO:tensorflow:loss = 20.033, step = 1101 (0.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 340.332\n",
      "INFO:tensorflow:loss = 20.9353, step = 1201 (0.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 306.924\n",
      "INFO:tensorflow:loss = 21.2726, step = 1301 (0.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.247\n",
      "INFO:tensorflow:loss = 21.7825, step = 1401 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 308.817\n",
      "INFO:tensorflow:loss = 21.4408, step = 1501 (0.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 299.573\n",
      "INFO:tensorflow:loss = 21.1772, step = 1601 (0.331 sec)\n",
      "INFO:tensorflow:global_step/sec: 313.659\n",
      "INFO:tensorflow:loss = 19.709, step = 1701 (0.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 304.125\n",
      "INFO:tensorflow:loss = 21.3248, step = 1801 (0.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.88\n",
      "INFO:tensorflow:loss = 20.4365, step = 1901 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 302.287\n",
      "INFO:tensorflow:loss = 20.6892, step = 2001 (0.331 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.65\n",
      "INFO:tensorflow:loss = 20.1331, step = 2101 (0.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 290.864\n",
      "INFO:tensorflow:loss = 21.5705, step = 2201 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 334.639\n",
      "INFO:tensorflow:loss = 20.3418, step = 2301 (0.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 363.845\n",
      "INFO:tensorflow:loss = 21.484, step = 2401 (0.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.945\n",
      "INFO:tensorflow:loss = 21.2929, step = 2501 (0.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 316.638\n",
      "INFO:tensorflow:loss = 20.5863, step = 2601 (0.317 sec)\n",
      "INFO:tensorflow:global_step/sec: 278.711\n",
      "INFO:tensorflow:loss = 21.2036, step = 2701 (0.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.423\n",
      "INFO:tensorflow:loss = 20.8116, step = 2801 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 323.81\n",
      "INFO:tensorflow:loss = 20.8372, step = 2901 (0.305 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into model/DNN\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 20.6672.\n"
     ]
    }
   ],
   "source": [
    "%pdb off\n",
    "classifier.train(input_fn=train_input_fn, steps=3000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test inputs  從 mnist 抄過來改的\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(x_test)[:,:xwidth]},\n",
    "    y=np.array(y_test),\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-08-02:47:58\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model/DNN\\model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-08-02:48:00\n",
      "INFO:tensorflow:Saving dict for global step 3000: accuracy = 0.55, accuracy_baseline = 0.55, auc = 0.915264, auc_precision_recall = 0.935299, average_loss = 0.652204, global_step = 3000, label/mean = 0.55, loss = 39.1322, precision = 0.55, prediction/mean = 0.558955, recall = 1.0\n",
      "\n",
      "Test Accuracy: 55.000001%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy\n",
    "evaluate = classifier.evaluate(input_fn=test_input_fn)\n",
    "accuracy_score = evaluate[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用 test 資料來比對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make input function for prediction:\n",
    "#   shuffle=False -> do not randomize input data\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(x_test)[:,:xwidth]},\n",
    "    y=np.array(y_test),\n",
    "    num_epochs=1,  # Just one epoch\n",
    "    shuffle=False, # Don't shuffle so we can compare to true labels later\n",
    "    )  # 參考 GitHub/tensorflow-workshop/examples/07_structured_data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從 wh300/DNNClassifier_Predicting.ipynb 抄來的例子\n",
    "# Predict 整批 test input \n",
    "preds = classifier.predict(pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model/DNN\\model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "# preds 好像是 interator 要反覆研究必須這樣取出來\n",
    "predict = [i for i in preds];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( DNN 猜測的結果 ) predictions tib. \\ ==> [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] (<class 'list'>)\n",
      "( 跟實際的比較 ) y_test tib. \\ ==> [0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0\n",
      " 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0] (<class 'numpy.ndarray'>)\n",
      "predictions py> sum(pop()) tib. \\ ==> 60 (<class 'numpy.int64'>)\n",
      "y_test py> sum(pop()) tib. \\ ==> 33 (<class 'numpy.int32'>)\n",
      "predictions np :> array(pop()).shape tib. \\ ==> (60,) (<class 'tuple'>)\n",
      "y_test np :> array(pop()).shape tib. \\ ==> (60,) (<class 'tuple'>)\n"
     ]
    }
   ],
   "source": [
    "# np.argmax() 就是用來從 probabilities array 讀出結論的方法\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html\n",
    "\n",
    "predictions = [np.argmax(predict[i]['probabilities'],axis=-1) for i in range(len(x_test))]; \n",
    "%f ( DNN 猜測的結果 ) predictions tib.\n",
    "%f ( 跟實際的比較 ) y_test tib.\n",
    "%f predictions py> sum(pop()) tib.\n",
    "%f y_test py> sum(pop()) tib.\n",
    "%f predictions np :> array(pop()).shape tib.\n",
    "%f y_test np :> array(pop()).shape tib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories ( 沒事的，漏掉的，多的，一致的 ) tib. \\ ==> [0, 0, 27, 33] (<class 'list'>)\n"
     ]
    }
   ],
   "source": [
    "# 查看 AI 預測的結果，與人工 tag 比較。\n",
    "categories = [0,0,0,0]\n",
    "for i in range(len(y_test)): \n",
    "    if int(predictions[i])==int(y_test[i]) and int(y_test[i])==0: categories[0]+=1; continue\n",
    "    if int(predictions[i])!=int(y_test[i]) and int(y_test[i])==1: categories[1]+=1; continue\n",
    "    if int(predictions[i])!=int(y_test[i]) and int(y_test[i])==0: categories[2]+=1; continue\n",
    "    if int(predictions[i])==int(y_test[i]) and int(y_test[i])==1: categories[3]+=1; continue\n",
    "%f categories ( 沒事的，漏掉的，多的，一致的 ) tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!start tensorboard --logdir=\"model/DNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用 iris 來檢驗一下我的程式\n",
    "###  漲停後 one-shot 天內問【今天續漲嗎？】\n",
    "\n",
    "iris 就沒問題，效果非常好，那可以說是 data 的問題囉？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "alert(\"要不要把 model/DNN directory 先殺掉？\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import peforth as f\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%f\n",
    "\\ Now we redefine the 'unknown' command that does nothing at default  \n",
    ": unknown ( token -- thing Y|N) // Try to find the unknown token in __main__\n",
    "  py> getattr(sys.modules['__main__'],pop(),\"Ûnknôwn\") \n",
    "  py> str(tos())==\"Ûnknôwn\" if drop false else true then ;\n",
    "  /// here after, when FORTH come accross an unknown token, instead of alerting \n",
    "  /// it try to find the token in python __main__ module name space.\n",
    ": path-to-find-modules ( <path> -- ) // Add path to sys.path so \"import module-name\" can find the module\n",
    "    CR word trim ( \"path\" ) py: sys.path.append(pop()) ;\n",
    "code \\ print(nexttoken('\\n')) end-code // Redefine \\ command to print the comment line \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=iris.data \n",
    "y=iris.target\n",
    "%f x :> shape tib.\n",
    "%f y :> shape tib.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f (  ) x :> shape tib. ( ?, 751 )\n",
    "%f (  ) x :> shape[0] tib. ( ? )\n",
    "%f (  ) y :> shape tib. ( ?, )    \n",
    "%f (  ) y :> shape[0] tib. ( ? )    \n",
    "%f (  ) y py> sum(pop()) tib. ( 1629 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f x_train np :> array(pop()).shape tib.\n",
    "%f y_train np :> array(pop()).shape tib.\n",
    "%f x_test np :> array(pop()).shape tib.\n",
    "%f y_test np :> array(pop()).shape tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 應該只有一個 feature 是個 shape = (751,) 的 array. 這怎麼表達?\n",
    "my_feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[4,])]  \n",
    "my_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build 2 hidden layer DNN\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    hidden_units=[64,32],\n",
    "    n_classes=3,\n",
    "    dropout=0.2,\n",
    "    model_dir=\"model/DNN\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training inputs 從 mnist 抄過來改的\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_train},\n",
    "    y=y_train,\n",
    "    num_epochs=None,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb off\n",
    "classifier.train(input_fn=train_input_fn, steps=3000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test inputs  從 mnist 抄過來改的\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_test},\n",
    "    y=y_test,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about a 'recall' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test inputs  從 mnist 抄過來改的\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_train},\n",
    "    y=y_train,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  漲停後 one-shot 天內問【今天續漲嗎？】\n",
    "\n",
    "- 照老師的方法，把資料整理成 150天 一長條一筆，如果往前沒有 150 天就不用。\n",
    "- 掃出所有漲停的日子，以及後 one-shot 天，如果又有漲停就繼續 one-shot。這樣縮小 dataset 範圍。\n",
    "- 以此 dataset 問 AI 明天是否【續漲】且【不破前低】？平衡正反雙方，取得 n 值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "alert(\"要不要把 model/DNN directory 先殺掉？\")\n",
    "alert(\"注意 batch_size 合不合理？一千也照跑，結果都是 0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import peforth as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%f\n",
    "\\ Now we redefine the 'unknown' command that does nothing at default  \n",
    ": unknown ( token -- thing Y|N) // Try to find the unknown token in __main__\n",
    "  py> getattr(sys.modules['__main__'],pop(),\"Ûnknôwn\") \n",
    "  py> str(tos())==\"Ûnknôwn\" if drop false else true then ;\n",
    "  /// here after, when FORTH come accross an unknown token, instead of alerting \n",
    "  /// it try to find the token in python __main__ module name space.\n",
    ": path-to-find-modules ( <path> -- ) // Add path to sys.path so \"import module-name\" can find the module\n",
    "    CR word trim ( \"path\" ) py: sys.path.append(pop()) ;\n",
    "code \\ print(nexttoken('\\n')) end-code // Redefine \\ command to print the comment line \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('000777.csv',encoding='gbk',parse_dates=[0],index_col=0)\n",
    "\n",
    "# data frame 中帶有 None 的 rows 都剔除掉。 本來有 4752 rows 現在只剩 4639 \n",
    "# 用法: df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "data = data.replace(to_replace='None', value=np.nan).dropna()  \n",
    "\n",
    "# pdb.set_trace() # data 本來是新的在上面\n",
    "data.sort_index(0,ascending=True,inplace=True) # 排序 sort 過後變成最舊的在上面\n",
    "\n",
    "# 上市第一天的資料不要，因為【漲跌幅】無效。\n",
    "data = data.query('index > \"1997-07-10\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data has 20 years long, but here only analysis the last 150 days of a chosen day\n",
    "dayfeature = 150\n",
    "oneshot = 5  # 漲停後慣性天數，使正反平衡\n",
    "dayforesee = oneshot  # 問未來幾天如何如何，最後就要扣掉幾天，避免 out of bounds. 0 代表當天。\n",
    "featurenum = 5*dayfeature  # 取五個有關係的欄位，ochlq 等五個特徵值。\n",
    "cross_validation_rate = 0.2\n",
    "xlength = data.shape[0]-dayfeature-dayforesee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%f ( 總天數 ) data :> shape[0] tib. ( 4638 )\n",
    "%f ( leading days that are not used ) dayfeature tib.   \n",
    "%f ( latest days that are not used ) dayforesee tib.\n",
    "%f ( 有效天數 ) data :> shape[0] dayfeature - dayforesee - tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.zeros((xlength,featurenum+2))  # +2 是當天開盤跟當天日期\n",
    "y=np.zeros((xlength));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重疊 shift dayfeature (150) 天的五個「行情特徵值」加上當天的開盤 共 751 個參數當作 feature X \n",
    "for i in range(0,xlength):\n",
    "    x[i,0:featurenum]=np.array(data[i:i+dayfeature] \\\n",
    "          [[u'收盘价',u'最高价',u'最低价',u'开盘价',u'成交量']]).reshape((1,featurenum))\n",
    "    x[i,featurenum]=data.iloc[i+dayfeature][u'开盘价']\n",
    "    x[i,featurenum+1]=data.index[i+dayfeature].value  # backward pd.to_datetime(value).date()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預測明天是否會「續漲」且「不破前低」? ochl \n",
    "for i in range(0,xlength):\n",
    "    y[i]=0              \n",
    "    i0h = float(data.iloc[i][u'最高价']) # i 這天的\n",
    "    i0l = float(data.iloc[i][u'最低价']) # i 這天的\n",
    "    i1h = float(data.iloc[i+1][u'最高价']) # i 明天的\n",
    "    i1l = float(data.iloc[i+1][u'最低价']) # i 明天的\n",
    "    if i1h > i0h and i1l > i0l :\n",
    "            y[i]=1\n",
    "y = np.int32(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f (  ) x :> shape tib. ( ?, 751 )\n",
    "%f (  ) x :> shape[0] tib. ( ? )\n",
    "%f (  ) y :> shape tib. ( ?, )    \n",
    "%f (  ) y :> shape[0] tib. ( ? )    \n",
    "%f (  ) y py> sum(pop()) tib. ( 1629 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接看【涨跌幅】> 9.8 就算漲停板，從【漲停板】的日子往後多取 oneshot 天。\n",
    "# 符合條件的日子組成 dataset 其中含有 oneshot 天慣性，使正反兩方平衡。\n",
    "# 這個簡單的 loop 是挑出要的日子且消除重複的方法，我考慮過用 set() 也是取 unique 的好方法。\n",
    "\n",
    "x_mask = np.zeros((xlength,)) # 全部放 0 以下只針對目標填 1 \n",
    "x_mask = np.int32(x_mask)\n",
    "for i in range(xlength):\n",
    "    if float(data.iloc[i+dayfeature-1][u'涨跌幅'])>=9.8:\n",
    "        for j in range(oneshot):\n",
    "            x_mask[i+j-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f x_mask tib.\n",
    "%f x_mask :> shape tib.\n",
    "%f x_mask py> sum(pop()) tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_original = []\n",
    "y_original = []\n",
    "for i in range(xlength):\n",
    "    if x_mask[i]:\n",
    "        x_original.append(x[i])\n",
    "        y_original.append(y[i])\n",
    "x,x_original = x_original,x\n",
    "y,y_original = y_original,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f x_original np :> array(pop()).shape tib.\n",
    "%f y_original np :> array(pop()).shape tib.\n",
    "%f x np :> array(pop()).shape tib.\n",
    "%f y np :> array(pop()).shape tib.\n",
    "%f ( 差不多是 dataset 總數的一半嗎？ ) y py> sum(pop()) tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f x_train np :> array(pop()).shape tib.\n",
    "%f y_train np :> array(pop()).shape tib.\n",
    "%f x_test np :> array(pop()).shape tib.\n",
    "%f y_test np :> array(pop()).shape tib.\n",
    "%f ( numpy 才能裁切 matrix ) x_train np :> array(pop())[:,:751].shape tib.\n",
    "%f ( numpy 才能裁切 matrix ) x_test np :> array(pop())[:,:751].shape tib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 應該只有一個 feature 是個 shape = (751,) 的 array. 這怎麼表達?\n",
    "my_feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[751,])]  \n",
    "my_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[64,32],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=2,\n",
    "    dropout=0.2,\n",
    "    model_dir=\"model/DNN\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training inputs 從 mnist 抄過來改的\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(x_train)[:,:751]},\n",
    "    y=np.array(y_train),\n",
    "    num_epochs=None,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb off\n",
    "classifier.train(input_fn=train_input_fn, steps=30000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test inputs  從 mnist 抄過來改的\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(x_test)[:,:751]},\n",
    "    y=np.array(y_test),\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "evaluate = classifier.evaluate(input_fn=test_input_fn)\n",
    "accuracy_score = evaluate[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about a 'recall' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test inputs  從 mnist 抄過來改的\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(x_train)[:,:751]},\n",
    "    y=np.array(y_train),\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用全部資料來比對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make input function for prediction:\n",
    "#   shuffle=False -> do not randomize input data\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(x)[:,:751]},\n",
    "    y=np.array(y),\n",
    "    num_epochs=1,  # Just one epoch\n",
    "    shuffle=False, # Don't shuffle so we can compare to true labels later\n",
    "    )  # 參考 GitHub/tensorflow-workshop/examples/07_structured_data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從 wh300/DNNClassifier_Predicting.ipynb 抄來的例子\n",
    "# Predict 整批 test input \n",
    "preds = classifier.predict(pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds 好像是 interator 要反覆研究必須這樣取出來\n",
    "predict = [i for i in preds];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argmax() 就是用來從 probabilities array 讀出結論的方法\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html\n",
    "\n",
    "predictions = [np.argmax(predict[i]['probabilities'],axis=-1) for i in range(len(x))]; \n",
    "%f ( DNN 猜測的結果 ) predictions tib.\n",
    "%f ( 跟實際的比較 ) y tib.\n",
    "%f predictions py> sum(pop()) tib.\n",
    "%f y py> sum(pop()) tib.\n",
    "%f predictions np :> array(pop()).shape tib.\n",
    "%f y np :> array(pop()).shape tib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 AI 預測的結果，與人工 tag 比較。\n",
    "categories = [0,0,0,0]\n",
    "for i in range(len(y)): \n",
    "    if int(predictions[i])==int(y[i]) and int(y[i])==0: categories[0]+=1; continue\n",
    "    if int(predictions[i])!=int(y[i]) and int(y[i])==1: categories[1]+=1; continue\n",
    "    if int(predictions[i])!=int(y[i]) and int(y[i])==0: categories[2]+=1; continue\n",
    "    if int(predictions[i])==int(y[i]) and int(y[i])==1: categories[3]+=1; continue\n",
    "%f categories ( 沒事的，漏掉的，多的，一致的 ) tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用 test 資料來比對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make input function for prediction:\n",
    "#   shuffle=False -> do not randomize input data\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(x_test)[:,:751]},\n",
    "    y=np.array(y_test),\n",
    "    num_epochs=1,  # Just one epoch\n",
    "    shuffle=False, # Don't shuffle so we can compare to true labels later\n",
    "    )  # 參考 GitHub/tensorflow-workshop/examples/07_structured_data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從 wh300/DNNClassifier_Predicting.ipynb 抄來的例子\n",
    "# Predict 整批 test input \n",
    "preds = classifier.predict(pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds 好像是 interator 要反覆研究必須這樣取出來\n",
    "predict = [i for i in preds];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argmax() 就是用來從 probabilities array 讀出結論的方法\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html\n",
    "\n",
    "predictions = [np.argmax(predict[i]['probabilities'],axis=-1) for i in range(len(x_test))]; \n",
    "%f ( DNN 猜測的結果 ) predictions tib.\n",
    "%f ( 跟實際的比較 ) y_test tib.\n",
    "%f predictions py> sum(pop()) tib.\n",
    "%f y_test py> sum(pop()) tib.\n",
    "%f predictions np :> array(pop()).shape tib.\n",
    "%f y_test np :> array(pop()).shape tib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 AI 預測的結果，與人工 tag 比較。\n",
    "categories = [0,0,0,0]\n",
    "for i in range(len(y_test)): \n",
    "    if int(predictions[i])==int(y_test[i]) and int(y_test[i])==0: categories[0]+=1; continue\n",
    "    if int(predictions[i])!=int(y_test[i]) and int(y_test[i])==1: categories[1]+=1; continue\n",
    "    if int(predictions[i])!=int(y_test[i]) and int(y_test[i])==0: categories[2]+=1; continue\n",
    "    if int(predictions[i])==int(y_test[i]) and int(y_test[i])==1: categories[3]+=1; continue\n",
    "%f categories ( 沒事的，漏掉的，多的，一致的 ) tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用全部資料來比對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make input function for prediction:\n",
    "#   shuffle=False -> do not randomize input data\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(x)[:,:751]},\n",
    "    y=np.array(y),\n",
    "    num_epochs=1,  # Just one epoch\n",
    "    shuffle=False, # Don't shuffle so we can compare to true labels later\n",
    "    )  # 參考 GitHub/tensorflow-workshop/examples/07_structured_data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從 wh300/DNNClassifier_Predicting.ipynb 抄來的例子\n",
    "# Predict 整批 test input \n",
    "preds = classifier.predict(pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds 好像是 interator 要反覆研究必須這樣取出來\n",
    "predict = [i for i in preds];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argmax() 就是用來從 probabilities array 讀出結論的方法\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html\n",
    "\n",
    "predictions = [np.argmax(predict[i]['probabilities'],axis=-1) for i in range(len(x))]; \n",
    "%f ( DNN 猜測的結果 ) predictions tib.\n",
    "%f ( 跟實際的比較 ) y tib.\n",
    "%f predictions py> sum(pop()) tib.\n",
    "%f y py> sum(pop()) tib.\n",
    "%f predictions np :> array(pop()).shape tib.\n",
    "%f y np :> array(pop()).shape tib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 AI 預測的結果，與人工 tag 比較。\n",
    "categories = [0,0,0,0]\n",
    "for i in range(len(y)): \n",
    "    if int(predictions[i])==int(y[i]) and int(y[i])==0: categories[0]+=1; continue\n",
    "    if int(predictions[i])!=int(y[i]) and int(y[i])==1: categories[1]+=1; continue\n",
    "    if int(predictions[i])!=int(y[i]) and int(y[i])==0: categories[2]+=1; continue\n",
    "    if int(predictions[i])==int(y[i]) and int(y[i])==1: categories[3]+=1; continue\n",
    "%f categories ( 沒事的，漏掉的，多的，一致的 ) tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(x_test)[:10,751]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(np.array(x_test)[:10,751])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!start tensorboard --logdir=\"model/DNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  [待續] 改良餵進資料的方式，直接用 shape(150,5) 跳過 150天打包的過程。\n",
    "\n",
    "- 要回溯的天數先指定，最前面這麼多天扣掉。\n",
    "- 要往後參考的天數先指定，最後面這麼多天扣掉。排除這兩條件，剩下來的就是可用的天數。\n",
    "- cross_validation 的工作很簡單，看取多少趴數當 test 從尾巴切走就對了。所以 ％ 數對應一個切分日期。\n",
    "- 改寫 input_fn 讓它從 train or test set 裡面挑一個出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 預測「未來 dayforesee 天會不會漲 20%？」\n",
    "\n",
    "實際 4638天當中有 1677天 符合條件\n",
    "用全部資料來比對： ( 沒事的，漏掉的，多的，一致的 ) tib. \\ ==> [2418, 1243, 383, 434] \n",
    "用 Test 比對 ： ( 沒事的，漏掉的，多的，一致的 ) tib. \\ ==> [433, 297, 110, 56]\n",
    "這樣看起來好像有點被記住了的問題，所以 dropout 用到了 0.4 （40％） 得到以上結果，只能這樣了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "alert(\"要不要把 model/DNN directory 先殺掉？\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import peforth as f\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%f\n",
    "\\ Now we redefine the 'unknown' command that does nothing at default  \n",
    ": unknown ( token -- thing Y|N) // Try to find the unknown token in __main__\n",
    "  py> getattr(sys.modules['__main__'],pop(),\"Ûnknôwn\") \n",
    "  py> str(tos())==\"Ûnknôwn\" if drop false else true then ;\n",
    "  /// here after, when FORTH come accross an unknown token, instead of alerting \n",
    "  /// it try to find the token in python __main__ module name space.\n",
    ": path-to-find-modules ( <path> -- ) // Add path to sys.path so \"import module-name\" can find the module\n",
    "    CR word trim ( \"path\" ) py: sys.path.append(pop()) ;\n",
    "code \\ print(nexttoken('\\n')) end-code // Redefine \\ command to print the comment line \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('000777.csv',encoding='gbk',parse_dates=[0],index_col=0)\n",
    "\n",
    "# data frame 中帶有 None 的 rows 都剔除掉。 本來有 4752 rows 現在只剩 4639 \n",
    "# 用法: df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "data = data.replace(to_replace='None', value=np.nan).dropna()  \n",
    "\n",
    "# pdb.set_trace() # data 本來是新的在上面\n",
    "data.sort_index(0,ascending=True,inplace=True) # 排序 sort 過後變成最舊的在上面\n",
    "\n",
    "# 上市第一天的資料不要，因為【漲跌幅】無效。\n",
    "data = data.query('index > \"1997-07-10\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data has 20 years long, but here only analysis the last 150 days of a chosen day\n",
    "dayfeature=150\n",
    "dayforesee=10  # 問未來幾天如何如何，最後就要扣掉幾天，避免 out of bounds. 0 代表當天。\n",
    "featurenum=5*dayfeature  # 取五個有關係的欄位，特徵值。\n",
    "x=np.zeros((data.shape[0]-dayfeature-dayforesee,featurenum+1))\n",
    "y=np.zeros((data.shape[0]-dayfeature-dayforesee));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重疊 shift 150 天的五個「行情數目」加上下一天的開盤 共 751 個「數目」當作 feature X \n",
    "\n",
    "for i in range(0,data.shape[0]-dayfeature-dayforesee):\n",
    "    x[i,0:featurenum]=np.array(data[i:i+dayfeature] \\\n",
    "          [[u'收盘价',u'最高价',u'最低价',u'开盘价',u'成交量']]).reshape((1,featurenum))\n",
    "    # x[i,featurenum]=data.ix[i+dayfeature][u'开盘价']  # ix deprecated\n",
    "    x[i,featurenum]=data.iloc[i+dayfeature][u'开盘价']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預測未來 dayforesee 天是否會「漲20%」?\n",
    "\n",
    "for i in range(0,data.shape[0]-dayfeature-dayforesee):\n",
    "    y[i]=0              \n",
    "    pi = float(data.iloc[i][u'收盘价'])\n",
    "    for j in range(dayforesee):\n",
    "        pi5 = float(data.iloc[j+i+dayfeature][u'最高价'])\n",
    "        if (pi5-pi)/pi >= 0.20 :\n",
    "            y[i]=1\n",
    "y = np.int32(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f ( 有這麼多天符合條件 ) y py> sum(pop()) tib.\n",
    "%f x_train :> shape tib.\n",
    "%f x_test :> shape tib.\n",
    "%f y_train :> shape tib.\n",
    "%f y_test :> shape tib.\n",
    "%f x_train :> shape[0] tib.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 應該只有一個 feature 是個 shape = (751,) 的 array. 這怎麼表達?\n",
    "my_feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[751,])]  \n",
    "my_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[256,128],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=2,\n",
    "    dropout=0.4,\n",
    "    model_dir=\"model/DNN\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training inputs 從 mnist 抄過來改的\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_train},\n",
    "    y=y_train,\n",
    "    num_epochs=None,\n",
    "    batch_size=1000,\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(input_fn=train_input_fn, steps=3000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test inputs  從 mnist 抄過來改的\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_test},\n",
    "    y=y_test,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))\n",
    "\n",
    "# End of \"mnist_estimator.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用全部資料來比對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make input function for prediction:\n",
    "#   shuffle=False -> do not randomize input data\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x},\n",
    "    y=y,\n",
    "    num_epochs=1,  # Just one epoch\n",
    "    shuffle=False, # Don't shuffle so we can compare to true labels later\n",
    "    )  # 參考 GitHub/tensorflow-workshop/examples/07_structured_data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從 wh300/DNNClassifier_Predicting.ipynb 抄來的例子\n",
    "# Predict 整批 test input \n",
    "preds = classifier.predict(pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = [i for i in preds];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [int(i['probabilities'][1] > i['probabilities'][0]) for i in predict];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%f predictions py> sum(pop()) tib.\n",
    "%f y py> sum(pop()) tib.\n",
    "%f y :> [:50] . cr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 AI 預測的結果，與人工 tag 比較。\n",
    "categories = [0,0,0,0]\n",
    "for i in range(y.shape[0]): \n",
    "    if int(predictions[i])==int(y[i]) and int(y[i])==0: categories[0]+=1; continue\n",
    "    if int(predictions[i])!=int(y[i]) and int(y[i])==1: categories[1]+=1; continue\n",
    "    if int(predictions[i])!=int(y[i]) and int(y[i])==0: categories[2]+=1; continue\n",
    "    if int(predictions[i])==int(y[i]) and int(y[i])==1: categories[3]+=1; continue\n",
    "%f categories ( 沒事的，漏掉的，多的，一致的 ) tib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 只用 Test dataset 來比對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make input function for prediction:\n",
    "#   shuffle=False -> do not randomize input data\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_test},\n",
    "    y=y_test,\n",
    "    num_epochs=1,  # Just one epoch\n",
    "    shuffle=False, # Don't shuffle so we can compare to true labels later\n",
    "    )  # 參考 GitHub/tensorflow-workshop/examples/07_structured_data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從 wh300/DNNClassifier_Predicting.ipynb 抄來的例子\n",
    "# Predict 整批 test input \n",
    "preds = classifier.predict(pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = [i for i in preds];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [int(i['probabilities'][1] > i['probabilities'][0]) for i in predict];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%f predictions py> sum(pop()) tib.\n",
    "%f y py> sum(pop()) tib.\n",
    "%f y :> [:50] . cr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 AI 預測的結果，與人工 tag 比較。\n",
    "categories = [0,0,0,0]\n",
    "for i in range(y_test.shape[0]): \n",
    "    if int(predictions[i])==int(y[i]) and int(y[i])==0: categories[0]+=1; continue\n",
    "    if int(predictions[i])!=int(y[i]) and int(y[i])==1: categories[1]+=1; continue\n",
    "    if int(predictions[i])!=int(y[i]) and int(y[i])==0: categories[2]+=1; continue\n",
    "    if int(predictions[i])==int(y[i]) and int(y[i])==1: categories[3]+=1; continue\n",
    "%f categories ( 沒事的，漏掉的，多的，一致的 ) tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!start tensorboard --logdir=\"model/DNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 都搞懂了，接下來打包 000777.csv 試試看吧!\n",
    "\n",
    "這把預測「明天會不會漲停？」結果因為實際 4638天當中只有88天漲停，全部猜「不」就有 98.10％ 的 accuracy. 可能正是因為怎麼猜別的都沒有這個好。 DNN 全部猜「不會」。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "alert(\"要不要把 DNN_model directory 先殺掉？\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import peforth as f\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%f\n",
    "\\ Now we redefine the 'unknown' command that does nothing at default  \n",
    ": unknown ( token -- thing Y|N) // Try to find the unknown token in __main__\n",
    "  py> getattr(sys.modules['__main__'],pop(),\"Ûnknôwn\") \n",
    "  py> str(tos())==\"Ûnknôwn\" if drop false else true then ;\n",
    "  /// here after, when FORTH come accross an unknown token, instead of alerting \n",
    "  /// it try to find the token in python __main__ module name space.\n",
    ": path-to-find-modules ( <path> -- ) // Add path to sys.path so \"import module-name\" can find the module\n",
    "    CR word trim ( \"path\" ) py: sys.path.append(pop()) ;\n",
    "code \\ print(nexttoken('\\n')) end-code // Redefine \\ command to print the comment line \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('000777.csv',encoding='gbk',parse_dates=[0],index_col=0)\n",
    "\n",
    "# data frame 中帶有 None 的 rows 都剔除掉。 本來有 4752 rows 現在只剩 4639 \n",
    "# 用法: df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "data = data.replace(to_replace='None', value=np.nan).dropna()  \n",
    "\n",
    "# pdb.set_trace() # data 本來是新的在上面\n",
    "data.sort_index(0,ascending=True,inplace=True) # 排序 sort 過後變成最舊的在上面\n",
    "\n",
    "# 上市第一天的資料不要，因為【漲跌幅】無效。\n",
    "data = data.query('index > \"1997-07-10\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data has 20 years long, but here only analysis the last 150 days of a chosen day\n",
    "dayfeature=150\n",
    "dayforesee=5  # 問未來幾天如何如何，最後就要扣掉幾天，避免 out of bounds. 0 代表當天。\n",
    "featurenum=5*dayfeature  # 取五個有關係的欄位，特徵值。\n",
    "x=np.zeros((data.shape[0]-dayfeature-dayforesee,featurenum+1))\n",
    "y=np.zeros((data.shape[0]-dayfeature-dayforesee));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重疊 shift 150 天的五個「行情數目」加上下一天的開盤 共 751 個「數目」當作 feature X \n",
    "\n",
    "for i in range(0,data.shape[0]-dayfeature-dayforesee):\n",
    "    x[i,0:featurenum]=np.array(data[i:i+dayfeature] \\\n",
    "          [[u'收盘价',u'最高价',u'最低价',u'开盘价',u'成交量']]).reshape((1,featurenum))\n",
    "    # x[i,featurenum]=data.ix[i+dayfeature][u'开盘价']  # ix deprecated\n",
    "    x[i,featurenum]=data.iloc[i+dayfeature][u'开盘价']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預測當天是否會「漲停」?\n",
    "\n",
    "# data.shape[0]-dayfeature 是 4489 同上 x, 所以這是在製作相對所有 x 的 y. x 是 (4488, 751)\n",
    "# dayfeature 是 150 \n",
    "# 直接看【涨跌幅】> 9.8 就算漲停板\n",
    "\n",
    "for i in range(0,data.shape[0]-dayfeature-dayforesee):\n",
    "    if float(data.iloc[i+dayfeature][u'涨跌幅'])>=9.8:\n",
    "        y[i]=1.0\n",
    "    else:\n",
    "        y[i]=0.0          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f x_train :> shape tib.\n",
    "%f x_test :> shape tib.\n",
    "%f y_train :> shape tib.\n",
    "%f y_test :> shape tib.\n",
    "%f x_train :> shape[0] tib.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用 cross_validation 切分好 train-test feature & label 之後, 可望弄成 input_fn 要的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 應該只有一個 feature 是個 shape = (751,) 的 array. 這怎麼表達?\n",
    "my_feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[751,])]  \n",
    "my_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[256, 128],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=2,\n",
    "    dropout=0.2,\n",
    "    model_dir=\"DNN_model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上 classifier 弄出來了，還是要面對 input_function 怎麼弄的問題。Lambda 太難懂了，從 wh300 或哪裡去抄吧。\n",
    "看到的都是 tf.estimator.inputs.pandas_input_fn 我曾查出有 'numpy_input_fn' \n",
    "\n",
    "    c:\\Users\\hcche\\Documents\\GitHub\\twstock\\mnist_estimator.ipynb \n",
    "    # Specify feature\n",
    "    feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[28, 28])]\n",
    "    幹！ 回想起來了，用這個例子可以簡化 150天 的資料製作方式\n",
    "\n",
    "Estimator 的 DNNClassifier 怎麼用? 上面這個 mnist 也有似乎更進一步的發揮\n",
    "\n",
    "    # Build 2 layer DNN classifier\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        hidden_units=[256, 32],\n",
    "        optimizer=tf.train.AdamOptimizer(1e-4),\n",
    "        n_classes=10,\n",
    "        dropout=0.1,\n",
    "        model_dir=\"./tmp/mnist_model\"\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training inputs 從 mnist 抄過來改的\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_train},\n",
    "    y=y_train,\n",
    "    num_epochs=None,\n",
    "    batch_size=x_train.shape[0],  # 漲停很少，故 batch 要用全部天數，否則它全部猜「不會」\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(input_fn=train_input_fn, steps=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test inputs  從 mnist 抄過來改的\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_test},\n",
    "    y=y_test,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))\n",
    "\n",
    "# End of \"mnist_estimator.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讚！ 接下來怎麼看 predict 的結果 . . . 參考 wh300 吧\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make input function for prediction:\n",
    "#   shuffle=False -> do not randomize input data\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x},\n",
    "    y=y,\n",
    "    num_epochs=1,  # Just one epoch\n",
    "    shuffle=False, # Don't shuffle so we can compare to true labels later\n",
    "    )  # 參考 GitHub/tensorflow-workshop/examples/07_structured_data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從 wh300/DNNClassifier_Predicting.ipynb 抄來的例子\n",
    "# Predict 整批 test input \n",
    "preds = classifier.predict(pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [int(i['probabilities'][1] > i['probabilities'][0]) for i in preds];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f predictions py> sum(pop()) tib.\n",
    "%f y py> sum(pop()) tib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我在 Tensorflow 官網的教學網頁上找到了用 Iris 當例題的 estimator tutorial \n",
    "\n",
    "https://www.tensorflow.org/versions/master/get_started/premade_estimators (also on my Ynote \"Get Started with Estimators _TensorFlow_\") 有正式、詳細的介紹。\n",
    "這篇文章鼓勵人用 Estimator API 跟 Dataset API。本文研讀、實驗這篇文章。\n",
    "文章引用的 GitHub resource 我已經 clone 下來了在 T550 `c:\\Users\\hcche\\Documents\\GitHub\\models`  \n",
    "\n",
    "重點是，直接用 pandas data-frame 來當作 feature - label 餵給 input function 這樣更自然，免去搞 150 天行情資料的麻煩。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peforth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%f\n",
    "\\ Now we redefine the 'unknown' command that does nothing at default  \n",
    ": unknown ( token -- thing Y|N) // Try to find the unknown token in __main__\n",
    "  py> getattr(sys.modules['__main__'],pop(),\"Ûnknôwn\") \n",
    "  py> str(tos())==\"Ûnknôwn\" if drop false else true then ;\n",
    "  /// here after, when FORTH come accross an unknown token, instead of alerting \n",
    "  /// it try to find the token in python __main__ module name space.\n",
    "\n",
    ": path-to-find-modules ( <path> -- ) // Add path to sys.path so \"import module-name\" can find the module\n",
    "    CR word trim ( \"path\" ) py: sys.path.append(pop()) ;\n",
    "\n",
    "\\ set path for 'import iris_data'\n",
    "path-to-find-modules c:\\Users\\hcche\\Documents\\GitHub\\models\\samples\\core\\get_started\n",
    "\n",
    "code \\ print(nexttoken('\\n')) end-code // Redefine \\ command to print the comment line \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(np.array(irisX).reshape(-1,4), columns = iris.feature_names)    \n",
    "x = pd.DataFrame(iris.data, columns = iris.feature_names)    \n",
    "x[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(iris.target, columns = [\"target\"])    \n",
    "y[48:52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/pandas_input_fn\n",
    "\n",
    "tf.estimator.inputs.pandas_input_fn(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    num_epochs=5,\n",
    "    shuffle=False,\n",
    "    queue_capacity=None,\n",
    "    num_threads=1,\n",
    "    target_column='target'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 照 Tutorial 執行 -- 一把就成功！\n",
    "\n",
    "直接把本 notebook 建個 hard link 到 c:\\Users\\hcche\\Documents\\GitHub\\models\\samples\\core\\get_started 去執行...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run premade_estimator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開始照教材研讀\n",
    "    \n",
    "premade_estimator.py iris_data.py 都抓過來，他本來都放在 main() 內執行，不利 study 今予拆開，分段執行亦可，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "\"\"\"An Example of a DNNClassifier for the Iris dataset.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "import iris_data  # iris_data.py 在本目錄下 GitHub\\models\\samples\\core\\get_started\\iris_data.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這 argparse module 很常見 DeepSpeech 就有，這裡用不著吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', default=100, type=int, help='batch size')\n",
    "parser.add_argument('--train_steps', default=1000, type=int,\n",
    "                    help='number of training steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 不能在 jupyter notebook 下直接跑，因為 argv 不如預期。\n",
    "from IPython.display import display,Image;display(Image('error1.jpg', width=700))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "研究一下 parser (argparse module) 怎麼回事兒 <== 只能在 python 用，ipython & jupyter notebook 都不行\n",
    "http://localhost:8888/notebooks/Documents/GitHub/models/samples/core/get_started/My%20estimator%20to%20predict%20iris.ipynb\n",
    "\n",
    "[x] 這是在 Jupyter Notebook 下記錄到的\n",
    "    OK Argv . cr\n",
    "    ['c:\\\\users\\\\hcche\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\site-packages\\\\ipykernel_launcher.py', '-f', 'C:\\\\Users\\\\hcche\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-d21c0b87-4db1-432d-82ca-cd398f2dd5ad.json']\n",
    "    OK \n",
    "    看起來有三個 args -- .py script 本身, -f , 還有一個 .json, 後兩個是 jupyter notebook 加的，難怪\n",
    "    要出問題。\n",
    "\n",
    "[x] 改用 ipython 直接跑 premade_estimator.py 看看 .... 成功。 \n",
    "    --> 那他看到的 Argv 是怎樣? -->['c:\\\\Users\\\\hcche\\\\Documents\\\\GitHub\\\\models\\\\samples\\\\core\\\\get_started\\\\premade_estimator.py']\n",
    "        等於是只有 argv[0] 的 .py script 本身一個 args 那就沒問題了。\n",
    "\n",
    "[x] 我猜測了一下，矇對了，這樣執行：\n",
    "    ipython premade_estimator.py --batch_size=1234 --train_steps=5678\n",
    "    搗出了一些 warning 警告說 args 可能有問題，但總之得到的 args 是：\n",
    "    argv 是這樣 \n",
    "    ['c:\\\\Users\\\\hcche\\\\Documents\\\\GitHub\\\\models\\\\samples\\\\core\\\\get_started\\\\premade_estimator.py']\n",
    "    args 是這樣\n",
    "    Namespace(batch_size=100, train_steps=1000)\n",
    "[x] 確定不能用 ipython, 用 python 以下皆可：\n",
    "    python premade_estimator.py --batch_size=64 --train_steps=1000\n",
    "    python premade_estimator.py --batch_size 64 --train_steps 1000\n",
    "\n",
    "反正我現在知道怎麼讓 jupyter notebook 能跑了! 改這樣即可： \n",
    "    args = parser.parse_args(\"\")  # 用 default 值\n",
    "    args = parser.parse_args([\"--batch_size=100\", \"--train_steps=1000\"])  # 代替 command line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Command Line Arguments 要用模擬的\n",
    "Module argparse 只能在 python command line 跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ [x] 沒有指定時，自動取 default 值，真不知道這哪來的？看到了，從 parser 那邊 code 裡給的。\n",
    "args = parser.parse_args([\"--batch_size=100\", \"--train_steps=1000\"])\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 研究  (train_x, train_y), (test_x, test_y) = iris_data.load_data()\n",
    "\n",
    "看出來了 iris_data.py 是把 150 筆資料切成 120 : 30 for training and testing respectively.\n",
    "這是個壞消息，他沒有像 sklearn 的自動切的那啥 function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " (train_x, train_y), (test_x, test_y) = iris_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f train_x type . cr\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f train_y type . cr\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f test_x type . cr\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f test_y type . cr\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ [ ] 這是 feature columns, 我晚點可以嘗試把它弄成光一個 shape 為 (4,) 的單一 feature \n",
    "for key in train_x.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ [ ] 這是 feature columns, 我晚點可以嘗試把它弄成光一個 shape 為 (4,) 的單一 feature \n",
    "my_feature_columns = []\n",
    "for key in train_x.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "my_feature_columns    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%f \\ 從 main 裡拆分出來，方便 tracing. 可看出若無指定 model directory 自動放在\n",
    "%f \\     AppData\\Local\\Temp\\tmpijrg8mkm <-- 每次都不一樣，這樣好嗎？\n",
    "%f \\ 以及本 classifier 的種種細節\n",
    "%f \\ \n",
    "\n",
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[10, 10],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ 複習一下 lambda --> \n",
    "%f \\ lambda 是個 function, 產生 key(i) 取 x 的第 i 個 element。用於 sorted() 時指定哪個當 key \n",
    "s = [('a', 3), ('b', 2), ('c', 1)]\n",
    "key = lambda x:x[1]\n",
    "print(sorted(s, key=key))\n",
    "%f \\ 單獨使用時，看起來不太一樣，其實做的是一樣的事情。\n",
    "%f key :> ([('a',3),('b',2),('c',1),('d',4)]) tib.\n",
    "%f key tib.\n",
    "%f key .source\n",
    "%f \\ 當沒有給 lambda argument 時，是怎樣？\n",
    "x = lambda:s\n",
    "%f x tib. \\ 直接傳回 : 之後的東西(執行後)的傳回值\n",
    "%f x type tib.\n",
    "%f \\ 查看 x 的 source code \n",
    "%f x .source\n",
    "%f x :> () tib.\n",
    "\n",
    "%f \\ 所以，input_fn = lambda:iris_data.train_input_fn(train_x, train_y,args.batch_size)\n",
    "%f \\ 當中的 iris_data.train_input_fn() 是傳回一個 function 但是該 function 的 argument 是活的，\n",
    "%f \\ 要延後到最後一秒鐘才確定，因此要用 lambda 或單純的 function 包一層用來指定 arguments。\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%f \\ 這就開始訓練了 . . .  \n",
    "%f \\ \n",
    "\n",
    "# Train the Model.\n",
    "classifier.train(\n",
    "    input_fn=lambda:iris_data.train_input_fn(train_x, train_y,\n",
    "                                             args.batch_size),\n",
    "    steps=args.train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這時候的 classifier 已經受過訓練了! 可以接受考核了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model.\n",
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda:iris_data.eval_input_fn(test_x, test_y,\n",
    "                                            args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ 複習一下 **eval_result 啥意思？\n",
    "%f eval_result . cr\n",
    "%f eval_result :> ['accuracy'] . cr\n",
    "{**eval_result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt = {'accuracy': 0.96666664, 'average_loss': 0.056607112, 'loss': 1.6982133, 'global_step': 1000}\n",
    "print('Test set accuracy: {loss:0.3f}'.format(**rslt))\n",
    "print('Test set accuracy: {:0.3f}'.format(r['accuracy']))\n",
    "print('Test set accuracy: {a:0.3f}'.format(**{'a': 11,'b': 22}))\n",
    "print('Test set accuracy: {b:0.3f}'.format(**{'a': 33,'b': 44}))\n",
    "%f \\ 最後一式拆解不成功，拆開來的寫法也沒意義，要嘛直接寫就好了。\n",
    "%f \\ print('Test set accuracy: {b:0.3f}'.format(({'a': 55},{'b': 66})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這一段的意思是, 準備了 3 或 4 組資料 predict_x 分別的正確答案如 expected 所列\n",
    "# Generate predictions from the model\n",
    "expected = ['Setosa', 'Versicolor', 'Virginica','Versicolor']\n",
    "predict_x = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9,6.2],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1,2.9],\n",
    "    'PetalLength': [1.7, 4.2, 5.4,4.3],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1,1.3],\n",
    "}\n",
    "\n",
    "[(i,j) for i,j in zip(predict_x,expected)]\n",
    "[i for i in zip(predict_x,expected)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(\n",
    "    input_fn=lambda:iris_data.eval_input_fn(predict_x,\n",
    "                                            labels=None,\n",
    "                                            batch_size=args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這樣寫也可以\n",
    "predictions = classifier.predict(\n",
    "    lambda:iris_data.eval_input_fn(predict_x,\n",
    "                                            labels=None,\n",
    "                                            batch_size=args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f predictions dir . cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ 以上 predictions 瞬間就取得了\n",
    "%f predictions tib.\n",
    "%f \\ 這 'generator' 用過就沒了，要再看一遍就得重新跑一次 classifier.predict()\n",
    "pred = [i for i in predictions]\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_dict, expec in zip(predictions, expected):\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print(template.format(iris_data.SPECIES[class_id],\n",
    "                          100 * probability, expec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip?\n",
    "%f \\ zip() 傳回 iterator 像拉拉鍊一樣，所有第一個一組、所有第二個一組、如此以往到哪個先結束為止\n",
    "z = zip((1,2,3,4),(11,22,33),{'aa':1,'bb':2})\n",
    "%f z type tib.\n",
    "z1 = [i for i in z]\n",
    "z2 = [i for i in z]\n",
    "%f z1 tib.\n",
    "%f z2 tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[(pred_dict, expec) for pred_dict, expec in zip(pred, expected)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以賣弄這個方法時 pred （測試資料）的個數與 expected （標準結果）就要吻合才有意思。以下簡單就是 prediction 與標準答案的比對列出來看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pred_dict, expec in zip(pred, expected):\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print(template.format(iris_data.SPECIES[class_id],\n",
    "                          100 * probability, expec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下是原本的教學程式\n",
    "\n",
    "### premade_estimator.py\n",
    "c:\\Users\\hcche\\Documents\\GitHub\\models\\samples\\core\\get_started\\premade_estimator.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    args = parser.parse_args(\"\")  # default is ([\"--batch_size=100\", \"--train_steps=1000\"])\n",
    "    # Fetch the data\n",
    "    (train_x, train_y), (test_x, test_y) = iris_data.load_data()\n",
    "\n",
    "    # Feature columns describe how to use the input.\n",
    "    my_feature_columns = []\n",
    "    for key in train_x.keys():\n",
    "        my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "\n",
    "    # Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=my_feature_columns,\n",
    "        # Two hidden layers of 10 nodes each.\n",
    "        hidden_units=[10, 10],\n",
    "        # The model must choose between 3 classes.\n",
    "        n_classes=3)\n",
    "\n",
    "    # Train the Model.\n",
    "    classifier.train(\n",
    "        input_fn=lambda:iris_data.train_input_fn(train_x, train_y,\n",
    "                                                 args.batch_size),\n",
    "        steps=args.train_steps)\n",
    "\n",
    "    # Evaluate the model.\n",
    "    eval_result = classifier.evaluate(\n",
    "        input_fn=lambda:iris_data.eval_input_fn(test_x, test_y,\n",
    "                                                args.batch_size))\n",
    "\n",
    "    print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
    "\n",
    "    # Generate predictions from the model\n",
    "    expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "    predict_x = {\n",
    "        'SepalLength': [5.1, 5.9, 6.9],\n",
    "        'SepalWidth': [3.3, 3.0, 3.1],\n",
    "        'PetalLength': [1.7, 4.2, 5.4],\n",
    "        'PetalWidth': [0.5, 1.5, 2.1],\n",
    "    }\n",
    "\n",
    "    predictions = classifier.predict(\n",
    "        input_fn=lambda:iris_data.eval_input_fn(predict_x,\n",
    "                                                labels=None,\n",
    "                                                batch_size=args.batch_size))\n",
    "\n",
    "    template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
    "\n",
    "    for pred_dict, expec in zip(predictions, expected):\n",
    "        class_id = pred_dict['class_ids'][0]\n",
    "        probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "        print(template.format(iris_data.SPECIES[class_id],\n",
    "                              100 * probability, expec))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    tf.app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iris_data.py\n",
    "c:\\Users\\hcche\\Documents\\GitHub\\models\\samples\\core\\get_started\\iris_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n",
    "                    'PetalLength', 'PetalWidth', 'Species']\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "def maybe_download():\n",
    "    train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)\n",
    "    test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)\n",
    "\n",
    "    return train_path, test_path\n",
    "\n",
    "# 這個有用到\n",
    "def load_data(y_name='Species'):\n",
    "    \"\"\"Returns the iris dataset as (train_x, train_y), (test_x, test_y).\"\"\"\n",
    "    train_path, test_path = maybe_download()\n",
    "\n",
    "    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    train_x, train_y = train, train.pop(y_name)\n",
    "\n",
    "    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    test_x, test_y = test, test.pop(y_name)\n",
    "\n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "# 這個有用到\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# 這個有用到\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    features=dict(features)\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# The remainder of this file contains a simple example of a csv parser,\n",
    "#     implemented using the `Dataset` class.\n",
    "\n",
    "# `tf.parse_csv` sets the types of the outputs to match the examples given in\n",
    "#     the `record_defaults` argument.\n",
    "CSV_TYPES = [[0.0], [0.0], [0.0], [0.0], [0]]\n",
    "\n",
    "def _parse_line(line):\n",
    "    # Decode the line into its fields\n",
    "    fields = tf.decode_csv(line, record_defaults=CSV_TYPES)\n",
    "\n",
    "    # Pack the result into a dictionary\n",
    "    features = dict(zip(CSV_COLUMN_NAMES, fields))\n",
    "\n",
    "    # Separate the label from the features\n",
    "    label = features.pop('Species')\n",
    "\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def csv_input_fn(csv_path, batch_size):\n",
    "    # Create a dataset containing the text lines.\n",
    "    dataset = tf.data.TextLineDataset(csv_path).skip(1)\n",
    "\n",
    "    # Parse each line.\n",
    "    dataset = dataset.map(_parse_line)\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ----- play ground -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates %%csv cell magic. See http://ipython-books.github.io/14-creating-an-ipython-extension-with-custom-magic-commands\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from IPython.core.magic import (register_cell_magic)\n",
    "@register_cell_magic\n",
    "def csv(line, cell):\n",
    "    return pd.read_csv(StringIO(cell), sep=line, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%csv \\s+\n",
    "a b c\n",
    "1 2 3\n",
    "4 5 6\n",
    "7 8 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f words module\n",
    "%f help modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f none value locals\n",
    "def smallworld():\n",
    "    x = 11\n",
    "    y = 22\n",
    "    peforth.push(locals()).dictate('to locals').ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peforth.ok()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn=lambda:iris_data.train_input_fn(train_x, train_y, args.batch_size)\n",
    "%f \\ lambda:iris_data.train_input_fn(_x, _y, _size)  # 定義時 identifiers 可以亂寫,那這好像是 macro 了?\n",
    "%f input_fn constant i/f\n",
    "%f i/f . cr\n",
    "%f i/f type . cr\n",
    "%f i/f dir . cr\n",
    "%f i/f :> () . cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda:locals\n",
    "\n",
    "def outer():\n",
    "    x = '1'\n",
    "    y = '2'\n",
    "    \n",
    "    def given(locs):\n",
    "        return(locs)\n",
    "    \n",
    "    def getit(locs=locals()):\n",
    "        return(locs) \n",
    "    \n",
    "    def getit2(locs=locals):\n",
    "        return(locs()) \n",
    "    \n",
    "    def testgiven():\n",
    "        x = 33\n",
    "        y = 44\n",
    "        l = given(locals())\n",
    "        return(x,y,l)\n",
    "    \n",
    "    def testgetit():\n",
    "        x = 55\n",
    "        y = 66\n",
    "        l = getit()\n",
    "        return(x,y,l)\n",
    "    \n",
    "    def testgetit2():\n",
    "        x = 77\n",
    "        y = 88\n",
    "        l = getit2()\n",
    "        return(x,y,l)\n",
    "\n",
    "    return testgiven(), testgetit(), testgetit2()\n",
    "\n",
    "outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myDebugger(loc):\n",
    "    print(loc)\n",
    "\n",
    "def debuggee():\n",
    "    x = 11\n",
    "    peforth.ok('11>', loc=dict(locals()))\n",
    "    x = 22\n",
    "    peforth.ok('22>', loc=dict(locals()))\n",
    "    x = 33\n",
    "    peforth.ok('33>', loc=dict(locals()))\n",
    "\n",
    "debuggee()    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
