{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 都搞懂了，接下來打包 000777.csv 試試看吧!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import peforth as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reDef unknown\n"
     ]
    }
   ],
   "source": [
    "%%f\n",
    ": unknown ( token -- thing Y|N) // Try to find the unknown token in __main__\n",
    "  py> getattr(sys.modules['__main__'],pop(),\"Ûnknôwn\") \n",
    "  py> str(tos())==\"Ûnknôwn\" if drop false else true then ;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ok>: test ;\n",
      "ok>: test ;\n",
      "reDef test\n",
      "ok>exit\n",
      "ok>"
     ]
    }
   ],
   "source": [
    "%f *debug* ok>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%f\n",
    "\\ Now we redefine the 'unknown' command that does nothing at default  \n",
    ": unknown ( token -- thing Y|N) // Try to find the unknown token in __main__\n",
    "  py> getattr(sys.modules['__main__'],pop(),\"Ûnknôwn\") \n",
    "  py> str(tos())==\"Ûnknôwn\" if drop false else true then ;\n",
    "  /// here after, when FORTH come accross an unknown token, instead of alerting \n",
    "  /// it try to find the token in python __main__ module name space.\n",
    ": path-to-find-modules ( <path> -- ) // Add path to sys.path so \"import module-name\" can find the module\n",
    "    CR word trim ( \"path\" ) py: sys.path.append(pop()) ;\n",
    "code \\ print(nexttoken('\\n')) end-code // Redefine \\ command to print the comment line \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 研究  (train_x, train_y), (test_x, test_y) = iris_data.load_data()\n",
    "\n",
    "看出來了 iris_data.py 是把 150 筆資料切成 120 : 30 for training and testing respectively.\n",
    "這是個壞消息，他沒有像 sklearn 的自動切的那啥 function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " (train_x, train_y), (test_x, test_y) = iris_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f train_x type . cr\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f train_y type . cr\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f test_x type . cr\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f test_y type . cr\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ [ ] 這是 feature columns, 我晚點可以嘗試把它弄成光一個 shape 為 (4,) 的單一 feature \n",
    "for key in train_x.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ [ ] 這是 feature columns, 我晚點可以嘗試把它弄成光一個 shape 為 (4,) 的單一 feature \n",
    "my_feature_columns = []\n",
    "for key in train_x.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "my_feature_columns    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%f \\ 從 main 裡拆分出來，方便 tracing. 可看出若無指定 model directory 自動放在\n",
    "%f \\     AppData\\Local\\Temp\\tmpijrg8mkm <-- 每次都不一樣，這樣好嗎？\n",
    "%f \\ 以及本 classifier 的種種細節\n",
    "%f \\ \n",
    "\n",
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[10, 10],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ 複習一下 lambda --> \n",
    "%f \\ lambda 是個 function, 產生 key(i) 取 x 的第 i 個 element。用於 sorted() 時指定哪個當 key \n",
    "s = [('a', 3), ('b', 2), ('c', 1)]\n",
    "key = lambda x:x[1]\n",
    "print(sorted(s, key=key))\n",
    "%f \\ 單獨使用時，看起來不太一樣，其實做的是一樣的事情。\n",
    "%f key :> ([('a',3),('b',2),('c',1),('d',4)]) tib.\n",
    "%f key tib.\n",
    "%f key .source\n",
    "%f \\ 當沒有給 lambda argument 時，是怎樣？\n",
    "x = lambda:s\n",
    "%f x tib. \\ 直接傳回 : 之後的東西(執行後)的傳回值\n",
    "%f x type tib.\n",
    "%f \\ 查看 x 的 source code \n",
    "%f x .source\n",
    "%f x :> () tib.\n",
    "\n",
    "%f \\ 所以，input_fn = lambda:iris_data.train_input_fn(train_x, train_y,args.batch_size)\n",
    "%f \\ 當中的 iris_data.train_input_fn() 是傳回一個 function 但是該 function 的 argument 是活的，\n",
    "%f \\ 要延後到最後一秒鐘才確定，因此要用 lambda 或單純的 function 包一層用來指定 arguments。\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%f \\ 這就開始訓練了 . . .  \n",
    "%f \\ \n",
    "\n",
    "# Train the Model.\n",
    "classifier.train(\n",
    "    input_fn=lambda:iris_data.train_input_fn(train_x, train_y,\n",
    "                                             args.batch_size),\n",
    "    steps=args.train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這時候的 classifier 已經受過訓練了! 可以接受考核了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model.\n",
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda:iris_data.eval_input_fn(test_x, test_y,\n",
    "                                            args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ 複習一下 **eval_result 啥意思？\n",
    "%f eval_result . cr\n",
    "%f eval_result :> ['accuracy'] . cr\n",
    "{**eval_result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt = {'accuracy': 0.96666664, 'average_loss': 0.056607112, 'loss': 1.6982133, 'global_step': 1000}\n",
    "print('Test set accuracy: {loss:0.3f}'.format(**rslt))\n",
    "print('Test set accuracy: {:0.3f}'.format(r['accuracy']))\n",
    "print('Test set accuracy: {a:0.3f}'.format(**{'a': 11,'b': 22}))\n",
    "print('Test set accuracy: {b:0.3f}'.format(**{'a': 33,'b': 44}))\n",
    "%f \\ 最後一式拆解不成功，拆開來的寫法也沒意義，要嘛直接寫就好了。\n",
    "%f \\ print('Test set accuracy: {b:0.3f}'.format(({'a': 55},{'b': 66})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這一段的意思是, 準備了 3 或 4 組資料 predict_x 分別的正確答案如 expected 所列\n",
    "# Generate predictions from the model\n",
    "expected = ['Setosa', 'Versicolor', 'Virginica','Versicolor']\n",
    "predict_x = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9,6.2],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1,2.9],\n",
    "    'PetalLength': [1.7, 4.2, 5.4,4.3],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1,1.3],\n",
    "}\n",
    "\n",
    "[(i,j) for i,j in zip(predict_x,expected)]\n",
    "[i for i in zip(predict_x,expected)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(\n",
    "    input_fn=lambda:iris_data.eval_input_fn(predict_x,\n",
    "                                            labels=None,\n",
    "                                            batch_size=args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這樣寫也可以\n",
    "predictions = classifier.predict(\n",
    "    lambda:iris_data.eval_input_fn(predict_x,\n",
    "                                            labels=None,\n",
    "                                            batch_size=args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f predictions dir . cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ 以上 predictions 瞬間就取得了\n",
    "%f predictions tib.\n",
    "%f \\ 這 'generator' 用過就沒了，要再看一遍就得重新跑一次 classifier.predict()\n",
    "pred = [i for i in predictions]\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_dict, expec in zip(predictions, expected):\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print(template.format(iris_data.SPECIES[class_id],\n",
    "                          100 * probability, expec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip?\n",
    "%f \\ zip() 傳回 iterator 像拉拉鍊一樣，所有第一個一組、所有第二個一組、如此以往到哪個先結束為止\n",
    "z = zip((1,2,3,4),(11,22,33),{'aa':1,'bb':2})\n",
    "%f z type tib.\n",
    "z1 = [i for i in z]\n",
    "z2 = [i for i in z]\n",
    "%f z1 tib.\n",
    "%f z2 tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[(pred_dict, expec) for pred_dict, expec in zip(pred, expected)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以賣弄這個方法時 pred （測試資料）的個數與 expected （標準結果）就要吻合才有意思。以下簡單就是 prediction 與標準答案的比對列出來看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pred_dict, expec in zip(pred, expected):\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print(template.format(iris_data.SPECIES[class_id],\n",
    "                          100 * probability, expec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我在 Tensorflow 官網的教學網頁上找到了用 Iris 當例題的 estimator tutorial https://www.tensorflow.org/versions/master/get_started/premade_estimators (also on my Ynote \"Get Started with Estimators _TensorFlow_\") 有正式、詳細的介紹。\n",
    "這篇文章鼓勵人用 Estimator API 跟 Dataset API。本文研讀、實驗這篇文章。\n",
    "文章引用的 GitHub resource 我已經 clone 下來了在 T550 `c:\\Users\\hcche\\Documents\\GitHub\\models`  \n",
    "\n",
    "重點是，直接用 pandas data-frame 來當作 feature - label 餵給 input function 這樣更自然，免去搞 150 天行情資料的麻煩。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peforth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%f\n",
    "\\ Now we redefine the 'unknown' command that does nothing at default  \n",
    ": unknown ( token -- thing Y|N) // Try to find the unknown token in __main__\n",
    "  py> getattr(sys.modules['__main__'],pop(),\"Ûnknôwn\") \n",
    "  py> str(tos())==\"Ûnknôwn\" if drop false else true then ;\n",
    "  /// here after, when FORTH come accross an unknown token, instead of alerting \n",
    "  /// it try to find the token in python __main__ module name space.\n",
    "\n",
    ": path-to-find-modules ( <path> -- ) // Add path to sys.path so \"import module-name\" can find the module\n",
    "    CR word trim ( \"path\" ) py: sys.path.append(pop()) ;\n",
    "\n",
    "\\ set path for 'import iris_data'\n",
    "path-to-find-modules c:\\Users\\hcche\\Documents\\GitHub\\models\\samples\\core\\get_started\n",
    "\n",
    "code \\ print(nexttoken('\\n')) end-code // Redefine \\ command to print the comment line \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(np.array(irisX).reshape(-1,4), columns = iris.feature_names)    \n",
    "x = pd.DataFrame(iris.data, columns = iris.feature_names)    \n",
    "x[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(iris.target, columns = [\"target\"])    \n",
    "y[48:52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/pandas_input_fn\n",
    "\n",
    "tf.estimator.inputs.pandas_input_fn(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    num_epochs=5,\n",
    "    shuffle=False,\n",
    "    queue_capacity=None,\n",
    "    num_threads=1,\n",
    "    target_column='target'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 照 Tutorial 執行 -- 一把就成功！\n",
    "\n",
    "直接把本 notebook 建個 hard link 到 c:\\Users\\hcche\\Documents\\GitHub\\models\\samples\\core\\get_started 去執行...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run premade_estimator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開始照教材研讀\n",
    "    \n",
    "premade_estimator.py iris_data.py 都抓過來，他本來都放在 main() 內執行，不利 study 今予拆開，分段執行亦可，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "\"\"\"An Example of a DNNClassifier for the Iris dataset.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "import iris_data  # iris_data.py 在本目錄下 GitHub\\models\\samples\\core\\get_started\\iris_data.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這 argparse module 很常見 DeepSpeech 就有，這裡用不著吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', default=100, type=int, help='batch size')\n",
    "parser.add_argument('--train_steps', default=1000, type=int,\n",
    "                    help='number of training steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 不能在 jupyter notebook 下直接跑，因為 argv 不如預期。\n",
    "from IPython.display import display,Image;display(Image('error1.jpg', width=700))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "研究一下 parser (argparse module) 怎麼回事兒 <== 只能在 python 用，ipython & jupyter notebook 都不行\n",
    "http://localhost:8888/notebooks/Documents/GitHub/models/samples/core/get_started/My%20estimator%20to%20predict%20iris.ipynb\n",
    "\n",
    "[x] 這是在 Jupyter Notebook 下記錄到的\n",
    "    OK Argv . cr\n",
    "    ['c:\\\\users\\\\hcche\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\site-packages\\\\ipykernel_launcher.py', '-f', 'C:\\\\Users\\\\hcche\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-d21c0b87-4db1-432d-82ca-cd398f2dd5ad.json']\n",
    "    OK \n",
    "    看起來有三個 args -- .py script 本身, -f , 還有一個 .json, 後兩個是 jupyter notebook 加的，難怪\n",
    "    要出問題。\n",
    "\n",
    "[x] 改用 ipython 直接跑 premade_estimator.py 看看 .... 成功。 \n",
    "    --> 那他看到的 Argv 是怎樣? -->['c:\\\\Users\\\\hcche\\\\Documents\\\\GitHub\\\\models\\\\samples\\\\core\\\\get_started\\\\premade_estimator.py']\n",
    "        等於是只有 argv[0] 的 .py script 本身一個 args 那就沒問題了。\n",
    "\n",
    "[x] 我猜測了一下，矇對了，這樣執行：\n",
    "    ipython premade_estimator.py --batch_size=1234 --train_steps=5678\n",
    "    搗出了一些 warning 警告說 args 可能有問題，但總之得到的 args 是：\n",
    "    argv 是這樣 \n",
    "    ['c:\\\\Users\\\\hcche\\\\Documents\\\\GitHub\\\\models\\\\samples\\\\core\\\\get_started\\\\premade_estimator.py']\n",
    "    args 是這樣\n",
    "    Namespace(batch_size=100, train_steps=1000)\n",
    "[x] 確定不能用 ipython, 用 python 以下皆可：\n",
    "    python premade_estimator.py --batch_size=64 --train_steps=1000\n",
    "    python premade_estimator.py --batch_size 64 --train_steps 1000\n",
    "\n",
    "反正我現在知道怎麼讓 jupyter notebook 能跑了! 改這樣即可： \n",
    "    args = parser.parse_args(\"\")  # 用 default 值\n",
    "    args = parser.parse_args([\"--batch_size=100\", \"--train_steps=1000\"])  # 代替 command line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Command Line Arguments 要用模擬的\n",
    "Module argparse 只能在 python command line 跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ [x] 沒有指定時，自動取 default 值，真不知道這哪來的？看到了，從 parser 那邊 code 裡給的。\n",
    "args = parser.parse_args([\"--batch_size=100\", \"--train_steps=1000\"])\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 研究  (train_x, train_y), (test_x, test_y) = iris_data.load_data()\n",
    "\n",
    "看出來了 iris_data.py 是把 150 筆資料切成 120 : 30 for training and testing respectively.\n",
    "這是個壞消息，他沒有像 sklearn 的自動切的那啥 function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " (train_x, train_y), (test_x, test_y) = iris_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f train_x type . cr\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f train_y type . cr\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f test_x type . cr\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f test_y type . cr\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ [ ] 這是 feature columns, 我晚點可以嘗試把它弄成光一個 shape 為 (4,) 的單一 feature \n",
    "for key in train_x.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ [ ] 這是 feature columns, 我晚點可以嘗試把它弄成光一個 shape 為 (4,) 的單一 feature \n",
    "my_feature_columns = []\n",
    "for key in train_x.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "my_feature_columns    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%f \\ 從 main 裡拆分出來，方便 tracing. 可看出若無指定 model directory 自動放在\n",
    "%f \\     AppData\\Local\\Temp\\tmpijrg8mkm <-- 每次都不一樣，這樣好嗎？\n",
    "%f \\ 以及本 classifier 的種種細節\n",
    "%f \\ \n",
    "\n",
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[10, 10],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ 複習一下 lambda --> \n",
    "%f \\ lambda 是個 function, 產生 key(i) 取 x 的第 i 個 element。用於 sorted() 時指定哪個當 key \n",
    "s = [('a', 3), ('b', 2), ('c', 1)]\n",
    "key = lambda x:x[1]\n",
    "print(sorted(s, key=key))\n",
    "%f \\ 單獨使用時，看起來不太一樣，其實做的是一樣的事情。\n",
    "%f key :> ([('a',3),('b',2),('c',1),('d',4)]) tib.\n",
    "%f key tib.\n",
    "%f key .source\n",
    "%f \\ 當沒有給 lambda argument 時，是怎樣？\n",
    "x = lambda:s\n",
    "%f x tib. \\ 直接傳回 : 之後的東西(執行後)的傳回值\n",
    "%f x type tib.\n",
    "%f \\ 查看 x 的 source code \n",
    "%f x .source\n",
    "%f x :> () tib.\n",
    "\n",
    "%f \\ 所以，input_fn = lambda:iris_data.train_input_fn(train_x, train_y,args.batch_size)\n",
    "%f \\ 當中的 iris_data.train_input_fn() 是傳回一個 function 但是該 function 的 argument 是活的，\n",
    "%f \\ 要延後到最後一秒鐘才確定，因此要用 lambda 或單純的 function 包一層用來指定 arguments。\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%f \\ 這就開始訓練了 . . .  \n",
    "%f \\ \n",
    "\n",
    "# Train the Model.\n",
    "classifier.train(\n",
    "    input_fn=lambda:iris_data.train_input_fn(train_x, train_y,\n",
    "                                             args.batch_size),\n",
    "    steps=args.train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這時候的 classifier 已經受過訓練了! 可以接受考核了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model.\n",
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda:iris_data.eval_input_fn(test_x, test_y,\n",
    "                                            args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ 複習一下 **eval_result 啥意思？\n",
    "%f eval_result . cr\n",
    "%f eval_result :> ['accuracy'] . cr\n",
    "{**eval_result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt = {'accuracy': 0.96666664, 'average_loss': 0.056607112, 'loss': 1.6982133, 'global_step': 1000}\n",
    "print('Test set accuracy: {loss:0.3f}'.format(**rslt))\n",
    "print('Test set accuracy: {:0.3f}'.format(r['accuracy']))\n",
    "print('Test set accuracy: {a:0.3f}'.format(**{'a': 11,'b': 22}))\n",
    "print('Test set accuracy: {b:0.3f}'.format(**{'a': 33,'b': 44}))\n",
    "%f \\ 最後一式拆解不成功，拆開來的寫法也沒意義，要嘛直接寫就好了。\n",
    "%f \\ print('Test set accuracy: {b:0.3f}'.format(({'a': 55},{'b': 66})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這一段的意思是, 準備了 3 或 4 組資料 predict_x 分別的正確答案如 expected 所列\n",
    "# Generate predictions from the model\n",
    "expected = ['Setosa', 'Versicolor', 'Virginica','Versicolor']\n",
    "predict_x = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9,6.2],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1,2.9],\n",
    "    'PetalLength': [1.7, 4.2, 5.4,4.3],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1,1.3],\n",
    "}\n",
    "\n",
    "[(i,j) for i,j in zip(predict_x,expected)]\n",
    "[i for i in zip(predict_x,expected)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(\n",
    "    input_fn=lambda:iris_data.eval_input_fn(predict_x,\n",
    "                                            labels=None,\n",
    "                                            batch_size=args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這樣寫也可以\n",
    "predictions = classifier.predict(\n",
    "    lambda:iris_data.eval_input_fn(predict_x,\n",
    "                                            labels=None,\n",
    "                                            batch_size=args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f predictions dir . cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f \\ 以上 predictions 瞬間就取得了\n",
    "%f predictions tib.\n",
    "%f \\ 這 'generator' 用過就沒了，要再看一遍就得重新跑一次 classifier.predict()\n",
    "pred = [i for i in predictions]\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_dict, expec in zip(predictions, expected):\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print(template.format(iris_data.SPECIES[class_id],\n",
    "                          100 * probability, expec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip?\n",
    "%f \\ zip() 傳回 iterator 像拉拉鍊一樣，所有第一個一組、所有第二個一組、如此以往到哪個先結束為止\n",
    "z = zip((1,2,3,4),(11,22,33),{'aa':1,'bb':2})\n",
    "%f z type tib.\n",
    "z1 = [i for i in z]\n",
    "z2 = [i for i in z]\n",
    "%f z1 tib.\n",
    "%f z2 tib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[(pred_dict, expec) for pred_dict, expec in zip(pred, expected)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以賣弄這個方法時 pred （測試資料）的個數與 expected （標準結果）就要吻合才有意思。以下簡單就是 prediction 與標準答案的比對列出來看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pred_dict, expec in zip(pred, expected):\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print(template.format(iris_data.SPECIES[class_id],\n",
    "                          100 * probability, expec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下是原本的教學程式\n",
    "\n",
    "### premade_estimator.py\n",
    "c:\\Users\\hcche\\Documents\\GitHub\\models\\samples\\core\\get_started\\premade_estimator.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    args = parser.parse_args(\"\")  # default is ([\"--batch_size=100\", \"--train_steps=1000\"])\n",
    "    # Fetch the data\n",
    "    (train_x, train_y), (test_x, test_y) = iris_data.load_data()\n",
    "\n",
    "    # Feature columns describe how to use the input.\n",
    "    my_feature_columns = []\n",
    "    for key in train_x.keys():\n",
    "        my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "\n",
    "    # Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=my_feature_columns,\n",
    "        # Two hidden layers of 10 nodes each.\n",
    "        hidden_units=[10, 10],\n",
    "        # The model must choose between 3 classes.\n",
    "        n_classes=3)\n",
    "\n",
    "    # Train the Model.\n",
    "    classifier.train(\n",
    "        input_fn=lambda:iris_data.train_input_fn(train_x, train_y,\n",
    "                                                 args.batch_size),\n",
    "        steps=args.train_steps)\n",
    "\n",
    "    # Evaluate the model.\n",
    "    eval_result = classifier.evaluate(\n",
    "        input_fn=lambda:iris_data.eval_input_fn(test_x, test_y,\n",
    "                                                args.batch_size))\n",
    "\n",
    "    print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
    "\n",
    "    # Generate predictions from the model\n",
    "    expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "    predict_x = {\n",
    "        'SepalLength': [5.1, 5.9, 6.9],\n",
    "        'SepalWidth': [3.3, 3.0, 3.1],\n",
    "        'PetalLength': [1.7, 4.2, 5.4],\n",
    "        'PetalWidth': [0.5, 1.5, 2.1],\n",
    "    }\n",
    "\n",
    "    predictions = classifier.predict(\n",
    "        input_fn=lambda:iris_data.eval_input_fn(predict_x,\n",
    "                                                labels=None,\n",
    "                                                batch_size=args.batch_size))\n",
    "\n",
    "    template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
    "\n",
    "    for pred_dict, expec in zip(predictions, expected):\n",
    "        class_id = pred_dict['class_ids'][0]\n",
    "        probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "        print(template.format(iris_data.SPECIES[class_id],\n",
    "                              100 * probability, expec))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    tf.app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iris_data.py\n",
    "c:\\Users\\hcche\\Documents\\GitHub\\models\\samples\\core\\get_started\\iris_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n",
    "                    'PetalLength', 'PetalWidth', 'Species']\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "def maybe_download():\n",
    "    train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)\n",
    "    test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)\n",
    "\n",
    "    return train_path, test_path\n",
    "\n",
    "# 這個有用到\n",
    "def load_data(y_name='Species'):\n",
    "    \"\"\"Returns the iris dataset as (train_x, train_y), (test_x, test_y).\"\"\"\n",
    "    train_path, test_path = maybe_download()\n",
    "\n",
    "    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    train_x, train_y = train, train.pop(y_name)\n",
    "\n",
    "    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    test_x, test_y = test, test.pop(y_name)\n",
    "\n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "# 這個有用到\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# 這個有用到\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    features=dict(features)\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# The remainder of this file contains a simple example of a csv parser,\n",
    "#     implemented using the `Dataset` class.\n",
    "\n",
    "# `tf.parse_csv` sets the types of the outputs to match the examples given in\n",
    "#     the `record_defaults` argument.\n",
    "CSV_TYPES = [[0.0], [0.0], [0.0], [0.0], [0]]\n",
    "\n",
    "def _parse_line(line):\n",
    "    # Decode the line into its fields\n",
    "    fields = tf.decode_csv(line, record_defaults=CSV_TYPES)\n",
    "\n",
    "    # Pack the result into a dictionary\n",
    "    features = dict(zip(CSV_COLUMN_NAMES, fields))\n",
    "\n",
    "    # Separate the label from the features\n",
    "    label = features.pop('Species')\n",
    "\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def csv_input_fn(csv_path, batch_size):\n",
    "    # Create a dataset containing the text lines.\n",
    "    dataset = tf.data.TextLineDataset(csv_path).skip(1)\n",
    "\n",
    "    # Parse each line.\n",
    "    dataset = dataset.map(_parse_line)\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ----- play ground -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates %%csv cell magic. See http://ipython-books.github.io/14-creating-an-ipython-extension-with-custom-magic-commands\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from IPython.core.magic import (register_cell_magic)\n",
    "@register_cell_magic\n",
    "def csv(line, cell):\n",
    "    return pd.read_csv(StringIO(cell), sep=line, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%csv \\s+\n",
    "a b c\n",
    "1 2 3\n",
    "4 5 6\n",
    "7 8 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f words module\n",
    "%f help modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%f none value locals\n",
    "def smallworld():\n",
    "    x = 11\n",
    "    y = 22\n",
    "    peforth.push(locals()).dictate('to locals').ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peforth.ok()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn=lambda:iris_data.train_input_fn(train_x, train_y, args.batch_size)\n",
    "%f \\ lambda:iris_data.train_input_fn(_x, _y, _size)  # 定義時 identifiers 可以亂寫,那這好像是 macro 了?\n",
    "%f input_fn constant i/f\n",
    "%f i/f . cr\n",
    "%f i/f type . cr\n",
    "%f i/f dir . cr\n",
    "%f i/f :> () . cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda:locals\n",
    "\n",
    "def outer():\n",
    "    x = '1'\n",
    "    y = '2'\n",
    "    \n",
    "    def given(locs):\n",
    "        return(locs)\n",
    "    \n",
    "    def getit(locs=locals()):\n",
    "        return(locs) \n",
    "    \n",
    "    def getit2(locs=locals):\n",
    "        return(locs()) \n",
    "    \n",
    "    def testgiven():\n",
    "        x = 33\n",
    "        y = 44\n",
    "        l = given(locals())\n",
    "        return(x,y,l)\n",
    "    \n",
    "    def testgetit():\n",
    "        x = 55\n",
    "        y = 66\n",
    "        l = getit()\n",
    "        return(x,y,l)\n",
    "    \n",
    "    def testgetit2():\n",
    "        x = 77\n",
    "        y = 88\n",
    "        l = getit2()\n",
    "        return(x,y,l)\n",
    "\n",
    "    return testgiven(), testgetit(), testgetit2()\n",
    "\n",
    "outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myDebugger(loc):\n",
    "    print(loc)\n",
    "\n",
    "def debuggee():\n",
    "    x = 11\n",
    "    peforth.ok('11>', loc=dict(locals()))\n",
    "    x = 22\n",
    "    peforth.ok('22>', loc=dict(locals()))\n",
    "    x = 33\n",
    "    peforth.ok('33>', loc=dict(locals()))\n",
    "\n",
    "debuggee()    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
